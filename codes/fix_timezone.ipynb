{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "import os\n",
    "from os import path\n",
    "from glob import glob\n",
    "\n",
    "import ijson\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_dir  = \"../data/zip\"\n",
    "\n",
    "files = glob(os.path.join(zip_dir, '*'))\n",
    "for filename in files:\n",
    "    if '.zip' in filename:\n",
    "        pass\n",
    "    elif '.json' in filename:\n",
    "        pass\n",
    "    else:\n",
    "        newfile = filename+'.json'\n",
    "        os.rename(filename, newfile)\n",
    "\n",
    "jsons = glob(os.path.join(zip_dir, '*.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/zip/part-v003-o001-r-00000.json',\n",
       " '../data/zip/part-v003-o001-r-00001.json',\n",
       " '../data/zip/part-v003-o001-r-00002.json',\n",
       " '../data/zip/part-v003-o001-r-00003.json',\n",
       " '../data/zip/part-v003-o001-r-00004.json',\n",
       " '../data/zip/part-v003-o001-r-00005.json',\n",
       " '../data/zip/part-v003-o001-r-00006.json',\n",
       " '../data/zip/part-v003-o001-r-00007.json',\n",
       " '../data/zip/part-v003-o001-r-00008.json',\n",
       " '../data/zip/part-v003-o001-r-00009.json']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jsons = jsons[:2]\n",
    "jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading json elapsed_time:707.3656289577484[sec]\n",
      "Loading json elapsed_time:482.0935015678406[sec]\n",
      "Loading json elapsed_time:489.4361777305603[sec]\n",
      "Loading json elapsed_time:485.1404960155487[sec]\n",
      "Loading json elapsed_time:424.2236924171448[sec]\n",
      "Loading json elapsed_time:487.4071800708771[sec]\n",
      "Loading json elapsed_time:392.8997402191162[sec]\n",
      "Loading json elapsed_time:339.85941886901855[sec]\n",
      "Loading json elapsed_time:346.0995237827301[sec]\n",
      "Loading json elapsed_time:414.8661563396454[sec]\n"
     ]
    }
   ],
   "source": [
    "# jsons = ['../data/part_v003_o001_r_00000.json', ..., '../data/part_v003_o001_r_00001.json']\n",
    "\n",
    "dfrts = []\n",
    "dfors = []\n",
    "for i, json in enumerate(jsons):\n",
    "    # Instance Preparation\n",
    "    dates = []\n",
    "    tweets = []\n",
    "    user_ids = []\n",
    "    rts = []\n",
    "    \n",
    "    start = time.time()\n",
    "    # Load json file: date and tweet\n",
    "    with open(json, 'r', encoding='utf8') as file:\n",
    "        pet_parse = ijson.parse(file, multiple_values=True)\n",
    "        for prefix, event, value  in pet_parse:\n",
    "            # Date\n",
    "            if prefix == 'created_at':\n",
    "                dates.append(datetime.strptime(value, '%a %b %d %H:%M:%S +0000 %Y'))\n",
    "            # Tweet \n",
    "            if prefix == 'text':\n",
    "                    tweets.append(value.replace('\\n', '').replace('\\t', '').replace('\\r', '').replace('\\r\\n', '').replace('　', '')) # Delte space and indet and \\r\n",
    "            # User id \n",
    "            if prefix == 'user.id':\n",
    "                user_ids.append(value)\n",
    "            # RT Flag\n",
    "            if len(dates)-1 == len(rts) and prefix == 'retweeted_status':\n",
    "                rts.append(True)\n",
    "            if len(dates)-2 == len(rts):\n",
    "                rts.append(False)\n",
    "                \n",
    "        if len(dates) != len(rts):\n",
    "                rts.append(False)\n",
    "#\n",
    "    print (\"Loading json elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n",
    "    \n",
    "#     print(len(dates), len(tweets), len(user_ids), len(rts))\n",
    "    \n",
    "    data = np.vstack([user_ids, dates, tweets, rts]).T\n",
    "    df = pd.DataFrame(data, columns=['user_id', 'date', 'tweet', 'RT_flag'])\n",
    "    \n",
    "    # Date\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['time'] = df['date'].dt.time\n",
    "    df['date'] = df['date'].dt.date\n",
    "    \n",
    "    # divide df by Original tweet and RT \n",
    "    dfrt = df[df['RT_flag'] == True]\n",
    "    dfor = df[df['RT_flag'] == False]\n",
    "    \n",
    "    # Save to csv file\n",
    "    dfor.to_csv('../data/new_dfs/dfor'+str(i)+'.csv')\n",
    "    dfrt.to_csv('../data/new_dfs/dfrt'+str(i)+'.csv')\n",
    "    \n",
    "#     # Append to dfs\n",
    "    dfrts.append(dfor)\n",
    "    dfors.append(dfrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1175787 1175787 1175787 1175787\n"
     ]
    }
   ],
   "source": [
    "print(len(dates), len(tweets), len(user_ids), len(rts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read json dfs from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "def fixTimezone(df):\n",
    "    # Convert Timestamp to datetime\n",
    "    df.date_time.dt.to_pydatetime()\n",
    "    # + 9hours\n",
    "    jst = df.date_time.dt.to_pydatetime() + dt.timedelta(hours = 9)\n",
    "    # Add Column\n",
    "    df['date_time'] = jst\n",
    "\n",
    "    # Adjustment\n",
    "    df['year'] = df['date_time'].dt.year\n",
    "    df['month'] = df['date_time'].dt.month\n",
    "    df['day'] = df['date_time'].dt.day\n",
    "    df['hour'] = df['date_time'].dt.hour\n",
    "    df['time'] = df['date_time'].dt.time\n",
    "    df['date'] = df['date_time'].dt.date\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ORs\n",
    "zip_dir  = \"../data/new_dfs2\"\n",
    "csvfiles = glob(os.path.join(zip_dir, 'dfor*'))\n",
    "dfors = []\n",
    "for csvfile in csvfiles:\n",
    "    df = pd.read_csv(csvfile, parse_dates=[['date', 'time']]).drop('Unnamed: 0', axis=1)\n",
    "    fixTimezone(df)\n",
    "    df.drop(['date_time'],axis=1)\n",
    "    dfors.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Rts\n",
    "zip_dir  = \"../data/new_dfs2\"\n",
    "csvfiles = glob(os.path.join(zip_dir, 'dfrt*'))\n",
    "\n",
    "dfrts = []\n",
    "for csvfile in csvfiles:\n",
    "    df = pd.read_csv(csvfile, parse_dates=[['date', 'time']]).drop('Unnamed: 0', axis=1)\n",
    "    fixTimezone(df)\n",
    "    dfrts.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfors = [df.drop(['date_time'],axis=1) for df in dfors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>RT_flag</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-31 23:59:58</td>\n",
       "      <td>1227267163884384257</td>\n",
       "      <td>勇作さんは影まで込みで(^O^)だと思っているところがあるから、影を描かないとソワソワする。</td>\n",
       "      <td>False</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "      <td>2020-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-31 23:59:58</td>\n",
       "      <td>1191941158181703681</td>\n",
       "      <td>始めて園に通うこどもたちを迎える先生はきっと1日フルパワー全開の1日になると思います。保育が...</td>\n",
       "      <td>False</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "      <td>2020-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-31 23:59:58</td>\n",
       "      <td>778167031598374912</td>\n",
       "      <td>なんかもうだるい自分が嫌</td>\n",
       "      <td>False</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "      <td>2020-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-31 23:59:57</td>\n",
       "      <td>825005837185748992</td>\n",
       "      <td>こまるチャレンジいきます https://t.co/pdNmj8SqxW</td>\n",
       "      <td>False</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:57</td>\n",
       "      <td>2020-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-31 23:59:57</td>\n",
       "      <td>1635335509</td>\n",
       "      <td>₍₍ ( * ॑  ॑*) ⁾⁾ｿﾜｿﾜ</td>\n",
       "      <td>False</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:57</td>\n",
       "      <td>2020-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114096</th>\n",
       "      <td>2020-03-28 21:36:00</td>\n",
       "      <td>1163845866408402946</td>\n",
       "      <td>@mikaze_ryo 見られてたら緊張するけど逆にフルコンできる</td>\n",
       "      <td>False</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>21:36:00</td>\n",
       "      <td>2020-03-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114097</th>\n",
       "      <td>2020-03-28 21:36:00</td>\n",
       "      <td>1009407120549146625</td>\n",
       "      <td>@waon_asuka 世界中コロナ蔓延していて、恐ろしいよね😱地球温暖化といい不安だらけな...</td>\n",
       "      <td>False</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>21:36:00</td>\n",
       "      <td>2020-03-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114098</th>\n",
       "      <td>2020-03-28 21:36:00</td>\n",
       "      <td>1117088971316580352</td>\n",
       "      <td>今の変にマセてる小学生じゃなくて、お互い気になるんだけどつい意地悪しちゃう！みたいな可愛い小...</td>\n",
       "      <td>False</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>21:36:00</td>\n",
       "      <td>2020-03-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114099</th>\n",
       "      <td>2020-03-28 21:36:00</td>\n",
       "      <td>1128590746691325952</td>\n",
       "      <td>@teterucameranko そそ、イベント系もそうやし現金給付に関してクレクレしてる奴...</td>\n",
       "      <td>False</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>21:36:00</td>\n",
       "      <td>2020-03-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114100</th>\n",
       "      <td>2020-03-28 21:36:00</td>\n",
       "      <td>1177852379091193857</td>\n",
       "      <td>いろいろだるい、モチベない</td>\n",
       "      <td>False</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>21:36:00</td>\n",
       "      <td>2020-03-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1114101 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date_time              user_id  \\\n",
       "0       2020-03-31 23:59:58  1227267163884384257   \n",
       "1       2020-03-31 23:59:58  1191941158181703681   \n",
       "2       2020-03-31 23:59:58   778167031598374912   \n",
       "3       2020-03-31 23:59:57   825005837185748992   \n",
       "4       2020-03-31 23:59:57           1635335509   \n",
       "...                     ...                  ...   \n",
       "1114096 2020-03-28 21:36:00  1163845866408402946   \n",
       "1114097 2020-03-28 21:36:00  1009407120549146625   \n",
       "1114098 2020-03-28 21:36:00  1117088971316580352   \n",
       "1114099 2020-03-28 21:36:00  1128590746691325952   \n",
       "1114100 2020-03-28 21:36:00  1177852379091193857   \n",
       "\n",
       "                                                     tweet  RT_flag  year  \\\n",
       "0           勇作さんは影まで込みで(^O^)だと思っているところがあるから、影を描かないとソワソワする。    False  2020   \n",
       "1        始めて園に通うこどもたちを迎える先生はきっと1日フルパワー全開の1日になると思います。保育が...    False  2020   \n",
       "2                                             なんかもうだるい自分が嫌    False  2020   \n",
       "3                     こまるチャレンジいきます https://t.co/pdNmj8SqxW    False  2020   \n",
       "4                                     ₍₍ ( * ॑  ॑*) ⁾⁾ｿﾜｿﾜ    False  2020   \n",
       "...                                                    ...      ...   ...   \n",
       "1114096                  @mikaze_ryo 見られてたら緊張するけど逆にフルコンできる    False  2020   \n",
       "1114097  @waon_asuka 世界中コロナ蔓延していて、恐ろしいよね😱地球温暖化といい不安だらけな...    False  2020   \n",
       "1114098  今の変にマセてる小学生じゃなくて、お互い気になるんだけどつい意地悪しちゃう！みたいな可愛い小...    False  2020   \n",
       "1114099  @teterucameranko そそ、イベント系もそうやし現金給付に関してクレクレしてる奴...    False  2020   \n",
       "1114100                                      いろいろだるい、モチベない    False  2020   \n",
       "\n",
       "         month  day  hour      time        date  \n",
       "0            3   31    23  23:59:58  2020-03-31  \n",
       "1            3   31    23  23:59:58  2020-03-31  \n",
       "2            3   31    23  23:59:58  2020-03-31  \n",
       "3            3   31    23  23:59:57  2020-03-31  \n",
       "4            3   31    23  23:59:57  2020-03-31  \n",
       "...        ...  ...   ...       ...         ...  \n",
       "1114096      3   28    21  21:36:00  2020-03-28  \n",
       "1114097      3   28    21  21:36:00  2020-03-28  \n",
       "1114098      3   28    21  21:36:00  2020-03-28  \n",
       "1114099      3   28    21  21:36:00  2020-03-28  \n",
       "1114100      3   28    21  21:36:00  2020-03-28  \n",
       "\n",
       "[1114101 rows x 10 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n"
     ]
    }
   ],
   "source": [
    "for df in dfors:\n",
    "    print(df.RT_flag.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True]\n",
      "[ True]\n",
      "[ True]\n",
      "[ True]\n",
      "[ True]\n",
      "[ True]\n",
      "[ True]\n",
      "[ True]\n",
      "[ True]\n",
      "[ True]\n"
     ]
    }
   ],
   "source": [
    "for df in dfrts:\n",
    "    print(df.RT_flag.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>RT_flag</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1111855071380168705</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @k7LssPYI5D85fxI: https://t.co/5KgwBN00NS志村...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>14:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127471475</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @84MadokaMary: この国には知らんだけでめちゃめちゃな量の社会保障があるの...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>14:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138821520</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @zibumitunari: 明日はエイプリルフールだが、コロナ関連の嘘は絶対にやめよ...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>14:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>962192359059472384</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @akagiichirou: マスクの７割が中国からの輸入に頼ってきたが流通網が混乱す...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>14:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128360620</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @bbcnewsjapan: BBCニュース - ハンガリー政府、新型ウイルス対策で強...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>14:59:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user_id        date  \\\n",
       "0  1111855071380168705  2020-03-31   \n",
       "1            127471475  2020-03-31   \n",
       "2            138821520  2020-03-31   \n",
       "3   962192359059472384  2020-03-31   \n",
       "4            128360620  2020-03-31   \n",
       "\n",
       "                                               tweet  RT_flag  year  month  \\\n",
       "0  RT @k7LssPYI5D85fxI: https://t.co/5KgwBN00NS志村...     True  2020      3   \n",
       "1  RT @84MadokaMary: この国には知らんだけでめちゃめちゃな量の社会保障があるの...     True  2020      3   \n",
       "2  RT @zibumitunari: 明日はエイプリルフールだが、コロナ関連の嘘は絶対にやめよ...     True  2020      3   \n",
       "3  RT @akagiichirou: マスクの７割が中国からの輸入に頼ってきたが流通網が混乱す...     True  2020      3   \n",
       "4  RT @bbcnewsjapan: BBCニュース - ハンガリー政府、新型ウイルス対策で強...     True  2020      3   \n",
       "\n",
       "   day  hour      time  \n",
       "0   31    14  14:59:58  \n",
       "1   31    14  14:59:58  \n",
       "2   31    14  14:59:58  \n",
       "3   31    14  14:59:58  \n",
       "4   31    14  14:59:57  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfrts[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of tweets included RT: 7967482\n"
     ]
    }
   ],
   "source": [
    "c = [len(df) for df in dfors]\n",
    "print('The amount of tweets included RT:', sum(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of tweets included RT: 6692071\n"
     ]
    }
   ],
   "source": [
    "c = [len(df) for df in dfrts]\n",
    "print('The amount of tweets included RT:', sum(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create csv by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_day(dfors, filetype, savedir, key_lang):\n",
    "    # Prepartion of dfw \n",
    "    results = []\n",
    "    dfs = []\n",
    "    dfw = pd.read_csv('../data/WordTimeSeries.csv', encoding='utf-8').rename(columns={'Unnamed: 0': 'types',\n",
    "                                                                                    'file name': 'file_name', \n",
    "                                                                                    'orignal form': 'orignal_form',\n",
    "                                                                                    'English translation': 'English_translation'})\n",
    "    booleanDictionary = {True: 'TRUE', False: 'FALSE'} # Drop nan and False in order to use query \n",
    "    dfw = dfw.replace(booleanDictionary)\n",
    "    dfw = dfw.dropna(how='all')\n",
    "    \n",
    "    # Prepration of keywords\n",
    "    filenames = [filename for filename in dfw.file_name.tolist() if filetype in filename]\n",
    "    xd = {}\n",
    "    xd['T'] = 'file_name.str.contains(\"T\")'\n",
    "    xd['D'] = 'file_name.str.contains(\"D\")'\n",
    "    xd['A'] = 'file_name.str.contains(\"A\")'\n",
    "    xd['V'] = 'file_name.str.contains(\"V\")'\n",
    "    xd['F'] = 'file_name.str.contains(\"F\")'\n",
    "    xd['C'] = 'file_name.str.contains(\"C\")'\n",
    "    \n",
    "    query = xd[filetype]\n",
    "    dfw_c = dfw.query(query, engine='python')\n",
    "    if key_lang == 'jp':\n",
    "        keywords = dfw_c.orignal_form.tolist() # English.ver: keywords = dfw_c.English_translation.tolist() \n",
    "    elif key_lang == 'en':\n",
    "        keywords = dfw_c.English_translation.tolist() \n",
    "    else:\n",
    "        print('Put accurate keyword language')\n",
    "            \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "         # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "        \n",
    "        # Add columns of keywords whose cell have 1 if this tweet includes a keyword \n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        \n",
    "        # Make rows\n",
    "        uni_dates = dfor_c['date'].tolist()\n",
    "        uni_dates = sorted(set(uni_dates), key=uni_dates.index) # date values\n",
    "        rows = []\n",
    "        for date in uni_dates:\n",
    "            d = {}\n",
    "            d['date'] = str(date)\n",
    "            for col in filenames:\n",
    "                _df = dfor_c.groupby('date').get_group(date)\n",
    "                d[col] = _df[col].sum()\n",
    "            rows.append(d)\n",
    "        \n",
    "        # Make cols\n",
    "        cols = filenames.copy()\n",
    "        cols.insert(0, 'date')\n",
    "        \n",
    "        # Make dfs with rows and cols\n",
    "        dft = pd.DataFrame(columns=cols)\n",
    "        for row in rows:\n",
    "            dft = dft.append(row, ignore_index=True) \n",
    "        dfs.append(dft)\n",
    "        \n",
    "    # Finally Connect dfs\n",
    "    for i, df in enumerate(dfs):\n",
    "        if i == 0:\n",
    "            dfnew = df\n",
    "        else:\n",
    "            dfnew = pd.concat([dfnew, df], axis=0)\n",
    "            \n",
    "    # Groupby and sort by date\n",
    "    dfnew = dfnew.groupby('date').sum()\n",
    "    \n",
    "    # Save\n",
    "    outname = filetype+'_original.csv'\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "    savename = os.path.join(outdir, outname)\n",
    "    dfnew.to_csv(savename)\n",
    "\n",
    "    return dfnew\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [03:38<00:00, 21.87s/it]\n",
      "100%|██████████| 10/10 [04:46<00:00, 28.64s/it]\n",
      "100%|██████████| 10/10 [04:46<00:00, 28.65s/it]\n",
      "100%|██████████| 10/10 [01:28<00:00,  8.83s/it]\n",
      "100%|██████████| 10/10 [02:04<00:00, 12.47s/it]\n",
      "100%|██████████| 10/10 [03:31<00:00, 21.18s/it]\n"
     ]
    }
   ],
   "source": [
    "# OR\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfors, filetype=filetype, savedir='../new_results_jst2/orjp/day', key_lang='jp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.09s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.07s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.05s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.04s/it]\n",
      "100%|██████████| 10/10 [00:09<00:00,  1.01it/s]\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "# OR\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfors, filetype=filetype, savedir='../new_results_jst/oren/day', key_lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [01:12<00:00,  7.30s/it]\n",
      "100%|██████████| 10/10 [01:40<00:00, 10.05s/it]\n",
      "100%|██████████| 10/10 [01:52<00:00, 11.21s/it]\n",
      "100%|██████████| 10/10 [00:36<00:00,  3.64s/it]\n",
      "100%|██████████| 10/10 [00:42<00:00,  4.26s/it]\n",
      "100%|██████████| 10/10 [01:46<00:00, 10.68s/it]\n"
     ]
    }
   ],
   "source": [
    "# RT\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfrts, filetype=filetype, savedir='../new_results_jst/rtjp/day', key_lang='jp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.58s/it]\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.49s/it]\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.39s/it]\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.24s/it]\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.59s/it]\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.84s/it]\n"
     ]
    }
   ],
   "source": [
    "# RT\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfrts, filetype=filetype, savedir='../new_results_jst/rten/day', key_lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create csv by hour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_hour(dfors, filetype, savedir, key_lang):\n",
    "    # Prepartion of dfw \n",
    "    results = []\n",
    "    dfs = []\n",
    "    dfw = pd.read_csv('../data/WordTimeSeries.csv', encoding='utf-8').rename(columns={'Unnamed: 0': 'types',\n",
    "                                                                                    'file name': 'file_name', \n",
    "                                                                                    'orignal form': 'orignal_form',\n",
    "                                                                                    'English translation': 'English_translation'})\n",
    "    booleanDictionary = {True: 'TRUE', False: 'FALSE'} # Drop nan and False in order to use query \n",
    "    dfw = dfw.replace(booleanDictionary)\n",
    "    dfw = dfw.dropna(how='all')\n",
    "\n",
    "    # Prepration of keywords\n",
    "    filenames = [filename for filename in dfw.file_name.tolist() if filetype in filename]\n",
    "    xd = {}\n",
    "    xd['T'] = 'file_name.str.contains(\"T\")'\n",
    "    xd['D'] = 'file_name.str.contains(\"D\")'\n",
    "    xd['A'] = 'file_name.str.contains(\"A\")'\n",
    "    xd['V'] = 'file_name.str.contains(\"V\")'\n",
    "    xd['F'] = 'file_name.str.contains(\"F\")'\n",
    "    xd['C'] = 'file_name.str.contains(\"C\")'\n",
    "\n",
    "    query = xd[filetype]\n",
    "    dfw_c = dfw.query(query, engine='python')\n",
    "    \n",
    "    # Keyword \n",
    "    if key_lang == 'jp':\n",
    "        keywords = dfw_c.orignal_form.tolist() # English.ver: keywords = dfw_c.English_translation.tolist() \n",
    "    elif key_lang == 'en':\n",
    "        keywords = dfw_c.English_translation.tolist() \n",
    "    else:\n",
    "        print('Put accurate keyword language')\n",
    "    \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "\n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        dfs.append(dfor_c)\n",
    "        \n",
    "    # Create csv file by day\n",
    "    all_df = []\n",
    "    nextdf, nextdf_date = '', ''\n",
    "    \n",
    "    # Save dir\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        unique_dates = df['date'].tolist()\n",
    "        unique_dates = sorted(set(unique_dates), key=unique_dates.index) # date values\n",
    "        df_date = df.groupby('date') # df grouped by date   \n",
    "\n",
    "        for date in unique_dates:\n",
    "            df_oneday = df_date.get_group(date) # df of one day\n",
    "            dfh = df_oneday.groupby('hour').sum()\n",
    "            dfh = dfh.drop(['year', 'month', 'user_id', 'RT_flag', 'day'],axis=1)\n",
    "            \n",
    "            # Save csv\n",
    "            outname = filetype+str(date)+'.csv'\n",
    "            savename = os.path.join(outdir, outname)\n",
    "            \n",
    "            if os.path.exists(savename):\n",
    "                # File1\n",
    "                existedfile = pd.read_csv(savename)\n",
    "                # いったんcsvにする\n",
    "                _saveonce = 'once.csv'\n",
    "                dfh.to_csv(_saveonce)\n",
    "                # File2 \n",
    "                once = pd.read_csv(_saveonce)\n",
    "                # Concat \n",
    "                df_oneday1 = pd.concat([existedfile, once], axis=0)\n",
    "                # Finally save\n",
    "                dfh = df_oneday1.groupby('hour').sum()\n",
    "                \n",
    "            # Save to csv\n",
    "            dfh.to_csv(savename)\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Orjp, Oren, Rtjp, Rten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [01:26<00:00,  8.68s/it]\n",
      "100%|██████████| 10/10 [01:55<00:00, 11.52s/it]\n",
      "100%|██████████| 10/10 [01:57<00:00, 11.79s/it]\n",
      "100%|██████████| 10/10 [00:37<00:00,  3.77s/it]\n",
      "100%|██████████| 10/10 [00:50<00:00,  5.00s/it]\n",
      "100%|██████████| 10/10 [01:20<00:00,  8.04s/it]\n"
     ]
    }
   ],
   "source": [
    "# Orjp\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfors, filetype=filetype, savedir='../new_results_jst/orjp/hour', key_lang='jp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.01s/it]\n",
      "100%|██████████| 10/10 [00:09<00:00,  1.07it/s]\n",
      "100%|██████████| 10/10 [00:09<00:00,  1.09it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.16it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.16it/s]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# Oren\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfors, filetype=filetype, savedir='../new_results_jst/oren/hour', key_lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [01:02<00:00,  6.29s/it]\n",
      "100%|██████████| 10/10 [01:27<00:00,  8.73s/it]\n",
      "100%|██████████| 10/10 [01:45<00:00, 10.56s/it]\n",
      "100%|██████████| 10/10 [00:33<00:00,  3.38s/it]\n",
      "100%|██████████| 10/10 [00:40<00:00,  4.10s/it]\n",
      "100%|██████████| 10/10 [01:35<00:00,  9.51s/it]\n"
     ]
    }
   ],
   "source": [
    "# Rtjp\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfrts, filetype=filetype, savedir='../new_results_jst/rtjp/hour', key_lang='jp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.32s/it]\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.33s/it]\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.31s/it]\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.14s/it]\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.46s/it]\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.71s/it]\n"
     ]
    }
   ],
   "source": [
    "# Rten\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfrts, filetype=filetype, savedir='../new_results_jst/rten/hour', key_lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7827"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft = pd.read_csv('../new_results_jst/rtjp/hour/T2020-02-28.csv')\n",
    "dft.T1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tT1\n",
    "21\t2958\n",
    "22\t2637\n",
    "23\t2546\n",
    "24\t4883\n",
    "25\t3352\n",
    "26\t2553\n",
    "27\t3206\n",
    "28\t5117\n",
    "29\t7827\n",
    "30\t2925"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コロナ含有率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "963821 tweets conatin \"コロナ\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_corona(dfors, filetype=\"T\"):\n",
    "    # Prepration of keywords\n",
    "    ## コロナ専用\n",
    "    keywords = ['コロナ']\n",
    "    \n",
    "    c = [] # count volume\n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "        \n",
    "        c.append(len(dfor_c))\n",
    "    \n",
    "    print(sum(c), 'tweets conatin \"コロナ\"')\n",
    "\n",
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "findtwt_corona(dfrts, filetype=\"T\")\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corona by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwtCrn_day(dfors, savename, savedir):\n",
    "    \n",
    "    dfs = []\n",
    "\n",
    "    # Keyowrds\n",
    "    keywords = ['コロナ', 'Corona', 'corona']\n",
    "    filenames = ['コロナ', 'Corona', 'corona']\n",
    "\n",
    "    \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet include keywords\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "        \n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        \n",
    "        # Make rows\n",
    "        uni_dates = dfor_c['date'].tolist()\n",
    "        uni_dates = sorted(set(uni_dates), key=uni_dates.index) # date values\n",
    "        rows = []\n",
    "        for date in uni_dates:\n",
    "            d = {}\n",
    "            d['date'] = str(date)\n",
    "            for col in filenames:\n",
    "                _df = dfor_c.groupby('date').get_group(date)\n",
    "                d[col] = _df[col].sum()\n",
    "            rows.append(d)\n",
    "        \n",
    "        # Make cols\n",
    "        cols = filenames.copy()\n",
    "        cols.insert(0, 'date')\n",
    "        \n",
    "        # Make dfs with rows and cols\n",
    "        dft = pd.DataFrame(columns=cols)\n",
    "        for row in rows:\n",
    "            dft = dft.append(row, ignore_index=True)\n",
    "        \n",
    "        dfs.append(dft)\n",
    "\n",
    "    # Finally Connect dfs\n",
    "    for i, df in enumerate(dfs):\n",
    "        if i == 0:\n",
    "            dfnew = df\n",
    "        else:\n",
    "            dfnew = pd.concat([dfnew, df], axis=0)\n",
    "            \n",
    "    # Groupby and sort by date\n",
    "    dfnew = dfnew.groupby('date').sum()\n",
    "    \n",
    "    # Save\n",
    "    outname = savename+'.csv'\n",
    "    outdir = savedir\n",
    "\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "\n",
    "    savename = os.path.join(outdir, outname)    \n",
    "    dfnew.to_csv(savename)\n",
    "\n",
    "    return dfnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "dfnew = findtwtCrn_day(dfors, savename='corona', savedir='../new_results/Corona/or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "dfnew = findtwtCrn_day(dfrts, savename='corona', savedir='../new_results/Corona/rt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corona by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findtwtCrn_hour(dfors, savename, savedir):\n",
    "    # Prepartion \n",
    "    results = []\n",
    "    dfs = []\n",
    "    \n",
    "    # Save dir\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "    \n",
    "    # Keyowrds\n",
    "    keywords = ['コロナ', 'Corona', 'corona']\n",
    "    filenames = ['コロナ', 'Corona', 'corona']\n",
    "    \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "\n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        \n",
    "        dfs.append(dfor_c)\n",
    "        \n",
    "    # Create csv file by day\n",
    "    all_df = []\n",
    "    nextdf, nextdf_date = '', ''\n",
    "    \n",
    "    # Save dir\n",
    "    outdir = savedir\n",
    "    filetype = savename\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        unique_dates = df['date'].tolist()\n",
    "        unique_dates = sorted(set(unique_dates), key=unique_dates.index) # date values\n",
    "        df_date = df.groupby('date') # df grouped by date   \n",
    "        \n",
    "        for date in unique_dates:\n",
    "            df_oneday = df_date.get_group(date) # df of one day\n",
    "            dfh = df_oneday.groupby('hour').sum()\n",
    "            dfh = dfh.drop(['year', 'month', 'user_id', 'RT_flag', 'day'],axis=1)\n",
    "            \n",
    "            outname = filetype+str(date)+'.csv'\n",
    "            savename = os.path.join(outdir, outname)\n",
    "            \n",
    "            if os.path.exists(savename):\n",
    "                # File1\n",
    "                existedfile = pd.read_csv(savename)\n",
    "                \n",
    "                # いったんcsvにする\n",
    "                _saveonce = 'once.csv'\n",
    "                dfh.to_csv(_saveonce)\n",
    "                # File2 \n",
    "                once = pd.read_csv(_saveonce)\n",
    "                \n",
    "                # Concat \n",
    "                df_oneday1 = pd.concat([existedfile, once], axis=0)\n",
    "\n",
    "                # Finally save\n",
    "                dfh = df_oneday1.groupby('hour').sum()\n",
    "                \n",
    "            # Save to csv\n",
    "            dfh.to_csv(savename)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "findtwtCrn_hour(dfors, savename='corona', savedir='../new_results_jst/Corona/or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "findtwtCrn_hour(dfrts, savename='corona', savedir='../new_results_jst/Corona/rt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
