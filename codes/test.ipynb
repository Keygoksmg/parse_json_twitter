{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "from datetime import datetime\n",
    "import os\n",
    "from os import path\n",
    "from glob import glob\n",
    "\n",
    "import ijson\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "zip_dir  = \"../data/zip\"\n",
    "\n",
    "files = glob(os.path.join(zip_dir, '*'))\n",
    "for filename in files:\n",
    "    if '.zip' in filename:\n",
    "        pass\n",
    "    elif '.json' in filename:\n",
    "        pass\n",
    "    else:\n",
    "        newfile = filename+'.json'\n",
    "        os.rename(filename, newfile)\n",
    "\n",
    "jsons = glob(os.path.join(zip_dir, '*.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading json elapsed_time:494.77058386802673[sec]\n"
     ]
    }
   ],
   "source": [
    "# jsons = ['../data/part_v003_o001_r_00000.json', ..., '../data/part_v003_o001_r_00001.json']\n",
    "\n",
    "dfrts = []\n",
    "dfors = []\n",
    "count = 1\n",
    "for i, json in enumerate(jsons):\n",
    "    # Instance Preparation\n",
    "    dates = []\n",
    "    tweets = []\n",
    "    user_ids = []\n",
    "    rts = []\n",
    "    \n",
    "    start = time.time()\n",
    "    # Load jsoten file: date and tweet\n",
    "    with open(json, 'r', encoding='utf8') as file:\n",
    "        pet_parse = ijson.parse(file, multiple_values=True)\n",
    "        for prefix, event, value  in pet_parse:\n",
    "            # Date\n",
    "            if prefix == 'created_at':\n",
    "                dates.append(datetime.strptime(value, '%a %b %d %H:%M:%S +0000 %Y'))\n",
    "            # Tweet \n",
    "            if prefix == 'text':\n",
    "                tweets.append(value.replace('\\n', '').replace(' ', '').replace('\\t', '').replace('　', '')) # Delte space and indet\n",
    "            # User id \n",
    "            if prefix == 'user.id':\n",
    "                user_ids.append(value)\n",
    "            # RT Flag\n",
    "            if len(dates)-1 == len(rts) and prefix == 'retweeted_status':\n",
    "                rts.append(True)\n",
    "            if len(dates)-2 == len(rts):\n",
    "                rts.append(False)\n",
    "                \n",
    "        if len(dates) != len(rts):\n",
    "                rts.append(False)\n",
    "#\n",
    "    print (\"Loading json elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n",
    "    \n",
    "#     print(len(dates), len(tweets), len(user_ids), len(rts))\n",
    "    \n",
    "    data = np.vstack([user_ids, dates, tweets, rts]).T\n",
    "    df = pd.DataFrame(data, columns=['user_id', 'date', 'tweet', 'RT_flag'])\n",
    "    \n",
    "    # Date\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['time'] = df['date'].dt.time\n",
    "    df['date'] = df['date'].dt.date\n",
    "    \n",
    "    # divide df by Original tweet and RT \n",
    "    dfrt = df[df['RT_flag'] == True]\n",
    "    dfor = df[df['RT_flag'] == False]\n",
    "    \n",
    "#     # Save to csv file\n",
    "#     dfor.to_csv('../data/new_dfs2/dfor'+str(i)+'.csv')\n",
    "#     dfrt.to_csv('../data/new_dfs2/dfrt'+str(i)+'.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ CSv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_dir  = \"../data/new_dfs\"\n",
    "csvfiles = glob(os.path.join(zip_dir, 'dfor*'))\n",
    "\n",
    "dfors = []\n",
    "for csvfile in csvfiles:\n",
    "    df = pd.read_csv(csvfile).drop('Unnamed: 0', axis=1)\n",
    "#     df = df.dropna(subset=['tweet'])\n",
    "    dfors.append(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check day data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_day(dfors, filetype, savedir, key_lang):\n",
    "    # Prepartion of dfw \n",
    "    results = []\n",
    "    dfs = []\n",
    "    dfw = pd.read_csv('../data/WordTimeSeries.csv', encoding='utf-8').rename(columns={'Unnamed: 0': 'types',\n",
    "                                                                                    'file name': 'file_name', \n",
    "                                                                                    'orignal form': 'orignal_form',\n",
    "                                                                                    'English translation': 'English_translation'})\n",
    "    booleanDictionary = {True: 'TRUE', False: 'FALSE'} # Drop nan and False in order to use query \n",
    "    dfw = dfw.replace(booleanDictionary)\n",
    "    dfw = dfw.dropna(how='all')\n",
    "    \n",
    "    # Prepration of keywords\n",
    "    filenames = [filename for filename in dfw.file_name.tolist() if filetype in filename]\n",
    "    xd = {}\n",
    "    xd['T'] = 'file_name.str.contains(\"T\")'\n",
    "    xd['D'] = 'file_name.str.contains(\"D\")'\n",
    "    xd['A'] = 'file_name.str.contains(\"A\")'\n",
    "    xd['V'] = 'file_name.str.contains(\"V\")'\n",
    "    xd['F'] = 'file_name.str.contains(\"F\")'\n",
    "    xd['C'] = 'file_name.str.contains(\"C\")'\n",
    "    \n",
    "    query = xd[filetype]\n",
    "    dfw_c = dfw.query(query, engine='python')\n",
    "    if key_lang == 'jp':\n",
    "        keywords = dfw_c.orignal_form.tolist() # English.ver: keywords = dfw_c.English_translation.tolist() \n",
    "    elif key_lang == 'en':\n",
    "        keywords = dfw_c.English_translation.tolist() \n",
    "    else:\n",
    "        print('Put accurate keyword language')\n",
    "            \n",
    "    # Find tweets\n",
    "    for i, dfor in tqdm(enumerate(dfors)):\n",
    "         # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "        \n",
    "        # Add columns of keywords whose cell have 1 if this tweet includes a keyword \n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        \n",
    "        d4 = dfor_c[dfor_c.V4 == 1]\n",
    "        \n",
    "        # Finally Connect dfs\n",
    "        if i == 0:\n",
    "            dfnew = d4\n",
    "        else:\n",
    "            dfnew = pd.concat([dfnew, d4], axis=0)\n",
    "    \n",
    "    \n",
    "    # DF Only V4\n",
    "    d4 = dfnew[dfnew.V4 == 1].drop(['V1', 'V2', 'V3', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20'], axis = 1)\n",
    "    # Remove 'エロい'\n",
    "    d4_without_eroi = d4[~d4['tweet'].str.contains('エロい', na = False)]\n",
    "\n",
    "     # Make rows\n",
    "    uni_dates = d4_without_eroi['date'].tolist()\n",
    "    uni_dates = sorted(set(uni_dates), key=uni_dates.index) # date values\n",
    "    rows = []\n",
    "    for date in uni_dates:\n",
    "        r = d4_without_eroi.groupby('date').get_group(date).V4.sum()\n",
    "        rows.append(r)\n",
    "    data = np.array([uni_dates, rows]).T\n",
    "    # columns\n",
    "    cols = ['date', 'V4']\n",
    "    # Create DataFrame\n",
    "    dft = pd.DataFrame(data, columns=cols).groupby('date').sum()\n",
    "    \n",
    "    # Save to csv\n",
    "    outname = 'V4b.csv'\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "    savename = os.path.join(outdir, outname)\n",
    "    dft.to_csv(savename)\n",
    "            \n",
    "    return dft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "10it [00:44,  4.45s/it]\n"
     ]
    }
   ],
   "source": [
    "# OR\n",
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "filetypes = ['V']\n",
    "for filetype in filetypes:\n",
    "    d4_without_eroi = findtwt_day(dfors, filetype=filetype, savedir='../new_results2/orjp/day', key_lang='jp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89038"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d4_without_eroi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2840"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d4_without_eroi.groupby('date').get_group('2020-03-22').V4.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Make rows\n",
    "uni_dates = d4_without_eroi['date'].tolist()\n",
    "uni_dates = sorted(set(uni_dates), key=uni_dates.index) # date values\n",
    "rows = []\n",
    "for date in uni_dates:\n",
    "    r = d4_without_eroi.groupby('date').get_group(date).V4.sum()\n",
    "    rows.append(r)\n",
    "data = np.array([uni_dates, rows]).T\n",
    "# columns\n",
    "cols = ['date', 'V4']\n",
    "# Create DataFrame\n",
    "dft = pd.DataFrame(data, columns=cols).groupby('date').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['date', 'V4']\n",
    "dft = pd.DataFrame(data, columns=cols)\n",
    "dft.groupby('date').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-02-29</th>\n",
       "      <td>609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01</th>\n",
       "      <td>3065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-02</th>\n",
       "      <td>2909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-03</th>\n",
       "      <td>2862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-04</th>\n",
       "      <td>2699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-05</th>\n",
       "      <td>2726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-06</th>\n",
       "      <td>2656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-07</th>\n",
       "      <td>2648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-08</th>\n",
       "      <td>3142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-09</th>\n",
       "      <td>3242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-10</th>\n",
       "      <td>2903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-11</th>\n",
       "      <td>3102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-12</th>\n",
       "      <td>3102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-13</th>\n",
       "      <td>2722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-14</th>\n",
       "      <td>2696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-15</th>\n",
       "      <td>3045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-16</th>\n",
       "      <td>2724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-17</th>\n",
       "      <td>2917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-18</th>\n",
       "      <td>2853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-19</th>\n",
       "      <td>2711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-20</th>\n",
       "      <td>2413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-21</th>\n",
       "      <td>2405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-22</th>\n",
       "      <td>2840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-23</th>\n",
       "      <td>2845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-24</th>\n",
       "      <td>2801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-25</th>\n",
       "      <td>2757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-26</th>\n",
       "      <td>2892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-27</th>\n",
       "      <td>3001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-28</th>\n",
       "      <td>2674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-29</th>\n",
       "      <td>3448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-30</th>\n",
       "      <td>3236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-31</th>\n",
       "      <td>2393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              V4\n",
       "date            \n",
       "2020-02-29   609\n",
       "2020-03-01  3065\n",
       "2020-03-02  2909\n",
       "2020-03-03  2862\n",
       "2020-03-04  2699\n",
       "2020-03-05  2726\n",
       "2020-03-06  2656\n",
       "2020-03-07  2648\n",
       "2020-03-08  3142\n",
       "2020-03-09  3242\n",
       "2020-03-10  2903\n",
       "2020-03-11  3102\n",
       "2020-03-12  3102\n",
       "2020-03-13  2722\n",
       "2020-03-14  2696\n",
       "2020-03-15  3045\n",
       "2020-03-16  2724\n",
       "2020-03-17  2917\n",
       "2020-03-18  2853\n",
       "2020-03-19  2711\n",
       "2020-03-20  2413\n",
       "2020-03-21  2405\n",
       "2020-03-22  2840\n",
       "2020-03-23  2845\n",
       "2020-03-24  2801\n",
       "2020-03-25  2757\n",
       "2020-03-26  2892\n",
       "2020-03-27  3001\n",
       "2020-03-28  2674\n",
       "2020-03-29  3448\n",
       "2020-03-30  3236\n",
       "2020-03-31  2393"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>RT_flag</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>time</th>\n",
       "      <th>V1</th>\n",
       "      <th>...</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>1107552161649483776</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>3月、確定申告して、確定申告して、確定申告して、確定申告して、BLEACHの大プロジェクト発...</td>\n",
       "      <td>False</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>14:59:25</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>201245935</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>なんと…まだ可能性を残してくれているんだ…!!希望は捨てずに胸の中に大事にしまっておこう✨す...</td>\n",
       "      <td>False</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>14:59:08</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>1007141454102159360</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>カメラを向けられると目線を逸らしたり、手が動いてしまったり。沢山感じていることが多い彼だから...</td>\n",
       "      <td>False</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>14:59:02</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>716926266713051137</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>お祝いで活力が増した気がします。ほんまに今日一日いろいろありがとうございました😊わいは明日か...</td>\n",
       "      <td>False</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>14:58:55</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>961735156577878016</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>去年の一年で得た学び→「『〇〇公式供給のおかげで現実生きれる😂』的言説は多いが、いかにオタク...</td>\n",
       "      <td>False</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>14:58:54</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 user_id        date  \\\n",
       "270  1107552161649483776  2020-03-31   \n",
       "382            201245935  2020-03-31   \n",
       "429  1007141454102159360  2020-03-31   \n",
       "482   716926266713051137  2020-03-31   \n",
       "488   961735156577878016  2020-03-31   \n",
       "\n",
       "                                                 tweet  RT_flag  year  month  \\\n",
       "270  3月、確定申告して、確定申告して、確定申告して、確定申告して、BLEACHの大プロジェクト発...    False  2020      3   \n",
       "382  なんと…まだ可能性を残してくれているんだ…!!希望は捨てずに胸の中に大事にしまっておこう✨す...    False  2020      3   \n",
       "429  カメラを向けられると目線を逸らしたり、手が動いてしまったり。沢山感じていることが多い彼だから...    False  2020      3   \n",
       "482  お祝いで活力が増した気がします。ほんまに今日一日いろいろありがとうございました😊わいは明日か...    False  2020      3   \n",
       "488  去年の一年で得た学び→「『〇〇公式供給のおかげで現実生きれる😂』的言説は多いが、いかにオタク...    False  2020      3   \n",
       "\n",
       "     day  hour      time  V1  ...  V11  V12  V13  V14  V15  V16  V17  V18  \\\n",
       "270   31    14  14:59:25   0  ...    0    0    0    0    0    0    0    0   \n",
       "382   31    14  14:59:08   0  ...    0    0    0    0    0    0    0    0   \n",
       "429   31    14  14:59:02   0  ...    0    0    0    0    0    0    0    0   \n",
       "482   31    14  14:58:55   0  ...    0    0    0    0    0    0    0    0   \n",
       "488   31    14  14:58:54   0  ...    0    0    0    0    0    0    0    0   \n",
       "\n",
       "     V19  V20  \n",
       "270    0    0  \n",
       "382    0    0  \n",
       "429    0    0  \n",
       "482    0    0  \n",
       "488    0    0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnew.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4 = dfnew[dfnew.V4 == 1].drop(['V1', 'V2', 'V3', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20'], axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2020-03-15    11510\n",
       "2020-03-29     3449\n",
       "2020-03-09     3246\n",
       "2020-03-30     3238\n",
       "2020-03-08     3144\n",
       "2020-03-12     3103\n",
       "2020-03-11     3102\n",
       "2020-03-01     3067\n",
       "2020-03-27     3001\n",
       "2020-03-17     2919\n",
       "2020-03-02     2909\n",
       "2020-03-10     2904\n",
       "2020-03-26     2893\n",
       "2020-03-03     2864\n",
       "2020-03-18     2853\n",
       "2020-03-23     2846\n",
       "2020-03-22     2840\n",
       "2020-03-24     2802\n",
       "2020-03-25     2758\n",
       "2020-03-05     2726\n",
       "2020-03-16     2724\n",
       "2020-03-13     2723\n",
       "2020-03-19     2711\n",
       "2020-03-04     2700\n",
       "2020-03-14     2696\n",
       "2020-03-28     2674\n",
       "2020-03-06     2657\n",
       "2020-03-07     2649\n",
       "2020-03-20     2413\n",
       "2020-03-21     2406\n",
       "2020-03-31     2393\n",
       "2020-02-29      609\n",
       "Name: date, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d4.date.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tweets: 609\n",
      "Unique tweets: 608\n",
      "Unique twt rate 0.9983579638752053\n"
     ]
    }
   ],
   "source": [
    "# 3-15\n",
    "d315 = d4[d4.date == '2020-02-29']\n",
    "print('total tweets:', len(d315))\n",
    "twtset = set(d315.tweet)\n",
    "print('Unique tweets:', len(twtset))\n",
    "print('Unique twt rate', len(twtset)/len(d315))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tweets: 2724\n",
      "Unique tweets: 2596\n",
      "Unique twt  rate 0.9530102790014684\n"
     ]
    }
   ],
   "source": [
    "# # 3-16(other days)\n",
    "# d316= d4[d4.date == '2020-03-16']\n",
    "# print('total tweets:', len(d316))\n",
    "# twtset = set(d316.tweet)\n",
    "# print('Unique tweets:', len(twtset))\n",
    "# print('Unique twt  rate', len(twtset)/len(d316))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'エロい'\n",
    "d315_wo_eroi = d315[~d315['tweet'].str.contains('エロい', na = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tweets: 11510\n",
      "Unique tweets: 3010\n",
      "Unique twt rate 0.9885057471264368\n"
     ]
    }
   ],
   "source": [
    "# 3-15 new \n",
    "print('total tweets:', len(d315))\n",
    "twtset_new = set(d315_wo_eroi.tweet)\n",
    "print('Unique tweets:', len(twtset_new))\n",
    "print('Unique twt rate', len(twtset_new)/len(d315_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V4の処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read hour csv file\n",
    "zip_dir  = \"../new_results/rtjp/hour\"\n",
    "csvfiles = glob(os.path.join(zip_dir, 'T*'))\n",
    "\n",
    "thours = []\n",
    "for csvfile in csvfiles:\n",
    "    t = pd.read_csv(csvfile)\n",
    "#     df = pd.read_csv(csvfile).drop('Unnamed: 0', axis=1)\n",
    "#     df = df.dropna(subset=['tweet'])\n",
    "    thours.append(t)\n",
    "    \n",
    "# date list\n",
    "dates = [csvfile.split('/')[-1].split('.')[0][1:] for csvfile in csvfiles] # ['2020-01-31', '2020-02-01', '2020-02-02', '2020-02-03', '2020-02-04']\n",
    "\n",
    "# day csv\n",
    "tday = pd.read_csv(\"../new_results/rtjp/day/T_original.csv\").set_index('date')\n",
    "\n",
    "# cols\n",
    "cols = list(tday.columns) # ['T1', 'T2', 'T3', 'T4', 'T5']\n",
    "\n",
    "# 一致をチェック\n",
    "for t, date in zip(thours, dates):\n",
    "    for col in cols:\n",
    "        if t[col].sum() == tday.loc[date, col]:\n",
    "            pass\n",
    "        else:\n",
    "            print(date, col, t[col].sum() - tday.loc[date, col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_dir  = \"../data/new_dfs2\"\n",
    "csvfiles = glob(os.path.join(zip_dir, 'dfor*'))\n",
    "\n",
    "dfors = []\n",
    "for csvfile in csvfiles:\n",
    "    df = pd.read_csv(csvfile).drop('Unnamed: 0', axis=1)\n",
    "#     df = df.dropna(subset=['tweet'])\n",
    "    dfors.append(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = pd.read_csv('../new_results/orjp/day/A_original.csv')\n",
    "A2 = pd.read_csv('../new_results_jst/orjp/day/A_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1.A4.sum() == A2.A4.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
