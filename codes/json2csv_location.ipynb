{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "from datetime import datetime\n",
    "import os\n",
    "from os import path\n",
    "from glob import glob\n",
    "\n",
    "import ijson\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_dir  = \"../data/zip\"\n",
    "\n",
    "files = glob(os.path.join(zip_dir, '*'))\n",
    "for filename in files:\n",
    "    if '.zip' in filename:\n",
    "        pass\n",
    "    elif '.json' in filename:\n",
    "        pass\n",
    "    else:\n",
    "        newfile = filename+'.json'\n",
    "        os.rename(filename, newfile)\n",
    "\n",
    "jsons = glob(os.path.join(zip_dir, '*.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/zip/part-v003-o001-r-00000.json',\n",
       " '../data/zip/part-v003-o001-r-00001.json',\n",
       " '../data/zip/part-v003-o001-r-00002.json',\n",
       " '../data/zip/part-v003-o001-r-00003.json',\n",
       " '../data/zip/part-v003-o001-r-00004.json',\n",
       " '../data/zip/part-v003-o001-r-00005.json',\n",
       " '../data/zip/part-v003-o001-r-00006.json',\n",
       " '../data/zip/part-v003-o001-r-00007.json',\n",
       " '../data/zip/part-v003-o001-r-00008.json',\n",
       " '../data/zip/part-v003-o001-r-00009.json']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jsons = jsons[:2]\n",
    "jsons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# jsons = ['../data/part_v003_o001_r_00000.json', ..., '../data/part_v003_o001_r_00001.json']\n",
    "\n",
    "dfrts = []\n",
    "dfors = []\n",
    "for i, json in enumerate(jsons):\n",
    "    # Instance Preparation\n",
    "    dates = []\n",
    "    tweets = []\n",
    "    user_ids = []\n",
    "    rts = []\n",
    "\n",
    "    # tweet's location\n",
    "    place_tweet = []\n",
    "    # tweet's country\n",
    "    nationality = []\n",
    "    \n",
    "    i = 0 \n",
    "    start = time.time()\n",
    "    # Load json file: date and tweet\n",
    "    with open(json, 'r', encoding='utf8') as file:\n",
    "        pet_parse = ijson.parse(file, multiple_values=True)\n",
    "        for prefix, event, value  in pet_parse:\n",
    "            # Date\n",
    "            if prefix == 'created_at':\n",
    "                dates.append(datetime.strptime(value, '%a %b %d %H:%M:%S +0000 %Y'))\n",
    "            # Tweet \n",
    "            if prefix == 'text':\n",
    "                tweets.append(value.replace('\\n', '').replace('\\t', '').replace('\\r', '').replace('\\r\\n', '').replace('　', '')) # Delte space and indet and \\r\n",
    "            # User id \n",
    "            if prefix == 'user.id':\n",
    "                user_ids.append(value)\n",
    "            # RT Flag\n",
    "            if len(dates)-1 == len(rts) and prefix == 'retweeted_status':\n",
    "                rts.append(True)\n",
    "            if len(dates)-2 == len(rts):\n",
    "                rts.append(False)\n",
    "            \n",
    "            # tweet's location\n",
    "            if len(dates)-1 == len(place_tweet) and prefix == 'place.name':\n",
    "                _name = [value]\n",
    "                place_tweet.append(_name[-1])\n",
    "            if len(dates)-2 == len(place_tweet):\n",
    "                place_tweet.append('nan')\n",
    "                \n",
    "            # tweet's nationality\n",
    "            if prefix == 'place.country':\n",
    "                i += 1\n",
    "                _natioanl = [value]\n",
    "                nationality.append(_natioanl[-1])\n",
    "            if len(dates)-2 == len(nationality):\n",
    "                nationality.append('nan')\n",
    "\n",
    "        # Add missing data\n",
    "        if len(dates) != len(rts):\n",
    "                rts.append(False)\n",
    "        if len(dates) != len(place_tweet):\n",
    "                place_tweet.append(False)\n",
    "        if len(dates) != len(place_bio):\n",
    "                place_bio.append(False)\n",
    "        if len(dates) != len(nationality):\n",
    "                nationality.append(False)\n",
    "\n",
    "    print(\"Loading json elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n",
    "    \n",
    "    print(len(dates), len(tweets), len(user_ids), len(rts), len(nationality), len(place_tweet))\n",
    "    \n",
    "    # Create dataframe\n",
    "    data = np.vstack([user_ids, dates, tweets, rts, nationality, place_tweet, place_bio]).T\n",
    "    df = pd.DataFrame(data, columns=['user_id', 'date', 'tweet', 'RT_flag', 'nationality', 'place_tweet', 'place_bio'])\n",
    "    \n",
    "    import datetime as dt\n",
    "    \n",
    "    # Df only with location data\n",
    "    df = df[df.place_tweet != 'nan']\n",
    "    \n",
    "    # Change time zone\n",
    "    df['date'] = df['date'] + dt.timedelta(hours = 9)\n",
    "    \n",
    "    # Date\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['time'] = df['date'].dt.time\n",
    "    df['date'] = df['date'].dt.date\n",
    "    \n",
    "    # divide df by RT_flag \n",
    "    dfrt = df[df['RT_flag'] == True]\n",
    "    dfor = df[df['RT_flag'] == False]\n",
    "    \n",
    "    # Save to csv file\n",
    "    dfor.to_csv('../data/dfs_location/dfor_lct'+str(i)+'.csv')\n",
    "    dfrt.to_csv('../data/dfs_location/dfrt_lct'+str(i)+'.csv')\n",
    "    break\n",
    "    \n",
    "    # Append to dfs\n",
    "    dfrts.append(dfrt)\n",
    "    dfors.append(dfor)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### ***************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '日本',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '日本',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '日本']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nationality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>RT_flag</th>\n",
       "      <th>nationality</th>\n",
       "      <th>place_tweet</th>\n",
       "      <th>place_bio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>43809995</td>\n",
       "      <td>2020-02-29 14:59:52</td>\n",
       "      <td>@rihito_karube お忙しいのにお返事ありがとうございますそろそろ社内で政治部とガ...</td>\n",
       "      <td>False</td>\n",
       "      <td>日本</td>\n",
       "      <td>船橋市</td>\n",
       "      <td>千葉 船橋市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>949838786287906817</td>\n",
       "      <td>2020-02-29 14:58:47</td>\n",
       "      <td>バカは言い過ぎですが、なんの根拠もないデマに踊らさせて…こういうのを付和雷同って言います。ち...</td>\n",
       "      <td>False</td>\n",
       "      <td>日本</td>\n",
       "      <td>米子市</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>925958009381036032</td>\n",
       "      <td>2020-02-29 14:58:41</td>\n",
       "      <td>@piyo_0126 GPS撮影会の井の頭公園の桜撮影🌸に申し込みました。まだ一度も抽選に当...</td>\n",
       "      <td>False</td>\n",
       "      <td>日本</td>\n",
       "      <td>世田谷区</td>\n",
       "      <td>東京 港区</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                user_id                date  \\\n",
       "89             43809995 2020-02-29 14:59:52   \n",
       "901  949838786287906817 2020-02-29 14:58:47   \n",
       "989  925958009381036032 2020-02-29 14:58:41   \n",
       "\n",
       "                                                 tweet RT_flag nationality  \\\n",
       "89   @rihito_karube お忙しいのにお返事ありがとうございますそろそろ社内で政治部とガ...   False          日本   \n",
       "901  バカは言い過ぎですが、なんの根拠もないデマに踊らさせて…こういうのを付和雷同って言います。ち...   False          日本   \n",
       "989  @piyo_0126 GPS撮影会の井の頭公園の桜撮影🌸に申し込みました。まだ一度も抽選に当...   False          日本   \n",
       "\n",
       "    place_tweet place_bio  \n",
       "89          船橋市    千葉 船橋市  \n",
       "901         米子市      None  \n",
       "989        世田谷区     東京 港区  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### ***************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|            Variable Name|    Memory|\n",
      " ------------------------------------ \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"{}{: >25}{}{: >10}{}\".format('|','Variable Name','|','Memory','|'))\n",
    "print(\" ------------------------------------ \")\n",
    "for var_name in dir():\n",
    "    if not var_name.startswith(\"_\") and sys.getsizeof(eval(var_name)) > 10000: #ここだけアレンジ\n",
    "        print(\"{}{: >25}{}{: >10}{}\".format('|',var_name,'|',sys.getsizeof(eval(var_name)),'|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dfors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read json dfs from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ORs\n",
    "zip_dir  = \"../data/new_dfs_jst\" # new_dfs2_jst\n",
    "csvfiles = glob(os.path.join(zip_dir, 'dfor*'))\n",
    "\n",
    "dfors = []\n",
    "for csvfile in csvfiles:\n",
    "    df = pd.read_csv(csvfile).drop('Unnamed: 0', axis=1)\n",
    "    dfors.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Rts\n",
    "zip_dir  = \"../data/new_dfs2_jst\"\n",
    "csvfiles = glob(os.path.join(zip_dir, 'dfor*'))\n",
    "\n",
    "dfors = []\n",
    "for csvfile in csvfiles:\n",
    "    df1 = pd.read_csv(csvfile).drop('Unnamed: 0', axis=1)\n",
    "    dfors.append(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n"
     ]
    }
   ],
   "source": [
    "for df in dfors:\n",
    "    print(df.RT_flag.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n"
     ]
    }
   ],
   "source": [
    "for df in dfrts:\n",
    "    print(df.RT_flag.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfrts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of tweets included RT: 9286476\n"
     ]
    }
   ],
   "source": [
    "c = [len(df) for df in dfors]\n",
    "print('The amount of tweets included RT:', sum(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of tweets included RT: 6692071\n"
     ]
    }
   ],
   "source": [
    "c = [len(df) for df in dfrts]\n",
    "print('The amount of tweets included RT:', sum(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create csv by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_day(dfors, filetype, savedir, key_lang):\n",
    "    # Prepartion of dfw \n",
    "    results = []\n",
    "    dfs = []\n",
    "    dfw = pd.read_csv('../data/WordTimeSeries.csv', encoding='utf-8').rename(columns={'Unnamed: 0': 'types',\n",
    "                                                                                    'file name': 'file_name', \n",
    "                                                                                    'orignal form': 'orignal_form',\n",
    "                                                                                    'English translation': 'English_translation'})\n",
    "    booleanDictionary = {True: 'TRUE', False: 'FALSE'} # Drop nan and False in order to use query \n",
    "    dfw = dfw.replace(booleanDictionary)\n",
    "    dfw = dfw.dropna(how='all')\n",
    "    \n",
    "    # Prepration of keywords\n",
    "    filenames = [filename for filename in dfw.file_name.tolist() if filetype in filename]\n",
    "    xd = {}\n",
    "    xd['T'] = 'file_name.str.contains(\"T\")'\n",
    "    xd['D'] = 'file_name.str.contains(\"D\")'\n",
    "    xd['A'] = 'file_name.str.contains(\"A\")'\n",
    "    xd['V'] = 'file_name.str.contains(\"V\")'\n",
    "    xd['F'] = 'file_name.str.contains(\"F\")'\n",
    "    xd['C'] = 'file_name.str.contains(\"C\")'\n",
    "    \n",
    "    query = xd[filetype]\n",
    "    dfw_c = dfw.query(query, engine='python')\n",
    "    if key_lang == 'jp':\n",
    "        keywords = dfw_c.orignal_form.tolist() # English.ver: keywords = dfw_c.English_translation.tolist() \n",
    "    elif key_lang == 'en':\n",
    "        keywords = dfw_c.English_translation.tolist() \n",
    "    else:\n",
    "        print('Put accurate keyword language')\n",
    "            \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "         # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "        \n",
    "        # Add columns of keywords whose cell have 1 if this tweet includes a keyword \n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        \n",
    "        # Make rows\n",
    "        uni_dates = dfor_c['date'].tolist()\n",
    "        uni_dates = sorted(set(uni_dates), key=uni_dates.index) # date values\n",
    "        rows = []\n",
    "        for date in uni_dates:\n",
    "            d = {}\n",
    "            d['date'] = str(date)\n",
    "            for col in filenames:\n",
    "                _df = dfor_c.groupby('date').get_group(date)\n",
    "                d[col] = _df[col].sum()\n",
    "            rows.append(d)\n",
    "        \n",
    "        # Make cols\n",
    "        cols = filenames.copy()\n",
    "        cols.insert(0, 'date')\n",
    "        \n",
    "        # Make dfs with rows and cols\n",
    "        dft = pd.DataFrame(columns=cols)\n",
    "        for row in rows:\n",
    "            dft = dft.append(row, ignore_index=True) \n",
    "        dfs.append(dft)\n",
    "        \n",
    "    # Finally Connect dfs\n",
    "    for i, df in enumerate(dfs):\n",
    "        if i == 0:\n",
    "            dfnew = df\n",
    "        else:\n",
    "            dfnew = pd.concat([dfnew, df], axis=0)\n",
    "            \n",
    "    # Groupby and sort by date\n",
    "    dfnew = dfnew.groupby('date').sum()\n",
    "    \n",
    "    # Save\n",
    "    outname = filetype+'_original.csv'\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "    savename = os.path.join(outdir, outname)\n",
    "    dfnew.to_csv(savename)\n",
    "\n",
    "    return dfnew\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [01:38<00:00,  9.81s/it]\n",
      "100%|██████████| 10/10 [02:01<00:00, 12.16s/it]\n",
      "100%|██████████| 10/10 [02:09<00:00, 12.97s/it]\n",
      "100%|██████████| 10/10 [00:37<00:00,  3.78s/it]\n",
      "100%|██████████| 10/10 [00:54<00:00,  5.48s/it]\n",
      "100%|██████████| 10/10 [01:29<00:00,  8.92s/it]\n"
     ]
    }
   ],
   "source": [
    "# OR\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfors, filetype=filetype, savedir='../new_results_jst/orjp/day', key_lang='jp') # new_results_jst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:21<00:00,  2.10s/it]\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.27s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.03s/it]\n",
      "100%|██████████| 10/10 [00:09<00:00,  1.07it/s]\n",
      "100%|██████████| 10/10 [00:09<00:00,  1.05it/s]\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "# OR\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfors, filetype=filetype, savedir='../new_results_jst/oren/day', key_lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [03:25<00:00, 20.53s/it]\n",
      "100%|██████████| 10/10 [03:34<00:00, 21.48s/it]\n",
      "100%|██████████| 10/10 [03:20<00:00, 20.05s/it]\n",
      "100%|██████████| 10/10 [01:41<00:00, 10.17s/it]\n",
      "100%|██████████| 10/10 [01:28<00:00,  8.88s/it]\n",
      "100%|██████████| 10/10 [03:53<00:00, 23.33s/it]\n"
     ]
    }
   ],
   "source": [
    "# RT\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfrts, filetype=filetype, savedir='../new_results_jst/rtjp/day', key_lang='jp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:19<00:00,  1.91s/it]\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.51s/it]\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.33s/it]\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.18s/it]\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.52s/it]\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.74s/it]\n"
     ]
    }
   ],
   "source": [
    "# RT\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfrts, filetype=filetype, savedir='../new_results_jst/rten/day', key_lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create csv by hour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_hour(dfors, filetype, savedir, key_lang):\n",
    "    # Prepartion of dfw \n",
    "    results = []\n",
    "    dfs = []\n",
    "    dfw = pd.read_csv('../data/WordTimeSeries.csv', encoding='utf-8').rename(columns={'Unnamed: 0': 'types',\n",
    "                                                                                    'file name': 'file_name', \n",
    "                                                                                    'orignal form': 'orignal_form',\n",
    "                                                                                    'English translation': 'English_translation'})\n",
    "    booleanDictionary = {True: 'TRUE', False: 'FALSE'} # Drop nan and False in order to use query \n",
    "    dfw = dfw.replace(booleanDictionary)\n",
    "    dfw = dfw.dropna(how='all')\n",
    "\n",
    "    # Prepration of keywords\n",
    "    filenames = [filename for filename in dfw.file_name.tolist() if filetype in filename]\n",
    "    xd = {}\n",
    "    xd['T'] = 'file_name.str.contains(\"T\")'\n",
    "    xd['D'] = 'file_name.str.contains(\"D\")'\n",
    "    xd['A'] = 'file_name.str.contains(\"A\")'\n",
    "    xd['V'] = 'file_name.str.contains(\"V\")'\n",
    "    xd['F'] = 'file_name.str.contains(\"F\")'\n",
    "    xd['C'] = 'file_name.str.contains(\"C\")'\n",
    "\n",
    "    query = xd[filetype]\n",
    "    dfw_c = dfw.query(query, engine='python')\n",
    "    \n",
    "    # Keyword \n",
    "    if key_lang == 'jp':\n",
    "        keywords = dfw_c.orignal_form.tolist() # English.ver: keywords = dfw_c.English_translation.tolist() \n",
    "    elif key_lang == 'en':\n",
    "        keywords = dfw_c.English_translation.tolist() \n",
    "    else:\n",
    "        print('Put accurate keyword language')\n",
    "    \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "\n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        dfs.append(dfor_c)\n",
    "        \n",
    "    # Create csv file by day\n",
    "    all_df = []\n",
    "    nextdf, nextdf_date = '', ''\n",
    "    \n",
    "    # Save dir\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        unique_dates = df['date'].tolist()\n",
    "        unique_dates = sorted(set(unique_dates), key=unique_dates.index) # date values\n",
    "        df_date = df.groupby('date') # df grouped by date   \n",
    "\n",
    "        for date in unique_dates:\n",
    "            df_oneday = df_date.get_group(date) # df of one day\n",
    "            dfh = df_oneday.groupby('hour').sum()\n",
    "            dfh = dfh.drop(['year', 'month', 'user_id', 'RT_flag', 'day'],axis=1)\n",
    "            \n",
    "            # Save csv\n",
    "            outname = filetype+str(date)+'.csv'\n",
    "            savename = os.path.join(outdir, outname)\n",
    "            \n",
    "            if os.path.exists(savename):\n",
    "                # File1\n",
    "                existedfile = pd.read_csv(savename)\n",
    "                # いったんcsvにする\n",
    "                _saveonce = 'once.csv'\n",
    "                dfh.to_csv(_saveonce)\n",
    "                # File2 \n",
    "                once = pd.read_csv(_saveonce)\n",
    "                # Concat \n",
    "                df_oneday1 = pd.concat([existedfile, once], axis=0)\n",
    "                # Finally save\n",
    "                dfh = df_oneday1.groupby('hour').sum()\n",
    "                \n",
    "            # Save to csv\n",
    "            dfh.to_csv(savename)\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Orjp, Oren, Rtjp, Rten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [01:24<00:00,  8.47s/it]\n",
      "100%|██████████| 10/10 [02:05<00:00, 12.57s/it]\n",
      "100%|██████████| 10/10 [02:12<00:00, 13.22s/it]\n",
      "100%|██████████| 10/10 [00:43<00:00,  4.33s/it]\n",
      "100%|██████████| 10/10 [00:54<00:00,  5.50s/it]\n",
      "100%|██████████| 10/10 [01:31<00:00,  9.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# Orjp\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfors, filetype=filetype, savedir='../new_results_jst2/orjp/hour', key_lang='jp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.57s/it]\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.26s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.06s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.01s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.02s/it]\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "# Oren\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfors, filetype=filetype, savedir='../new_results_jst2/oren/hour', key_lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [01:25<00:00,  8.57s/it]\n",
      "100%|██████████| 10/10 [01:31<00:00,  9.10s/it]\n",
      "100%|██████████| 10/10 [01:42<00:00, 10.22s/it]\n",
      "100%|██████████| 10/10 [00:38<00:00,  3.90s/it]\n",
      "100%|██████████| 10/10 [01:05<00:00,  6.53s/it]\n",
      "100%|██████████| 10/10 [01:56<00:00, 11.65s/it]\n"
     ]
    }
   ],
   "source": [
    "# Rtjp\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfrts, filetype=filetype, savedir='../new_results_jst2/rtjp/hour', key_lang='jp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.37s/it]\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.46s/it]\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.55s/it]\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.20s/it]\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.23s/it]\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "# Rten\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfrts, filetype=filetype, savedir='../new_results_jst2/rten/hour', key_lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7827"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft = pd.read_csv('../results/rtjp/hour/T2020-02-28.csv')\n",
    "dft.T1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tT1\n",
    "21\t2958\n",
    "22\t2637\n",
    "23\t2546\n",
    "24\t4883\n",
    "25\t3352\n",
    "26\t2553\n",
    "27\t3206\n",
    "28\t5117\n",
    "29\t7827\n",
    "30\t2925"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コロナ含有率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "963821 tweets conatin \"コロナ\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_corona(dfors, filetype=\"T\"):\n",
    "    # Prepration of keywords\n",
    "    ## コロナ専用\n",
    "    keywords = ['コロナ']\n",
    "    \n",
    "    c = [] # count volume\n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "        \n",
    "        c.append(len(dfor_c))\n",
    "    \n",
    "    print(sum(c), 'tweets conatin \"コロナ\"')\n",
    "\n",
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "findtwt_corona(dfrts, filetype=\"T\")\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corona by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwtCrn_day(dfors, savename, savedir):\n",
    "    \n",
    "    dfs = []\n",
    "\n",
    "    # Keyowrds\n",
    "    keywords = ['コロナ', 'Corona', 'corona']\n",
    "    filenames = ['コロナ', 'Corona', 'corona']\n",
    "\n",
    "    \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet include keywords\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "        \n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        \n",
    "        # Make rows\n",
    "        uni_dates = dfor_c['date'].tolist()\n",
    "        uni_dates = sorted(set(uni_dates), key=uni_dates.index) # date values\n",
    "        rows = []\n",
    "        for date in uni_dates:\n",
    "            d = {}\n",
    "            d['date'] = str(date)\n",
    "            for col in filenames:\n",
    "                _df = dfor_c.groupby('date').get_group(date)\n",
    "                d[col] = _df[col].sum()\n",
    "            rows.append(d)\n",
    "        \n",
    "        # Make cols\n",
    "        cols = filenames.copy()\n",
    "        cols.insert(0, 'date')\n",
    "        \n",
    "        # Make dfs with rows and cols\n",
    "        dft = pd.DataFrame(columns=cols)\n",
    "        for row in rows:\n",
    "            dft = dft.append(row, ignore_index=True)\n",
    "        \n",
    "        dfs.append(dft)\n",
    "\n",
    "    # Finally Connect dfs\n",
    "    for i, df in enumerate(dfs):\n",
    "        if i == 0:\n",
    "            dfnew = df\n",
    "        else:\n",
    "            dfnew = pd.concat([dfnew, df], axis=0)\n",
    "            \n",
    "    # Groupby and sort by date\n",
    "    dfnew = dfnew.groupby('date').sum()\n",
    "    \n",
    "    # Save\n",
    "    outname = savename+'.csv'\n",
    "    outdir = savedir\n",
    "\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "\n",
    "    savename = os.path.join(outdir, outname)    \n",
    "    dfnew.to_csv(savename)\n",
    "\n",
    "    return dfnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "dfnew = findtwtCrn_day(dfors, savename='corona', savedir='../new_results_jst/Corona/or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "dfnew = findtwtCrn_day(dfrts, savename='corona', savedir='../new_results_jst2/Corona/rt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corona by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findtwtCrn_hour(dfors, savename, savedir):\n",
    "    # Prepartion \n",
    "    results = []\n",
    "    dfs = []\n",
    "    \n",
    "    # Save dir\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "    \n",
    "    # Keyowrds\n",
    "    keywords = ['コロナ', 'Corona', 'corona']\n",
    "    filenames = ['コロナ', 'Corona', 'corona']\n",
    "    \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "\n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        \n",
    "        dfs.append(dfor_c)\n",
    "        \n",
    "    # Create csv file by day\n",
    "    all_df = []\n",
    "    nextdf, nextdf_date = '', ''\n",
    "    \n",
    "    # Save dir\n",
    "    outdir = savedir\n",
    "    filetype = savename\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        unique_dates = df['date'].tolist()\n",
    "        unique_dates = sorted(set(unique_dates), key=unique_dates.index) # date values\n",
    "        df_date = df.groupby('date') # df grouped by date   \n",
    "        \n",
    "        for date in unique_dates:\n",
    "            df_oneday = df_date.get_group(date) # df of one day\n",
    "            dfh = df_oneday.groupby('hour').sum()\n",
    "            dfh = dfh.drop(['year', 'month', 'user_id', 'RT_flag', 'day'],axis=1)\n",
    "            \n",
    "            outname = filetype+str(date)+'.csv'\n",
    "            savename = os.path.join(outdir, outname)\n",
    "            \n",
    "            if os.path.exists(savename):\n",
    "                # File1\n",
    "                existedfile = pd.read_csv(savename)\n",
    "                \n",
    "                # いったんcsvにする\n",
    "                _saveonce = 'once.csv'\n",
    "                dfh.to_csv(_saveonce)\n",
    "                # File2 \n",
    "                once = pd.read_csv(_saveonce)\n",
    "                \n",
    "                # Concat \n",
    "                df_oneday1 = pd.concat([existedfile, once], axis=0)\n",
    "\n",
    "                # Finally save\n",
    "                dfh = df_oneday1.groupby('hour').sum()\n",
    "                \n",
    "            # Save to csv\n",
    "            dfh.to_csv(savename)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:09<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "findtwtCrn_hour(dfors, savename='corona', savedir='../new_results_jst2/Corona/or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "findtwtCrn_hour(dfrts, savename='corona', savedir='../new_results_jst/Corona/rt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_csv('../new_results_jst/rten/hour/V2020-02-01.csv')\n",
    "t2 = pd.read_csv('../new_results/rten/hour/V2020-01-31.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "print(t1.loc[0:8,'V4'].sum(), t2.V4.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>RT_flag</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1111855071380168705</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @k7LssPYI5D85fxI: https://t.co/5KgwBN00NS志村...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127471475</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @84MadokaMary: この国には知らんだけでめちゃめちゃな量の社会保障があるの...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138821520</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @zibumitunari: 明日はエイプリルフールだが、コロナ関連の嘘は絶対にやめよ...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>962192359059472384</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @akagiichirou: マスクの７割が中国からの輸入に頼ってきたが流通網が混乱す...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128360620</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @bbcnewsjapan: BBCニュース - ハンガリー政府、新型ウイルス対策で強...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user_id        date  \\\n",
       "0  1111855071380168705  2020-03-31   \n",
       "1            127471475  2020-03-31   \n",
       "2            138821520  2020-03-31   \n",
       "3   962192359059472384  2020-03-31   \n",
       "4            128360620  2020-03-31   \n",
       "\n",
       "                                               tweet  RT_flag  year  month  \\\n",
       "0  RT @k7LssPYI5D85fxI: https://t.co/5KgwBN00NS志村...     True  2020      3   \n",
       "1  RT @84MadokaMary: この国には知らんだけでめちゃめちゃな量の社会保障があるの...     True  2020      3   \n",
       "2  RT @zibumitunari: 明日はエイプリルフールだが、コロナ関連の嘘は絶対にやめよ...     True  2020      3   \n",
       "3  RT @akagiichirou: マスクの７割が中国からの輸入に頼ってきたが流通網が混乱す...     True  2020      3   \n",
       "4  RT @bbcnewsjapan: BBCニュース - ハンガリー政府、新型ウイルス対策で強...     True  2020      3   \n",
       "\n",
       "   day  hour      time  \n",
       "0   31    23  23:59:58  \n",
       "1   31    23  23:59:58  \n",
       "2   31    23  23:59:58  \n",
       "3   31    23  23:59:58  \n",
       "4   31    23  23:59:57  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfrts[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
