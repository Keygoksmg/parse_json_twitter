{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "from datetime import datetime\n",
    "import os\n",
    "from os import path\n",
    "from glob import glob\n",
    "\n",
    "import ijson\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_dir  = \"../data/zip2\"\n",
    "\n",
    "files = glob(os.path.join(zip_dir, '*'))\n",
    "for filename in files:\n",
    "    if '.zip' in filename:\n",
    "        pass\n",
    "    elif '.json' in filename:\n",
    "        pass\n",
    "    else:\n",
    "        newfile = filename+'.json'\n",
    "        os.rename(filename, newfile)\n",
    "\n",
    "jsons = glob(os.path.join(zip_dir, '*.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading json elapsed_time:718.873034954071[sec]\n",
      "Loading json elapsed_time:648.138466835022[sec]\n",
      "Loading json elapsed_time:526.3302919864655[sec]\n",
      "Loading json elapsed_time:507.3329060077667[sec]\n",
      "Loading json elapsed_time:496.4747657775879[sec]\n",
      "Loading json elapsed_time:536.7752206325531[sec]\n",
      "Loading json elapsed_time:555.0317189693451[sec]\n",
      "Loading json elapsed_time:566.3812553882599[sec]\n",
      "Loading json elapsed_time:529.7259113788605[sec]\n",
      "Loading json elapsed_time:740.5443599224091[sec]\n"
     ]
    }
   ],
   "source": [
    "dfrts = []\n",
    "dfors = []\n",
    "for i, json in enumerate(jsons):\n",
    "    # Instance Preparation\n",
    "    dates = []\n",
    "    tweets = []\n",
    "    user_ids = []\n",
    "    \n",
    "    # tweet's location\n",
    "    place_fullname_tweet = []\n",
    "    # tweet's country\n",
    "    nationality = []\n",
    "    \n",
    "    start = time.time()\n",
    "    # Load json file: date and tweet\n",
    "    with open(json, 'r', encoding='utf8') as file:\n",
    "        pet_parse = ijson.parse(file, multiple_values=True)\n",
    "        for prefix, event, value  in pet_parse:\n",
    "            # Date\n",
    "            if prefix == 'created_at':\n",
    "                dates.append(datetime.strptime(value, '%a %b %d %H:%M:%S +0000 %Y'))\n",
    "            # Tweet \n",
    "            if prefix == 'text':\n",
    "                tweets.append(value.replace('\\n', '').replace('\\t', '').replace('\\r', '').replace('\\r\\n', '').replace('ã€€', '')) # Delte space and indet and \\r\n",
    "            # User id \n",
    "            if prefix == 'user.id':\n",
    "                user_ids.append(value)\n",
    "\n",
    "            # tweet's location - fullname\n",
    "            if prefix == 'place.full_name':\n",
    "                place_fullname_tweet.append(value)\n",
    "            if len(dates)-2 == len(place_fullname_tweet):\n",
    "                place_fullname_tweet.append('nan')\n",
    "                \n",
    "            # tweet's nationality\n",
    "            if prefix == 'place.country':\n",
    "                nationality.append(value)\n",
    "            if len(dates)-2 == len(nationality):\n",
    "                nationality.append('nan')\n",
    "\n",
    "        # Add missing last one data\n",
    "        if len(dates) != len(place_fullname_tweet):\n",
    "                place_fullname_tweet.append('nan')\n",
    "        if len(dates) != len(nationality):\n",
    "                nationality.append('nan')\n",
    "\n",
    "    print(\"Loading json elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n",
    "    \n",
    "    # Create dataframe\n",
    "    data = np.vstack([user_ids, dates, tweets, nationality, place_fullname_tweet]).T\n",
    "    df = pd.DataFrame(data, columns=['user_id', 'date', 'tweet', 'nationality', 'place_tweet'])\n",
    "    \n",
    "    import datetime as dt\n",
    "    \n",
    "    # Df only with location data\n",
    "    df = df[df.place_tweet != 'nan']\n",
    "    \n",
    "    # Change time zone\n",
    "    df['date'] = df['date'] + dt.timedelta(hours = 9)\n",
    "    \n",
    "    # Date\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['time'] = df['date'].dt.time\n",
    "    df['date'] = df['date'].dt.date\n",
    "    \n",
    "#     # divide df by RT_flag \n",
    "#     dfrt = df[df['RT_flag'] == True]\n",
    "#     dfor = df[df['RT_flag'] == False]\n",
    "    \n",
    "    # Save to csv file\n",
    "    df.to_csv('../data/dfs_location2/df_lct'+str(i)+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv of dictionary\n",
    "def crate_area_dictionary(file):\n",
    "    area = pd.read_csv(file)\n",
    "    area_jp = area.drop(['pref_en'], axis=1).set_index(['pref_jp']).Area.to_dict()\n",
    "    area_en = area.drop(['pref_jp'], axis=1).set_index(['pref_en']).Area.to_dict()\n",
    "    return area_jp, area_en\n",
    "\n",
    "area_jp, area_en = crate_area_dictionary('../data/area.csv')\n",
    "area_shutoken_jp, area_shutoken_en = crate_area_dictionary('../data/area_shutoken.csv')\n",
    "area_tokyo_jp, area_tokyo_en = crate_area_dictionary('../data/area_tokyo.csv')\n",
    "area_kitakan_jp, area_kitakan_en = crate_area_dictionary('../data/area_kitakanto.csv')\n",
    "\n",
    "# Divide tweet by area\n",
    "def df_by_area(df, d_area_jp, d_area_en):\n",
    "    l_area = []\n",
    "    for natio, place in zip(df.nationality, df.place_tweet):\n",
    "        if natio == 'æ—¥æœ¬' and place.split(' ')[0] in d_area_jp:\n",
    "            l_area.append(d_area_jp[place.split(' ')[0]])\n",
    "        elif natio == 'æ—¥æœ¬' and place.split(' ')[-1] in d_area_jp:\n",
    "            l_area.append(d_area_jp[place.split(' ')[-1]])\n",
    "        elif l_area == 'Japan' and place.split(', ')[-1] in d_area_en:\n",
    "            l_area.append(d_area_en[place.split(', ')[-1]])\n",
    "        elif natio == 'Japan' and place.split(', ')[0] in d_area_en:\n",
    "            l_area.append(d_area_en[place.split(', ')[0]])\n",
    "        else:\n",
    "            l_area.append('nan')\n",
    "    df['area'] = l_area\n",
    "    df_lct = df[df.area != 'nan']\n",
    "    return df_lct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "zip_dir  = \"../data/dfs_location2\" # new_dfs2_jst\n",
    "csvfiles = glob(os.path.join(zip_dir, 'df*'))\n",
    "\n",
    "dfs = []\n",
    "for csvfile in csvfiles:\n",
    "    df = pd.read_csv(csvfile).drop('Unnamed: 0', axis=1)\n",
    "    dfs.append(df)\n",
    "    \n",
    "# Connet all df to one data\n",
    "df_lcts = [df_by_area(df, area_jp, area_en)  for df in dfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze data monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat dfs\n",
    "for i, df in enumerate(df_lcts):\n",
    "    if i == 0:\n",
    "        dfm = df\n",
    "    else:\n",
    "        dfm = pd.concat([dfm, df], axis=0)\n",
    "_df = dfm.copy()\n",
    "# é¦–éƒ½åœ\n",
    "df_shutoken = df_by_area(_df, area_shutoken_jp, area_shutoken_en)\n",
    "dfm = pd.concat([dfm, df_shutoken], axis=0)\n",
    "# æ±äº¬\n",
    "df_tokyo = df_by_area(_df, area_tokyo_jp, area_tokyo_en)\n",
    "dfm = pd.concat([dfm, df_tokyo], axis=0)\n",
    "# Drop axis\n",
    "dfm = dfm.drop(['nationality', 'place_tweet'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>time</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>778167031598374912</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>ãªã‚“ã‹ã‚‚ã†ã ã‚‹ã„è‡ªåˆ†ãŒå«Œ</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "      <td>æ±åŒ—åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2989602496</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>æ—©ã!!å¯¾å¿œã—ã¦ãã ã•ã„ï¼é‡å…šã‚„ã‚‰ä¸å…šã‚„ã‚‰è¨€ã£ã¦ã‚‹å ´åˆã§ã¯ãªã„ã§ã™ã‚ˆï¼äº¡ããªã£ãŸæ–¹ã‚’æŒã¡ã ã—...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:55</td>\n",
       "      <td>æ±åŒ—åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>986162527007490048</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>å®®è—¤å®˜ä¹éƒã•ã‚“ãŒæ„ŸæŸ“ã—ã¦ã—ã¾ã£ãŸã¿ãŸã„ã§ã™ãŒ22æ—¥ã«ä¸­å±±å„ªé¦¬ãã‚“ã®èˆå°ã«å®®è—¤å®˜ä¹éƒã•ã‚“ã¨æ—¥æ‘...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:34</td>\n",
       "      <td>é–¢æ±åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3281248128</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>ã¯ããã€ã™ã§ã«ç·Šå¼µã™ã‚‹ãªãã€‚ã€‚ã€‚</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:58:33</td>\n",
       "      <td>ä¸­éƒ¨åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50943527</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>#å¿—æ‘ã‘ã‚“é¢ç™½ã„ã€‚è¦‹ãŸã‚‰æ³£ã‘ã¦ããŸã€‚æ‚²ã—ã„ã€‚</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:58:11</td>\n",
       "      <td>é–¢æ±åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4973</th>\n",
       "      <td>334036907</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>å®‰å€ã¡ã‚ƒã‚“ã«ã¯ã¾ã è¾ã‚ã‚‰ã‚Œã¡ã‚ƒå›°ã‚‹ã€‚ãƒ‰ãƒ³åº•ã¾ã§è¦‹å±Šã‘ã¦è²°ã‚ãªã„ã¨</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:13:24</td>\n",
       "      <td>æ±äº¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4975</th>\n",
       "      <td>611507926</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>@nyantaremama ãƒ†ã‚£ãƒƒã‚·ãƒ¥ä¸€ç®±è²·ã†ã®ã«â€¼ï¸ä½•è»’ã¾ã‚ã£ãŸã‹ï¼Ÿçµå±€åœ°å…ƒã®ã‚³ãƒ³ãƒ“ãƒ‹ğŸªã«...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:13:11</td>\n",
       "      <td>æ±äº¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>352308836</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>æ ªå®‰ã§å¹´é‡‘ãŒã¾ãŸã€æ¸›ã‚‰ã•ã‚Œã‚‹â€¼ï¸æ ªå¼æŠ•è³‡ã—ã‚ã¯ã¨è¨€ã£ã¦ãªã„ã®ã«ä½•ã ã‹æ–‡å¥ã°ã‹ã‚Šã®ã€ãƒ„ã‚¤ãƒ¼ãƒˆè‡ª...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:04:04</td>\n",
       "      <td>æ±äº¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>1635284976</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>ç†Šæœ¬ã¯3å¹´+2å¹´ï¼ˆé–“ã‚ã„ã¦ã‚‹ï¼‰ä½ã‚“ã§ãŸã‘ã©ã€ãƒ‘ãƒ«ã‚³ãªããªã£ãŸã®æ‚²ã—ã„ã€‚ã‚‚ã£ã¨é•·ãä½ã‚“ã§ã‚‹ç†Šæœ¬...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:03:22</td>\n",
       "      <td>æ±äº¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>141486067</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>æ€’ã‚Šã‚’ã©ã“ã‹ã«ã¶ã¤ã‘ãŸããªã‚‹ã®ã¯ã‚ã‹ã‚‹ã‘ã©ã€åº—å“¡ã•ã‚“ã«ã¶ã¤ã‘ã‚‹å¥´ã‚‚ã„ã‚‹ã‚“ã ãªã€‚ã“ã†ã„ã†äº‹æ…‹ã®...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:01:33</td>\n",
       "      <td>æ±äº¬</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57880 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 user_id        date  \\\n",
       "0     778167031598374912  2020-03-31   \n",
       "1             2989602496  2020-03-31   \n",
       "2     986162527007490048  2020-03-31   \n",
       "3             3281248128  2020-03-31   \n",
       "4               50943527  2020-03-31   \n",
       "...                  ...         ...   \n",
       "4973           334036907  2020-03-01   \n",
       "4975           611507926  2020-03-01   \n",
       "4992           352308836  2020-03-01   \n",
       "4993          1635284976  2020-03-01   \n",
       "4994           141486067  2020-03-01   \n",
       "\n",
       "                                                  tweet  year  month  day  \\\n",
       "0                                          ãªã‚“ã‹ã‚‚ã†ã ã‚‹ã„è‡ªåˆ†ãŒå«Œ  2020      3   31   \n",
       "1     æ—©ã!!å¯¾å¿œã—ã¦ãã ã•ã„ï¼é‡å…šã‚„ã‚‰ä¸å…šã‚„ã‚‰è¨€ã£ã¦ã‚‹å ´åˆã§ã¯ãªã„ã§ã™ã‚ˆï¼äº¡ããªã£ãŸæ–¹ã‚’æŒã¡ã ã—...  2020      3   31   \n",
       "2     å®®è—¤å®˜ä¹éƒã•ã‚“ãŒæ„ŸæŸ“ã—ã¦ã—ã¾ã£ãŸã¿ãŸã„ã§ã™ãŒ22æ—¥ã«ä¸­å±±å„ªé¦¬ãã‚“ã®èˆå°ã«å®®è—¤å®˜ä¹éƒã•ã‚“ã¨æ—¥æ‘...  2020      3   31   \n",
       "3                                      ã¯ããã€ã™ã§ã«ç·Šå¼µã™ã‚‹ãªãã€‚ã€‚ã€‚  2020      3   31   \n",
       "4                                #å¿—æ‘ã‘ã‚“é¢ç™½ã„ã€‚è¦‹ãŸã‚‰æ³£ã‘ã¦ããŸã€‚æ‚²ã—ã„ã€‚  2020      3   31   \n",
       "...                                                 ...   ...    ...  ...   \n",
       "4973                   å®‰å€ã¡ã‚ƒã‚“ã«ã¯ã¾ã è¾ã‚ã‚‰ã‚Œã¡ã‚ƒå›°ã‚‹ã€‚ãƒ‰ãƒ³åº•ã¾ã§è¦‹å±Šã‘ã¦è²°ã‚ãªã„ã¨  2020      3    1   \n",
       "4975  @nyantaremama ãƒ†ã‚£ãƒƒã‚·ãƒ¥ä¸€ç®±è²·ã†ã®ã«â€¼ï¸ä½•è»’ã¾ã‚ã£ãŸã‹ï¼Ÿçµå±€åœ°å…ƒã®ã‚³ãƒ³ãƒ“ãƒ‹ğŸªã«...  2020      3    1   \n",
       "4992  æ ªå®‰ã§å¹´é‡‘ãŒã¾ãŸã€æ¸›ã‚‰ã•ã‚Œã‚‹â€¼ï¸æ ªå¼æŠ•è³‡ã—ã‚ã¯ã¨è¨€ã£ã¦ãªã„ã®ã«ä½•ã ã‹æ–‡å¥ã°ã‹ã‚Šã®ã€ãƒ„ã‚¤ãƒ¼ãƒˆè‡ª...  2020      3    1   \n",
       "4993  ç†Šæœ¬ã¯3å¹´+2å¹´ï¼ˆé–“ã‚ã„ã¦ã‚‹ï¼‰ä½ã‚“ã§ãŸã‘ã©ã€ãƒ‘ãƒ«ã‚³ãªããªã£ãŸã®æ‚²ã—ã„ã€‚ã‚‚ã£ã¨é•·ãä½ã‚“ã§ã‚‹ç†Šæœ¬...  2020      3    1   \n",
       "4994  æ€’ã‚Šã‚’ã©ã“ã‹ã«ã¶ã¤ã‘ãŸããªã‚‹ã®ã¯ã‚ã‹ã‚‹ã‘ã©ã€åº—å“¡ã•ã‚“ã«ã¶ã¤ã‘ã‚‹å¥´ã‚‚ã„ã‚‹ã‚“ã ãªã€‚ã“ã†ã„ã†äº‹æ…‹ã®...  2020      3    1   \n",
       "\n",
       "      hour      time  area  \n",
       "0       23  23:59:58  æ±åŒ—åœ°æ–¹  \n",
       "1       23  23:59:55  æ±åŒ—åœ°æ–¹  \n",
       "2       23  23:59:34  é–¢æ±åœ°æ–¹  \n",
       "3       23  23:58:33  ä¸­éƒ¨åœ°æ–¹  \n",
       "4       23  23:58:11  é–¢æ±åœ°æ–¹  \n",
       "...    ...       ...   ...  \n",
       "4973     0  00:13:24    æ±äº¬  \n",
       "4975     0  00:13:11    æ±äº¬  \n",
       "4992     0  00:04:04    æ±äº¬  \n",
       "4993     0  00:03:22    æ±äº¬  \n",
       "4994     0  00:01:33    æ±äº¬  \n",
       "\n",
       "[57880 rows x 9 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## group by area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_dict = {}\n",
    "# areas = dfm.area.unique().tolist()\n",
    "areas = ['åŒ—æµ·é“åœ°æ–¹',\n",
    "         'æ±åŒ—åœ°æ–¹',\n",
    "         'é–¢æ±åœ°æ–¹',\n",
    "         'ä¸­éƒ¨åœ°æ–¹',\n",
    "         'è¿‘ç•¿åœ°æ–¹',\n",
    "         'ä¸­å›½åœ°æ–¹',\n",
    "         'å››å›½åœ°æ–¹',\n",
    "         'ä¹å·ãƒ»æ²–ç¸„åœ°æ–¹',\n",
    "         'é¦–éƒ½åœ',\n",
    "         'æ±äº¬']\n",
    "initial_area = ['A','B','C','D','E','F','G','H','I','J']\n",
    "for area, initial in zip(areas, initial_area):\n",
    "    area_dict[area] = initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into csv of each area\n",
    "for area in areas:\n",
    "    dfm.groupby('area').get_group(area).to_csv('../data/dfs_by_area2/mar_'+area_dict[area]+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read \n",
    "zip_dir  = \"../data/dfs_by_area2\" # new_dfs2_jst\n",
    "csvfiles = glob(os.path.join(zip_dir, 'mar_*'))\n",
    "\n",
    "feb_dfs = []\n",
    "for csvfile in csvfiles:\n",
    "    df = pd.read_csv(csvfile).drop(['Unnamed: 0'], axis=1)\n",
    "    feb_dfs.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count tweet by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_hour(dfor, filetype, savedir, key_lang, area):\n",
    "    # Prepartion of dfw \n",
    "    results = []\n",
    "    dfw = pd.read_csv('../data/WordTimeSeries.csv', encoding='utf-8').rename(columns={'Unnamed: 0': 'types',\n",
    "                                                                                    'file name': 'file_name', \n",
    "                                                                                    'orignal form': 'orignal_form',\n",
    "                                                                                    'English translation': 'English_translation'})\n",
    "    booleanDictionary = {True: 'TRUE', False: 'FALSE'} # Drop nan and False in order to use query \n",
    "    dfw = dfw.replace(booleanDictionary)\n",
    "    dfw = dfw.dropna(how='all')\n",
    "\n",
    "    # Prepration of keywords\n",
    "    filenames = [filename for filename in dfw.file_name.tolist() if filetype in filename]\n",
    "    xd = {}\n",
    "    xd['T'] = 'file_name.str.contains(\"T\")'\n",
    "    xd['D'] = 'file_name.str.contains(\"D\")'\n",
    "    xd['A'] = 'file_name.str.contains(\"A\")'\n",
    "    xd['V'] = 'file_name.str.contains(\"V\")'\n",
    "    xd['F'] = 'file_name.str.contains(\"F\")'\n",
    "    xd['C'] = 'file_name.str.contains(\"C\")'\n",
    "\n",
    "    query = xd[filetype]\n",
    "    dfw_c = dfw.query(query, engine='python')\n",
    "    \n",
    "    # Keyword \n",
    "    if key_lang == 'jp':\n",
    "        keywords = dfw_c.orignal_form.tolist() # English.ver: keywords = dfw_c.English_translation.tolist() \n",
    "    elif key_lang == 'en':\n",
    "        keywords = dfw_c.English_translation.tolist() \n",
    "    else:\n",
    "        print('Put accurate keyword language')\n",
    "    \n",
    "    # Find tweets\n",
    "    # Extract tweet\n",
    "    dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "\n",
    "    for col, key in zip(filenames, keywords):\n",
    "        l = []\n",
    "        for row in dfor_c.itertuples():\n",
    "            if key in row.tweet:\n",
    "                l.append(1)\n",
    "            else:\n",
    "                l.append(0)\n",
    "        dfor_c[col] = l\n",
    "\n",
    "    # Save dir\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "\n",
    "#     for i, df in enumerate(dfs):\n",
    "    df = dfor_c\n",
    "    unique_dates = df['date'].tolist()\n",
    "    unique_dates = sorted(set(unique_dates), key=unique_dates.index) # date values\n",
    "    df_date = df.groupby('date') # df grouped by date   \n",
    "\n",
    "    for date in unique_dates:\n",
    "        df_oneday = df_date.get_group(date) # df of one day\n",
    "        dfh = df_oneday.groupby('hour').sum()\n",
    "        dfh = dfh.drop(['year', 'month', 'user_id', 'day'],axis=1)\n",
    "        \n",
    "        # Save csv\n",
    "        outname = 'area'+area+'_'+filetype+str(date)+'.csv'\n",
    "        savename = os.path.join(outdir, outname)\n",
    "\n",
    "        # Save to csv\n",
    "        dfh.to_csv(savename)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "G\n",
      "H\n",
      "I\n",
      "J\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "initial_area = ['A','B','C','D','E','F','G','H','I','J']\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for area, df in zip(initial_area, feb_dfs):\n",
    "    print(area)\n",
    "    for filetype in filetypes:\n",
    "        findtwt_hour(df, filetype=filetype, savedir='../results_location2/hour', key_lang='jp', area=area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['åŒ—æµ·é“åœ°æ–¹']\n",
      "['æ±åŒ—åœ°æ–¹']\n",
      "['é–¢æ±åœ°æ–¹']\n",
      "['ä¸­éƒ¨åœ°æ–¹']\n",
      "['è¿‘ç•¿åœ°æ–¹']\n",
      "['ä¸­å›½åœ°æ–¹']\n",
      "['å››å›½åœ°æ–¹']\n",
      "['ä¹å·ãƒ»æ²–ç¸„åœ°æ–¹']\n",
      "['é¦–éƒ½åœ']\n",
      "['æ±äº¬']\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "for df in feb_dfs:\n",
    "    print(df.area.unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
