{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "from datetime import datetime\n",
    "import os\n",
    "from os import path\n",
    "from glob import glob\n",
    "\n",
    "import ijson\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_dir  = \"../data/zip2\"\n",
    "\n",
    "files = glob(os.path.join(zip_dir, '*'))\n",
    "for filename in files:\n",
    "    if '.zip' in filename:\n",
    "        pass\n",
    "    elif '.json' in filename:\n",
    "        pass\n",
    "    else:\n",
    "        newfile = filename+'.json'\n",
    "        os.rename(filename, newfile)\n",
    "\n",
    "jsons = glob(os.path.join(zip_dir, '*.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/zip2/part-v003-o001-r-00000.json',\n",
       " '../data/zip2/part-v003-o001-r-00001.json',\n",
       " '../data/zip2/part-v003-o001-r-00002.json',\n",
       " '../data/zip2/part-v003-o001-r-00003.json',\n",
       " '../data/zip2/part-v003-o001-r-00004.json',\n",
       " '../data/zip2/part-v003-o001-r-00005.json',\n",
       " '../data/zip2/part-v003-o001-r-00006.json',\n",
       " '../data/zip2/part-v003-o001-r-00007.json',\n",
       " '../data/zip2/part-v003-o001-r-00008.json',\n",
       " '../data/zip2/part-v003-o001-r-00009.json']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jsons = jsons[:2]\n",
    "jsons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading json elapsed_time:718.873034954071[sec]\n",
      "Loading json elapsed_time:648.138466835022[sec]\n",
      "Loading json elapsed_time:526.3302919864655[sec]\n",
      "Loading json elapsed_time:507.3329060077667[sec]\n",
      "Loading json elapsed_time:496.4747657775879[sec]\n",
      "Loading json elapsed_time:536.7752206325531[sec]\n",
      "Loading json elapsed_time:555.0317189693451[sec]\n",
      "Loading json elapsed_time:566.3812553882599[sec]\n",
      "Loading json elapsed_time:529.7259113788605[sec]\n",
      "Loading json elapsed_time:740.5443599224091[sec]\n"
     ]
    }
   ],
   "source": [
    "# jsons = ['../data/part_v003_o001_r_00000.json', ..., '../data/part_v003_o001_r_00001.json']\n",
    "\n",
    "dfrts = []\n",
    "dfors = []\n",
    "for i, json in enumerate(jsons):\n",
    "    # Instance Preparation\n",
    "    dates = []\n",
    "    tweets = []\n",
    "    user_ids = []\n",
    "#     rts = []\n",
    "\n",
    "#     # tweet's location\n",
    "#     place_tweet = []\n",
    "    # tweet's location\n",
    "    place_fullname_tweet = []\n",
    "    # tweet's country\n",
    "    nationality = []\n",
    "    \n",
    "    start = time.time()\n",
    "    # Load json file: date and tweet\n",
    "    with open(json, 'r', encoding='utf8') as file:\n",
    "        pet_parse = ijson.parse(file, multiple_values=True)\n",
    "        for prefix, event, value  in pet_parse:\n",
    "            # Date\n",
    "            if prefix == 'created_at':\n",
    "                dates.append(datetime.strptime(value, '%a %b %d %H:%M:%S +0000 %Y'))\n",
    "            # Tweet \n",
    "            if prefix == 'text':\n",
    "                tweets.append(value.replace('\\n', '').replace('\\t', '').replace('\\r', '').replace('\\r\\n', '').replace('ã€€', '')) # Delte space and indet and \\r\n",
    "            # User id \n",
    "            if prefix == 'user.id':\n",
    "                user_ids.append(value)\n",
    "                \n",
    "#             # RT Flag\n",
    "#             if len(dates)-1 == len(rts) and prefix == 'retweeted_status':\n",
    "#                 rts.append(True)\n",
    "#             if len(dates)-2 == len(rts):\n",
    "#                 rts.append(False)\n",
    "            \n",
    "#             # tweet's location\n",
    "#             if prefix == 'place.name':\n",
    "#                 place_tweet.append(value)\n",
    "#             if len(dates)-2 == len(place_tweet):\n",
    "#                 place_tweet.append('nan')\n",
    "\n",
    "            # tweet's location - fullname\n",
    "            if prefix == 'place.full_name':\n",
    "                place_fullname_tweet.append(value)\n",
    "            if len(dates)-2 == len(place_fullname_tweet):\n",
    "                place_fullname_tweet.append('nan')\n",
    "                \n",
    "            # tweet's nationality\n",
    "            if prefix == 'place.country':\n",
    "                nationality.append(value)\n",
    "            if len(dates)-2 == len(nationality):\n",
    "                nationality.append('nan')\n",
    "\n",
    "        # Add missing data\n",
    "#         if len(dates) != len(rts):\n",
    "#                 rts.append(False)\n",
    "        if len(dates) != len(place_fullname_tweet):\n",
    "                place_fullname_tweet.append('nan')\n",
    "        if len(dates) != len(nationality):\n",
    "                nationality.append('nan')\n",
    "\n",
    "    print(\"Loading json elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n",
    "    \n",
    "#     print(len(dates), len(tweets), len(user_ids), len(rts), len(nationality), len(place_tweet))\n",
    "    \n",
    "    # Create dataframe\n",
    "    data = np.vstack([user_ids, dates, tweets, nationality, place_fullname_tweet]).T\n",
    "    df = pd.DataFrame(data, columns=['user_id', 'date', 'tweet', 'nationality', 'place_tweet'])\n",
    "    \n",
    "    import datetime as dt\n",
    "    \n",
    "    # Df only with location data\n",
    "    df = df[df.place_tweet != 'nan']\n",
    "    \n",
    "    # Change time zone\n",
    "    df['date'] = df['date'] + dt.timedelta(hours = 9)\n",
    "    \n",
    "    # Date\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['time'] = df['date'].dt.time\n",
    "    df['date'] = df['date'].dt.date\n",
    "    \n",
    "#     # divide df by RT_flag \n",
    "#     dfrt = df[df['RT_flag'] == True]\n",
    "#     dfor = df[df['RT_flag'] == False]\n",
    "    \n",
    "    # Save to csv file\n",
    "    df.to_csv('../data/dfs_location2/df_lct'+str(i)+'.csv')\n",
    "#     dfrt.to_csv('../data/dfs_location/dfrt_lct'+str(i)+'.csv')\n",
    "    \n",
    "    # Append to dfs\n",
    "#     dfrts.append(dfrt)\n",
    "#     dfors.append(dfor)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### ***************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|            Variable Name|    Memory|\n",
      " ------------------------------------ \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"{}{: >25}{}{: >10}{}\".format('|','Variable Name','|','Memory','|'))\n",
    "print(\" ------------------------------------ \")\n",
    "for var_name in dir():\n",
    "    if not var_name.startswith(\"_\") and sys.getsizeof(eval(var_name)) > 10000: #ã“ã“ã ã‘ã‚¢ãƒ¬ãƒ³ã‚¸\n",
    "        print(\"{}{: >25}{}{: >10}{}\".format('|',var_name,'|',sys.getsizeof(eval(var_name)),'|'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = pd.read_csv('../data/area_japan.csv')\n",
    "t = pd.read_csv('../data/dfs_location/df_lct1.csv').drop('Unnamed: 0', axis=1)\n",
    "area_jp = area.drop(['pref_en'], axis=1).set_index(['pref_jp']).Area.to_dict()\n",
    "area_en = area.drop(['pref_jp'], axis=1).set_index(['pref_en']).Area.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = []\n",
    "for natio, place in zip(t.nationality, t.place_tweet):\n",
    "    if natio == 'æ—¥æœ¬' and place.split(' ')[0] in area_jp:\n",
    "        area.append(area_jp[place.split(' ')[0]])\n",
    "    elif natio == 'æ—¥æœ¬' and place.split(' ')[-1] in area_jp:\n",
    "        area.append(area_jp[place.split(' ')[-1]])\n",
    "    elif natio == 'Japan' and place.split(', ')[-1] in area_en:\n",
    "        area.append(area_en[place.split(', ')[-1]])\n",
    "    elif natio == 'Japan' and place.split(', ')[0] in area_en:\n",
    "        area.append(area_en[place.split(', ')[0]])\n",
    "    else:\n",
    "        area.append('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "t['area'] = area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>nationality</th>\n",
       "      <th>place_tweet</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>time</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>823461826339872768</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>ãƒˆãƒ«ã¡ã‚ƒã‚“æœ€çµ‚æ—¥ã€EXã‚¸ãƒ å¤ã®æ€ã„å‡ºã€è‡ªè‰²ã§é‡‘ã‚¸ãƒ ã€çµ‚ã‚ã‚Šã‚®ãƒªå¤§è¦ªå‹1äººã¨äººãŒæ¥ãªãã¦å›°ã£ãŸ...</td>\n",
       "      <td>æ—¥æœ¬</td>\n",
       "      <td>å®®åŸ é’è‘‰åŒº</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>02:14:36</td>\n",
       "      <td>æ±åŒ—åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200059773</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>ã¾ã å‰åŠã ã‘ã©ã‚·ãƒŠãƒªã‚ªçš„ã«ã¯å¤©å¸ãŸã¡ã®ç‰©èªã¨åŒ¹æ•µã™ã‚‹ã»ã©ã§ã¯ãªã„ã£ã¦æ„Ÿã˜w pvã¯å£®å¤§ã™ãã§...</td>\n",
       "      <td>ä¸­åäººæ°‘å…±å’Œå›½</td>\n",
       "      <td>å¹¿ä¸œ, ä¸­åäººæ°‘å…±å’Œå›½</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>02:08:45</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1221438530867515392</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>è¿½åŠ B'zhideâ†ãƒãƒ³ãƒ‰ã˜ã‚ƒãªãã¦ã‚‚ã§ã‚‚çµµã‚’æãæ™‚ã¯çµ¶å¯¾#ç±³æ´¥ç„å¸«å›ºå®šã•ã‚Œã¾ã—ãŸã€‚æœ€è¿‘æã„...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Minamiboso-shi, Chiba</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>02:08:25</td>\n",
       "      <td>é–¢æ±åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>140297465</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>ã©ãˆã‚‰ã„ã‚¨ãƒ­ãƒ‹ã‚­ãƒ“ãŒæ²»ã‚‰ãªã„ã€‚å›°ã£ãŸã€‚</td>\n",
       "      <td>æ—¥æœ¬</td>\n",
       "      <td>é¹¿å… é¹¿å…å³¶å¸‚</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>02:05:23</td>\n",
       "      <td>ä¹å·ãƒ»æ²–ç¸„åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>931291856477986816</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ã€‚ã£ã¦è¨€ã†ã‹ã“ã‚“ã°ã‚“ã‚ã€‚m(_ _)mæ˜¨æ—¥ã§ç†Šæœ¬ã§ã®ä»•äº‹ã‚‚çµ‚ã‚ã‚Šã€ã•ã€œå‘‘ã¿...</td>\n",
       "      <td>æ—¥æœ¬</td>\n",
       "      <td>ç†Šæœ¬ å®‡åŸå¸‚</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>02:02:48</td>\n",
       "      <td>ä¹å·ãƒ»æ²–ç¸„åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4659</th>\n",
       "      <td>1102038905811955713</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>@kento47011659 åŸå› åˆ†ã‹ã‚‰ãªã„ã®ãŒä¸å®‰ã§ã™ã‚ˆã­ï¼Ÿç—…é™¢ã§æ¤œæŸ»ã—ã¦ãã ã•ã„ã­ã€‚</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Numazu-shi, Shizuoka</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>05:02:55</td>\n",
       "      <td>ä¸­éƒ¨åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4660</th>\n",
       "      <td>4799944992</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>ä½•æ›²ã‹æ­Œã£ãŸæ‰€ã§ã€Œåå¤å±‹ã®åå‰ã‚’å‡ºã—ã¦å¤§é˜ªã§ç…½ã£ãŸã‚“ã ä»Šæ—¥ç››ã‚Šä¸ŠãŒã‚‰ãªã‹ã£ãŸã‚‰7æ›²ã§å¸°ã‚‹â—ãƒ»...</td>\n",
       "      <td>æ—¥æœ¬</td>\n",
       "      <td>å²é˜œ è¼ªä¹‹å†…ç”º</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>04:56:59</td>\n",
       "      <td>ä¸­éƒ¨åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4661</th>\n",
       "      <td>392696345</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>ã‚¦ã‚©ãƒ«ãƒˆãƒ‡ã‚£ã‚ºãƒ‹ãƒ¼ã‚‚ã‚¹ãƒ†ã‚£ãƒ¼ãƒ–ãƒ»ã‚¸ãƒ§ãƒ–ã‚ºã‚‚å¥½ããªã‚“ã‚„ã‘ã©ã€2äººã¯äº¡ããªã£ã¦ã‚‹ã€‚ä»Šã§ã¯å‰äººã ã®...</td>\n",
       "      <td>France</td>\n",
       "      <td>Paris, France</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>04:50:57</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4662</th>\n",
       "      <td>2220016902</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>å®‡é‡ã‹ã‚‰é«˜æ¾ã«å››å›½ãƒ•ã‚§ãƒªãƒ¼ã¨åŒã˜ãã‚‰ã„ã®å€¤æ®µã¨æ™‚åˆ»ã§è¡Œãæ–¹æ³•ã¿ã¤ã‘ãŸç›´å³¶çµŒç”±ã—ã¦é«˜æ¾ã«è¡Œãã®...</td>\n",
       "      <td>æ—¥æœ¬</td>\n",
       "      <td>é“ã®é§… ã¿ã‚„ã¾å…¬åœ’</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>04:49:30</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4663</th>\n",
       "      <td>1009053225503285248</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>@erinappan ãƒ”ãƒ¼ã‚¹æŒ‡â‘¡æœ¬ãªã®ã«.å‹æ‰‹ã«æŒ‡ã‚ã£ã¡è¡ŒããŸã„ã£ã¦ãªã£ã¦ã‚‹ã‚„ã‚“f(^_^)...</td>\n",
       "      <td>æ—¥æœ¬</td>\n",
       "      <td>å¯Œå±± é«˜å²¡å¸‚</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>04:48:26</td>\n",
       "      <td>ä¸­éƒ¨åœ°æ–¹</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4664 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id        date  \\\n",
       "0      823461826339872768  2020-02-27   \n",
       "1               200059773  2020-02-27   \n",
       "2     1221438530867515392  2020-02-27   \n",
       "3               140297465  2020-02-27   \n",
       "4      931291856477986816  2020-02-27   \n",
       "...                   ...         ...   \n",
       "4659  1102038905811955713  2020-02-24   \n",
       "4660           4799944992  2020-02-24   \n",
       "4661            392696345  2020-02-24   \n",
       "4662           2220016902  2020-02-24   \n",
       "4663  1009053225503285248  2020-02-24   \n",
       "\n",
       "                                                  tweet nationality  \\\n",
       "0     ãƒˆãƒ«ã¡ã‚ƒã‚“æœ€çµ‚æ—¥ã€EXã‚¸ãƒ å¤ã®æ€ã„å‡ºã€è‡ªè‰²ã§é‡‘ã‚¸ãƒ ã€çµ‚ã‚ã‚Šã‚®ãƒªå¤§è¦ªå‹1äººã¨äººãŒæ¥ãªãã¦å›°ã£ãŸ...          æ—¥æœ¬   \n",
       "1     ã¾ã å‰åŠã ã‘ã©ã‚·ãƒŠãƒªã‚ªçš„ã«ã¯å¤©å¸ãŸã¡ã®ç‰©èªã¨åŒ¹æ•µã™ã‚‹ã»ã©ã§ã¯ãªã„ã£ã¦æ„Ÿã˜w pvã¯å£®å¤§ã™ãã§...     ä¸­åäººæ°‘å…±å’Œå›½   \n",
       "2     è¿½åŠ B'zhideâ†ãƒãƒ³ãƒ‰ã˜ã‚ƒãªãã¦ã‚‚ã§ã‚‚çµµã‚’æãæ™‚ã¯çµ¶å¯¾#ç±³æ´¥ç„å¸«å›ºå®šã•ã‚Œã¾ã—ãŸã€‚æœ€è¿‘æã„...       Japan   \n",
       "3                                   ã©ãˆã‚‰ã„ã‚¨ãƒ­ãƒ‹ã‚­ãƒ“ãŒæ²»ã‚‰ãªã„ã€‚å›°ã£ãŸã€‚          æ—¥æœ¬   \n",
       "4     ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ã€‚ã£ã¦è¨€ã†ã‹ã“ã‚“ã°ã‚“ã‚ã€‚m(_ _)mæ˜¨æ—¥ã§ç†Šæœ¬ã§ã®ä»•äº‹ã‚‚çµ‚ã‚ã‚Šã€ã•ã€œå‘‘ã¿...          æ—¥æœ¬   \n",
       "...                                                 ...         ...   \n",
       "4659       @kento47011659 åŸå› åˆ†ã‹ã‚‰ãªã„ã®ãŒä¸å®‰ã§ã™ã‚ˆã­ï¼Ÿç—…é™¢ã§æ¤œæŸ»ã—ã¦ãã ã•ã„ã­ã€‚       Japan   \n",
       "4660  ä½•æ›²ã‹æ­Œã£ãŸæ‰€ã§ã€Œåå¤å±‹ã®åå‰ã‚’å‡ºã—ã¦å¤§é˜ªã§ç…½ã£ãŸã‚“ã ä»Šæ—¥ç››ã‚Šä¸ŠãŒã‚‰ãªã‹ã£ãŸã‚‰7æ›²ã§å¸°ã‚‹â—ãƒ»...          æ—¥æœ¬   \n",
       "4661  ã‚¦ã‚©ãƒ«ãƒˆãƒ‡ã‚£ã‚ºãƒ‹ãƒ¼ã‚‚ã‚¹ãƒ†ã‚£ãƒ¼ãƒ–ãƒ»ã‚¸ãƒ§ãƒ–ã‚ºã‚‚å¥½ããªã‚“ã‚„ã‘ã©ã€2äººã¯äº¡ããªã£ã¦ã‚‹ã€‚ä»Šã§ã¯å‰äººã ã®...      France   \n",
       "4662  å®‡é‡ã‹ã‚‰é«˜æ¾ã«å››å›½ãƒ•ã‚§ãƒªãƒ¼ã¨åŒã˜ãã‚‰ã„ã®å€¤æ®µã¨æ™‚åˆ»ã§è¡Œãæ–¹æ³•ã¿ã¤ã‘ãŸç›´å³¶çµŒç”±ã—ã¦é«˜æ¾ã«è¡Œãã®...          æ—¥æœ¬   \n",
       "4663  @erinappan ãƒ”ãƒ¼ã‚¹æŒ‡â‘¡æœ¬ãªã®ã«.å‹æ‰‹ã«æŒ‡ã‚ã£ã¡è¡ŒããŸã„ã£ã¦ãªã£ã¦ã‚‹ã‚„ã‚“f(^_^)...          æ—¥æœ¬   \n",
       "\n",
       "                place_tweet  year  month  day  hour      time     area  \n",
       "0                    å®®åŸ é’è‘‰åŒº  2020      2   27     2  02:14:36     æ±åŒ—åœ°æ–¹  \n",
       "1               å¹¿ä¸œ, ä¸­åäººæ°‘å…±å’Œå›½  2020      2   27     2  02:08:45      nan  \n",
       "2     Minamiboso-shi, Chiba  2020      2   27     2  02:08:25     é–¢æ±åœ°æ–¹  \n",
       "3                   é¹¿å… é¹¿å…å³¶å¸‚  2020      2   27     2  02:05:23  ä¹å·ãƒ»æ²–ç¸„åœ°æ–¹  \n",
       "4                    ç†Šæœ¬ å®‡åŸå¸‚  2020      2   27     2  02:02:48  ä¹å·ãƒ»æ²–ç¸„åœ°æ–¹  \n",
       "...                     ...   ...    ...  ...   ...       ...      ...  \n",
       "4659   Numazu-shi, Shizuoka  2020      2   24     5  05:02:55     ä¸­éƒ¨åœ°æ–¹  \n",
       "4660                å²é˜œ è¼ªä¹‹å†…ç”º  2020      2   24     4  04:56:59     ä¸­éƒ¨åœ°æ–¹  \n",
       "4661          Paris, France  2020      2   24     4  04:50:57      nan  \n",
       "4662              é“ã®é§… ã¿ã‚„ã¾å…¬åœ’  2020      2   24     4  04:49:30      nan  \n",
       "4663                 å¯Œå±± é«˜å²¡å¸‚  2020      2   24     4  04:48:26     ä¸­éƒ¨åœ°æ–¹  \n",
       "\n",
       "[4664 rows x 11 columns]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = t[t.area == 'nan']\n",
    "n2 = n[n.nationality == 'æ—¥æœ¬']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>nationality</th>\n",
       "      <th>place_tweet</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>time</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>214091568</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>ä»Šæ—¥ã¯ãŠèª•ç”Ÿæ—¥ã®ã“ã¡ã‚‰ã®æ–¹ã®è½èªä¼šã¸ãƒ¼ï¼ï¼ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ãƒ¼ãŠ—ï¸ä»Šæ—¥ã‚‚ç¬‘ã‚ã›ã¦ã‚‚ã‚‰ã„ã¾ã—...</td>\n",
       "      <td>æ—¥æœ¬</td>\n",
       "      <td>ã‚ã¹ã®ãƒãƒ«ã‚«ã‚¹ (Abeno Harukas)</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>01:52:29</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>283672965</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>å…ˆé€±ã¯ã€ã‚·ã‚²æ¼”å‡ºã®ã€ã‚µã‚¼ãƒ³å¹•ã€ã‚’è¦³ã¦ãã¾ã—ãŸâœ¨å¾Œå‘³ã®è‰¯ã„èˆå°ã¯å¤§å¥½ãã§ã™â€¼ï¸å·æ³£ã‚’æ‡¸å¿µã—ã¦ã‚¿...</td>\n",
       "      <td>æ—¥æœ¬</td>\n",
       "      <td>ã‚ã†ã‚‹ã™ã½ã£ã¨ (èˆå°èŠ¸è¡“äº¤æµã‚»ãƒ³ã‚¿ãƒ¼)</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>22:47:25</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>1020450848395046912</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>ã€æ–°å®¿SAMURAIã‚ã‚ŠãŒã¨ã†ã€‘ğŸ’«2/26meiyoã‚»ãƒƒãƒˆãƒªã‚¹ãƒˆğŸ’«1.ã‚·ãƒˆãƒ©ã‚¹2.æº¶ã‘ã‚‹é’æ˜¥...</td>\n",
       "      <td>æ—¥æœ¬</td>\n",
       "      <td>Shinjuku SAMURAI</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>22:44:14</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>1481572735</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>ãƒã‚¸ã‹ãã‚„ã£ã±ãã†ãªã‚‹ã‚ˆãªãã‚ã£ã¡ã‚ƒæ¥½ã—ã¿ã«ã—ã¦ãŸã®ã«...(â—â€¸â—Ÿã†€)ã§ã‚‚ãƒ¡ãƒ³ãƒãƒ¼ã®æ–¹ãŒã‚¬...</td>\n",
       "      <td>æ—¥æœ¬</td>\n",
       "      <td>ãƒã‚¹ã‚¿æ–°å®¿ (Shinjuku Expressway Bus Terminal)</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>22:22:50</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>184733957</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>ãŠè…¹ã„ã£ã±ã„ã«ãªã£ã¦è½ã¡ç€ã„ãŸã®ã§ãƒ‡ã‚¶ãƒ¼ãƒˆã¨ç´…èŒ¶ã‚’é ‚ããªãŒã‚‰æ±äº¬ãƒ‰ãƒ¼ãƒ ã‚’çœºã‚ã¦æƒ³ã„ã«ãµã‘ã‚‹ã€‚...</td>\n",
       "      <td>æ—¥æœ¬</td>\n",
       "      <td>å™ã€…è‹‘</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>22:19:19</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>786091920145735680</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>æ—…è¡Œè¨˜éŒ²1æ—¥ç›®ã€‚äº¬éƒ½æ°´æ—é¤¨ã‚’æº€å–«ã€‚æ¢…å°è·¯å…¬åœ’ã§æ„å‘³ã‚‚ãªãå­ã©ã‚‚ãŸã¡ã¨é¬¼ã”ã£ã“ã—ã¦ç–²ã‚Œã‚‹ã€‚ãƒ›ãƒ†...</td>\n",
       "      <td>æ—¥æœ¬</td>\n",
       "      <td>äº¬éƒ½æ°´æ—é¤¨ (KYOTO AQUARIUM)</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>07:20:27</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4606</th>\n",
       "      <td>78149438</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>ã‹ã‚ã„ã„ã’ã‚“ãğŸ˜#ã„ã‚ã¦S1ã‚¹ã‚¤ãƒ¼ãƒ„ãƒ•ã‚§ã‚¢ https://t.co/bOOXYHfVDG</td>\n",
       "      <td>æ—¥æœ¬</td>\n",
       "      <td>ãƒ“ãƒƒã‚°ãƒ«ãƒ¼ãƒ•æ»æ²¢</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>07:07:52</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4627</th>\n",
       "      <td>1085457911332036609</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>ç§ã¯å–§å˜©ã—ãŸç›¸æ‰‹ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã¾ã§ã—ãªã‹ã£ãŸã€‚ãŸã¶ã‚“ã“ã‚Œã‚‚ãã®äººã¯è¦‹ã‚‹ã ã‚ã†ã€‚ä»Šã¯FFã§ã¯ãªã„ã...</td>\n",
       "      <td>æ—¥æœ¬</td>\n",
       "      <td>æ—¥æœ¬</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>06:34:42</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4628</th>\n",
       "      <td>1085457911332036609</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>æœã¯ã‚„ï¼è–¬å…¥ã‚ŒãŸã‹ã‚‰ã‚‚ã†ã¡ã‚‡ã£ã¨å¯ã‚‹ã‘ã©ã€ã“ã‚Œè¦‹ã‚‹ã‹ã‚ã‹ã‚‰ã‚“ã‘ã©ã€Twitterã£ã¦ã„ã‚ã‚“ãª...</td>\n",
       "      <td>æ—¥æœ¬</td>\n",
       "      <td>æ—¥æœ¬</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>06:34:41</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4662</th>\n",
       "      <td>2220016902</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>å®‡é‡ã‹ã‚‰é«˜æ¾ã«å››å›½ãƒ•ã‚§ãƒªãƒ¼ã¨åŒã˜ãã‚‰ã„ã®å€¤æ®µã¨æ™‚åˆ»ã§è¡Œãæ–¹æ³•ã¿ã¤ã‘ãŸç›´å³¶çµŒç”±ã—ã¦é«˜æ¾ã«è¡Œãã®...</td>\n",
       "      <td>æ—¥æœ¬</td>\n",
       "      <td>é“ã®é§… ã¿ã‚„ã¾å…¬åœ’</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>04:49:30</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id        date  \\\n",
       "10              214091568  2020-02-27   \n",
       "217             283672965  2020-02-26   \n",
       "224   1020450848395046912  2020-02-26   \n",
       "264            1481572735  2020-02-26   \n",
       "268             184733957  2020-02-26   \n",
       "...                   ...         ...   \n",
       "4596   786091920145735680  2020-02-24   \n",
       "4606             78149438  2020-02-24   \n",
       "4627  1085457911332036609  2020-02-24   \n",
       "4628  1085457911332036609  2020-02-24   \n",
       "4662           2220016902  2020-02-24   \n",
       "\n",
       "                                                  tweet nationality  \\\n",
       "10    ä»Šæ—¥ã¯ãŠèª•ç”Ÿæ—¥ã®ã“ã¡ã‚‰ã®æ–¹ã®è½èªä¼šã¸ãƒ¼ï¼ï¼ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ãƒ¼ãŠ—ï¸ä»Šæ—¥ã‚‚ç¬‘ã‚ã›ã¦ã‚‚ã‚‰ã„ã¾ã—...          æ—¥æœ¬   \n",
       "217   å…ˆé€±ã¯ã€ã‚·ã‚²æ¼”å‡ºã®ã€ã‚µã‚¼ãƒ³å¹•ã€ã‚’è¦³ã¦ãã¾ã—ãŸâœ¨å¾Œå‘³ã®è‰¯ã„èˆå°ã¯å¤§å¥½ãã§ã™â€¼ï¸å·æ³£ã‚’æ‡¸å¿µã—ã¦ã‚¿...          æ—¥æœ¬   \n",
       "224   ã€æ–°å®¿SAMURAIã‚ã‚ŠãŒã¨ã†ã€‘ğŸ’«2/26meiyoã‚»ãƒƒãƒˆãƒªã‚¹ãƒˆğŸ’«1.ã‚·ãƒˆãƒ©ã‚¹2.æº¶ã‘ã‚‹é’æ˜¥...          æ—¥æœ¬   \n",
       "264   ãƒã‚¸ã‹ãã‚„ã£ã±ãã†ãªã‚‹ã‚ˆãªãã‚ã£ã¡ã‚ƒæ¥½ã—ã¿ã«ã—ã¦ãŸã®ã«...(â—â€¸â—Ÿã†€)ã§ã‚‚ãƒ¡ãƒ³ãƒãƒ¼ã®æ–¹ãŒã‚¬...          æ—¥æœ¬   \n",
       "268   ãŠè…¹ã„ã£ã±ã„ã«ãªã£ã¦è½ã¡ç€ã„ãŸã®ã§ãƒ‡ã‚¶ãƒ¼ãƒˆã¨ç´…èŒ¶ã‚’é ‚ããªãŒã‚‰æ±äº¬ãƒ‰ãƒ¼ãƒ ã‚’çœºã‚ã¦æƒ³ã„ã«ãµã‘ã‚‹ã€‚...          æ—¥æœ¬   \n",
       "...                                                 ...         ...   \n",
       "4596  æ—…è¡Œè¨˜éŒ²1æ—¥ç›®ã€‚äº¬éƒ½æ°´æ—é¤¨ã‚’æº€å–«ã€‚æ¢…å°è·¯å…¬åœ’ã§æ„å‘³ã‚‚ãªãå­ã©ã‚‚ãŸã¡ã¨é¬¼ã”ã£ã“ã—ã¦ç–²ã‚Œã‚‹ã€‚ãƒ›ãƒ†...          æ—¥æœ¬   \n",
       "4606      ã‹ã‚ã„ã„ã’ã‚“ãğŸ˜#ã„ã‚ã¦S1ã‚¹ã‚¤ãƒ¼ãƒ„ãƒ•ã‚§ã‚¢ https://t.co/bOOXYHfVDG          æ—¥æœ¬   \n",
       "4627  ç§ã¯å–§å˜©ã—ãŸç›¸æ‰‹ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã¾ã§ã—ãªã‹ã£ãŸã€‚ãŸã¶ã‚“ã“ã‚Œã‚‚ãã®äººã¯è¦‹ã‚‹ã ã‚ã†ã€‚ä»Šã¯FFã§ã¯ãªã„ã...          æ—¥æœ¬   \n",
       "4628  æœã¯ã‚„ï¼è–¬å…¥ã‚ŒãŸã‹ã‚‰ã‚‚ã†ã¡ã‚‡ã£ã¨å¯ã‚‹ã‘ã©ã€ã“ã‚Œè¦‹ã‚‹ã‹ã‚ã‹ã‚‰ã‚“ã‘ã©ã€Twitterã£ã¦ã„ã‚ã‚“ãª...          æ—¥æœ¬   \n",
       "4662  å®‡é‡ã‹ã‚‰é«˜æ¾ã«å››å›½ãƒ•ã‚§ãƒªãƒ¼ã¨åŒã˜ãã‚‰ã„ã®å€¤æ®µã¨æ™‚åˆ»ã§è¡Œãæ–¹æ³•ã¿ã¤ã‘ãŸç›´å³¶çµŒç”±ã—ã¦é«˜æ¾ã«è¡Œãã®...          æ—¥æœ¬   \n",
       "\n",
       "                                   place_tweet  year  month  day  hour  \\\n",
       "10                     ã‚ã¹ã®ãƒãƒ«ã‚«ã‚¹ (Abeno Harukas)  2020      2   27     1   \n",
       "217                       ã‚ã†ã‚‹ã™ã½ã£ã¨ (èˆå°èŠ¸è¡“äº¤æµã‚»ãƒ³ã‚¿ãƒ¼)  2020      2   26    22   \n",
       "224                           Shinjuku SAMURAI  2020      2   26    22   \n",
       "264   ãƒã‚¹ã‚¿æ–°å®¿ (Shinjuku Expressway Bus Terminal)  2020      2   26    22   \n",
       "268                                        å™ã€…è‹‘  2020      2   26    22   \n",
       "...                                        ...   ...    ...  ...   ...   \n",
       "4596                    äº¬éƒ½æ°´æ—é¤¨ (KYOTO AQUARIUM)  2020      2   24     7   \n",
       "4606                                  ãƒ“ãƒƒã‚°ãƒ«ãƒ¼ãƒ•æ»æ²¢  2020      2   24     7   \n",
       "4627                                        æ—¥æœ¬  2020      2   24     6   \n",
       "4628                                        æ—¥æœ¬  2020      2   24     6   \n",
       "4662                                 é“ã®é§… ã¿ã‚„ã¾å…¬åœ’  2020      2   24     4   \n",
       "\n",
       "          time area  \n",
       "10    01:52:29  nan  \n",
       "217   22:47:25  nan  \n",
       "224   22:44:14  nan  \n",
       "264   22:22:50  nan  \n",
       "268   22:19:19  nan  \n",
       "...        ...  ...  \n",
       "4596  07:20:27  nan  \n",
       "4606  07:07:52  nan  \n",
       "4627  06:34:42  nan  \n",
       "4628  06:34:41  nan  \n",
       "4662  04:49:30  nan  \n",
       "\n",
       "[248 rows x 11 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= t.place_tweet[0].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>nationality</th>\n",
       "      <th>place_tweet</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>time</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1221438530867515392</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>è¿½åŠ B'zhideâ†ãƒãƒ³ãƒ‰ã˜ã‚ƒãªãã¦ã‚‚ã§ã‚‚çµµã‚’æãæ™‚ã¯çµ¶å¯¾#ç±³æ´¥ç„å¸«å›ºå®šã•ã‚Œã¾ã—ãŸã€‚æœ€è¿‘æã„...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Minamiboso-shi, Chiba</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>02:08:25</td>\n",
       "      <td>é–¢æ±åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1217417832020922368</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>ãã‚“ãªä¸­ã§ã€ç§ãŒå­¦ã‚“ã ç©ºæ‰‹ã¯ä»–ã®ç©ºæ‰‹æµæ´¾ã«ã¯ç„¡ã„ã€Œã‚µãƒã‚­ã€ã¨ã„ã†æŠ€è¡“ãŒã‚ã‚Šã¾ã—ãŸã€‚ã“ã‚Œã¯æ‰“æ’ƒ...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Itabashi-ku, Tokyo</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>01:41:40</td>\n",
       "      <td>é–¢æ±åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1221438530867515392</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>@95nEcyc70XvOl3i ãƒ“ãƒ‡ã‚ªã€å†™çœŸã€å±±ç¨‹æ’®ã£ã¦ã¾ã™â˜†ãã—ã¦æ—¢ã«æ‡ã‹ã—ã„ğŸ˜ğŸ’¦å«Œã€…æœŸ...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Minamiboso-shi, Chiba</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>01:34:32</td>\n",
       "      <td>é–¢æ±åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1217417832020922368</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>çš†ã•ã‚“ã“ã‚Œã€ã©ã†æ€ã„ã¾ã™ï¼Ÿã‚±ãƒ³ã‚«ã£ã¦ã€é¡”é¢ãƒ‘ãƒ³ãƒã™ã‚‹ã§ã—ã‚‡ï¼Ÿæ´ã‚€ã§ã—ã‚‡ï¼Ÿã‚±ãƒ³ã‚«ã«åå‰‡ã‚‚ä½•ã‚‚ãª...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Itabashi-ku, Tokyo</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>01:34:19</td>\n",
       "      <td>é–¢æ±åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1217417832020922368</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>å”çªã§ã™ãŒã€ã‚±ãƒ³ã‚«ã®è©±ã‚’ã—ã¾ã™ã€‚ç©ºæ‰‹ã‚’å­¦ã¶ã¨ã€ã‚±ãƒ³ã‚«ã«å¼±ããªã‚Šã¾ã™ã€‚é»’å¸¯ã¾ã§é£›ã³æŠœã‘ã¦ã—ã¾ãˆ...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Itabashi-ku, Tokyo</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>01:19:58</td>\n",
       "      <td>é–¢æ±åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4626</th>\n",
       "      <td>999942880486608897</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>ä»Šæ—¥ã®ãƒ‘ãƒ”ã‚³é«˜ç†±ã‚’å‡ºã—ãŸã®ã§ç—…é™¢ã«é€£ã‚Œã¦ã„ãã€‚å¾…åˆå®¤ã§ã—ã‚“ã©ãã†ã ã£ãŸã®ã§è‚©ã«ã‚‚ãŸã‚Œã•ã›ã¦ã‚„...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Sapporo-shi Kita-ku, Hokkaido</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>06:39:40</td>\n",
       "      <td>åŒ—æµ·é“åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4631</th>\n",
       "      <td>984416076220919808</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>@7eGKVE51tuxuPyg ç³¸äº•ã®å®ˆå‚™ã¯ä¸å®‰ã ã‚‰ã‘ã‚„ã‹ã‚‰é«˜å±±ä½¿ã†ã¹ãã ã¨æ€ã„ã¾ã™</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Ikaruga-cho, Nara</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>06:29:38</td>\n",
       "      <td>è¿‘ç•¿åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4649</th>\n",
       "      <td>1221774819248594945</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>@Sugi103258 @gorillaman017 ãŠé‡‘ãŒãªã„ã®ã§å›°ã‚Šã¾ã—ãŸ</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Takamatsu-shi, Kagawa</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>05:34:11</td>\n",
       "      <td>å››å›½åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4653</th>\n",
       "      <td>1227625153397309442</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>ç‹¬å±…ç”Ÿæ´»ã®ç‹¬å±…è€äººã®å­¤ç‹¬ã•ã¯æ‚²æƒ¨éãã‚‹ã‚‚ã®ã ã¨ã¤ãã¥ãæ€ã£ãŸã€‚å®¶æ—ãŒä¸€äººã‚‚å±…ãªã„ç‹¬å±…è€äººã¯ã“...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Higashisonogi-cho, Nagasaki</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>05:13:47</td>\n",
       "      <td>ä¹å·ãƒ»æ²–ç¸„åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4659</th>\n",
       "      <td>1102038905811955713</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>@kento47011659 åŸå› åˆ†ã‹ã‚‰ãªã„ã®ãŒä¸å®‰ã§ã™ã‚ˆã­ï¼Ÿç—…é™¢ã§æ¤œæŸ»ã—ã¦ãã ã•ã„ã­ã€‚</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Numazu-shi, Shizuoka</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>05:02:55</td>\n",
       "      <td>ä¸­éƒ¨åœ°æ–¹</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>558 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id        date  \\\n",
       "2     1221438530867515392  2020-02-27   \n",
       "16    1217417832020922368  2020-02-27   \n",
       "19    1221438530867515392  2020-02-27   \n",
       "20    1217417832020922368  2020-02-27   \n",
       "27    1217417832020922368  2020-02-27   \n",
       "...                   ...         ...   \n",
       "4626   999942880486608897  2020-02-24   \n",
       "4631   984416076220919808  2020-02-24   \n",
       "4649  1221774819248594945  2020-02-24   \n",
       "4653  1227625153397309442  2020-02-24   \n",
       "4659  1102038905811955713  2020-02-24   \n",
       "\n",
       "                                                  tweet nationality  \\\n",
       "2     è¿½åŠ B'zhideâ†ãƒãƒ³ãƒ‰ã˜ã‚ƒãªãã¦ã‚‚ã§ã‚‚çµµã‚’æãæ™‚ã¯çµ¶å¯¾#ç±³æ´¥ç„å¸«å›ºå®šã•ã‚Œã¾ã—ãŸã€‚æœ€è¿‘æã„...       Japan   \n",
       "16    ãã‚“ãªä¸­ã§ã€ç§ãŒå­¦ã‚“ã ç©ºæ‰‹ã¯ä»–ã®ç©ºæ‰‹æµæ´¾ã«ã¯ç„¡ã„ã€Œã‚µãƒã‚­ã€ã¨ã„ã†æŠ€è¡“ãŒã‚ã‚Šã¾ã—ãŸã€‚ã“ã‚Œã¯æ‰“æ’ƒ...       Japan   \n",
       "19    @95nEcyc70XvOl3i ãƒ“ãƒ‡ã‚ªã€å†™çœŸã€å±±ç¨‹æ’®ã£ã¦ã¾ã™â˜†ãã—ã¦æ—¢ã«æ‡ã‹ã—ã„ğŸ˜ğŸ’¦å«Œã€…æœŸ...       Japan   \n",
       "20    çš†ã•ã‚“ã“ã‚Œã€ã©ã†æ€ã„ã¾ã™ï¼Ÿã‚±ãƒ³ã‚«ã£ã¦ã€é¡”é¢ãƒ‘ãƒ³ãƒã™ã‚‹ã§ã—ã‚‡ï¼Ÿæ´ã‚€ã§ã—ã‚‡ï¼Ÿã‚±ãƒ³ã‚«ã«åå‰‡ã‚‚ä½•ã‚‚ãª...       Japan   \n",
       "27    å”çªã§ã™ãŒã€ã‚±ãƒ³ã‚«ã®è©±ã‚’ã—ã¾ã™ã€‚ç©ºæ‰‹ã‚’å­¦ã¶ã¨ã€ã‚±ãƒ³ã‚«ã«å¼±ããªã‚Šã¾ã™ã€‚é»’å¸¯ã¾ã§é£›ã³æŠœã‘ã¦ã—ã¾ãˆ...       Japan   \n",
       "...                                                 ...         ...   \n",
       "4626  ä»Šæ—¥ã®ãƒ‘ãƒ”ã‚³é«˜ç†±ã‚’å‡ºã—ãŸã®ã§ç—…é™¢ã«é€£ã‚Œã¦ã„ãã€‚å¾…åˆå®¤ã§ã—ã‚“ã©ãã†ã ã£ãŸã®ã§è‚©ã«ã‚‚ãŸã‚Œã•ã›ã¦ã‚„...       Japan   \n",
       "4631        @7eGKVE51tuxuPyg ç³¸äº•ã®å®ˆå‚™ã¯ä¸å®‰ã ã‚‰ã‘ã‚„ã‹ã‚‰é«˜å±±ä½¿ã†ã¹ãã ã¨æ€ã„ã¾ã™       Japan   \n",
       "4649            @Sugi103258 @gorillaman017 ãŠé‡‘ãŒãªã„ã®ã§å›°ã‚Šã¾ã—ãŸ       Japan   \n",
       "4653  ç‹¬å±…ç”Ÿæ´»ã®ç‹¬å±…è€äººã®å­¤ç‹¬ã•ã¯æ‚²æƒ¨éãã‚‹ã‚‚ã®ã ã¨ã¤ãã¥ãæ€ã£ãŸã€‚å®¶æ—ãŒä¸€äººã‚‚å±…ãªã„ç‹¬å±…è€äººã¯ã“...       Japan   \n",
       "4659       @kento47011659 åŸå› åˆ†ã‹ã‚‰ãªã„ã®ãŒä¸å®‰ã§ã™ã‚ˆã­ï¼Ÿç—…é™¢ã§æ¤œæŸ»ã—ã¦ãã ã•ã„ã­ã€‚       Japan   \n",
       "\n",
       "                        place_tweet  year  month  day  hour      time     area  \n",
       "2             Minamiboso-shi, Chiba  2020      2   27     2  02:08:25     é–¢æ±åœ°æ–¹  \n",
       "16               Itabashi-ku, Tokyo  2020      2   27     1  01:41:40     é–¢æ±åœ°æ–¹  \n",
       "19            Minamiboso-shi, Chiba  2020      2   27     1  01:34:32     é–¢æ±åœ°æ–¹  \n",
       "20               Itabashi-ku, Tokyo  2020      2   27     1  01:34:19     é–¢æ±åœ°æ–¹  \n",
       "27               Itabashi-ku, Tokyo  2020      2   27     1  01:19:58     é–¢æ±åœ°æ–¹  \n",
       "...                             ...   ...    ...  ...   ...       ...      ...  \n",
       "4626  Sapporo-shi Kita-ku, Hokkaido  2020      2   24     6  06:39:40    åŒ—æµ·é“åœ°æ–¹  \n",
       "4631              Ikaruga-cho, Nara  2020      2   24     6  06:29:38     è¿‘ç•¿åœ°æ–¹  \n",
       "4649          Takamatsu-shi, Kagawa  2020      2   24     5  05:34:11     å››å›½åœ°æ–¹  \n",
       "4653    Higashisonogi-cho, Nagasaki  2020      2   24     5  05:13:47  ä¹å·ãƒ»æ²–ç¸„åœ°æ–¹  \n",
       "4659           Numazu-shi, Shizuoka  2020      2   24     5  05:02:55     ä¸­éƒ¨åœ°æ–¹  \n",
       "\n",
       "[558 rows x 11 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[t.nationality == 'Japan']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### ***************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ORs\n",
    "zip_dir  = \"../data/dfs\" # new_dfs2_jst\n",
    "csvfiles = glob(os.path.join(zip_dir, 'dfor*'))\n",
    "\n",
    "dfors = []\n",
    "for csvfile in csvfiles:\n",
    "    df = pd.read_csv(csvfile).drop('Unnamed: 0', axis=1)\n",
    "    dfors.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Rts\n",
    "zip_dir  = \"../data/new_dfs2_jst\"\n",
    "csvfiles = glob(os.path.join(zip_dir, 'dfor*'))\n",
    "\n",
    "dfors = []\n",
    "for csvfile in csvfiles:\n",
    "    df1 = pd.read_csv(csvfile).drop('Unnamed: 0', axis=1)\n",
    "    dfors.append(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n"
     ]
    }
   ],
   "source": [
    "for df in dfors:\n",
    "    print(df.RT_flag.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n"
     ]
    }
   ],
   "source": [
    "for df in dfrts:\n",
    "    print(df.RT_flag.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of tweets included RT: 9286476\n"
     ]
    }
   ],
   "source": [
    "c = [len(df) for df in dfors]\n",
    "print('The amount of tweets included RT:', sum(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of tweets included RT: 6692071\n"
     ]
    }
   ],
   "source": [
    "c = [len(df) for df in dfrts]\n",
    "print('The amount of tweets included RT:', sum(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create csv by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_day(dfors, filetype, savedir, key_lang):\n",
    "    # Prepartion of dfw \n",
    "    results = []\n",
    "    dfs = []\n",
    "    dfw = pd.read_csv('../data/WordTimeSeries.csv', encoding='utf-8').rename(columns={'Unnamed: 0': 'types',\n",
    "                                                                                    'file name': 'file_name', \n",
    "                                                                                    'orignal form': 'orignal_form',\n",
    "                                                                                    'English translation': 'English_translation'})\n",
    "    booleanDictionary = {True: 'TRUE', False: 'FALSE'} # Drop nan and False in order to use query \n",
    "    dfw = dfw.replace(booleanDictionary)\n",
    "    dfw = dfw.dropna(how='all')\n",
    "    \n",
    "    # Prepration of keywords\n",
    "    filenames = [filename for filename in dfw.file_name.tolist() if filetype in filename]\n",
    "    xd = {}\n",
    "    xd['T'] = 'file_name.str.contains(\"T\")'\n",
    "    xd['D'] = 'file_name.str.contains(\"D\")'\n",
    "    xd['A'] = 'file_name.str.contains(\"A\")'\n",
    "    xd['V'] = 'file_name.str.contains(\"V\")'\n",
    "    xd['F'] = 'file_name.str.contains(\"F\")'\n",
    "    xd['C'] = 'file_name.str.contains(\"C\")'\n",
    "    \n",
    "    query = xd[filetype]\n",
    "    dfw_c = dfw.query(query, engine='python')\n",
    "    if key_lang == 'jp':\n",
    "        keywords = dfw_c.orignal_form.tolist() # English.ver: keywords = dfw_c.English_translation.tolist() \n",
    "    elif key_lang == 'en':\n",
    "        keywords = dfw_c.English_translation.tolist() \n",
    "    else:\n",
    "        print('Put accurate keyword language')\n",
    "            \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "         # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "        \n",
    "        # Add columns of keywords whose cell have 1 if this tweet includes a keyword \n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        \n",
    "        # Make rows\n",
    "        uni_dates = dfor_c['date'].tolist()\n",
    "        uni_dates = sorted(set(uni_dates), key=uni_dates.index) # date values\n",
    "        rows = []\n",
    "        for date in uni_dates:\n",
    "            d = {}\n",
    "            d['date'] = str(date)\n",
    "            for col in filenames:\n",
    "                _df = dfor_c.groupby('date').get_group(date)\n",
    "                d[col] = _df[col].sum()\n",
    "            rows.append(d)\n",
    "        \n",
    "        # Make cols\n",
    "        cols = filenames.copy()\n",
    "        cols.insert(0, 'date')\n",
    "        \n",
    "        # Make dfs with rows and cols\n",
    "        dft = pd.DataFrame(columns=cols)\n",
    "        for row in rows:\n",
    "            dft = dft.append(row, ignore_index=True) \n",
    "        dfs.append(dft)\n",
    "        \n",
    "    # Finally Connect dfs\n",
    "    for i, df in enumerate(dfs):\n",
    "        if i == 0:\n",
    "            dfnew = df\n",
    "        else:\n",
    "            dfnew = pd.concat([dfnew, df], axis=0)\n",
    "            \n",
    "    # Groupby and sort by date\n",
    "    dfnew = dfnew.groupby('date').sum()\n",
    "    \n",
    "    # Save\n",
    "    outname = filetype+'_original.csv'\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "    savename = os.path.join(outdir, outname)\n",
    "    dfnew.to_csv(savename)\n",
    "\n",
    "    return dfnew\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:38<00:00,  9.81s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:01<00:00, 12.16s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:09<00:00, 12.97s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:37<00:00,  3.78s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:54<00:00,  5.48s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:29<00:00,  8.92s/it]\n"
     ]
    }
   ],
   "source": [
    "# OR\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfors, filetype=filetype, savedir='../new_results_jst/orjp/day', key_lang='jp') # new_results_jst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:21<00:00,  2.10s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.27s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:10<00:00,  1.03s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:09<00:00,  1.07it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:09<00:00,  1.05it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "# OR\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfors, filetype=filetype, savedir='../new_results_jst/oren/day', key_lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:25<00:00, 20.53s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:34<00:00, 21.48s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:20<00:00, 20.05s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:41<00:00, 10.17s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:28<00:00,  8.88s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:53<00:00, 23.33s/it]\n"
     ]
    }
   ],
   "source": [
    "# RT\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfrts, filetype=filetype, savedir='../new_results_jst/rtjp/day', key_lang='jp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.91s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.51s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.33s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:11<00:00,  1.18s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.52s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:17<00:00,  1.74s/it]\n"
     ]
    }
   ],
   "source": [
    "# RT\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfrts, filetype=filetype, savedir='../new_results_jst/rten/day', key_lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create csv by hour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_hour(dfors, filetype, savedir, key_lang):\n",
    "    # Prepartion of dfw \n",
    "    results = []\n",
    "    dfs = []\n",
    "    dfw = pd.read_csv('../data/WordTimeSeries.csv', encoding='utf-8').rename(columns={'Unnamed: 0': 'types',\n",
    "                                                                                    'file name': 'file_name', \n",
    "                                                                                    'orignal form': 'orignal_form',\n",
    "                                                                                    'English translation': 'English_translation'})\n",
    "    booleanDictionary = {True: 'TRUE', False: 'FALSE'} # Drop nan and False in order to use query \n",
    "    dfw = dfw.replace(booleanDictionary)\n",
    "    dfw = dfw.dropna(how='all')\n",
    "\n",
    "    # Prepration of keywords\n",
    "    filenames = [filename for filename in dfw.file_name.tolist() if filetype in filename]\n",
    "    xd = {}\n",
    "    xd['T'] = 'file_name.str.contains(\"T\")'\n",
    "    xd['D'] = 'file_name.str.contains(\"D\")'\n",
    "    xd['A'] = 'file_name.str.contains(\"A\")'\n",
    "    xd['V'] = 'file_name.str.contains(\"V\")'\n",
    "    xd['F'] = 'file_name.str.contains(\"F\")'\n",
    "    xd['C'] = 'file_name.str.contains(\"C\")'\n",
    "\n",
    "    query = xd[filetype]\n",
    "    dfw_c = dfw.query(query, engine='python')\n",
    "    \n",
    "    # Keyword \n",
    "    if key_lang == 'jp':\n",
    "        keywords = dfw_c.orignal_form.tolist() # English.ver: keywords = dfw_c.English_translation.tolist() \n",
    "    elif key_lang == 'en':\n",
    "        keywords = dfw_c.English_translation.tolist() \n",
    "    else:\n",
    "        print('Put accurate keyword language')\n",
    "    \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "\n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        dfs.append(dfor_c)\n",
    "        \n",
    "    # Create csv file by day\n",
    "    all_df = []\n",
    "    nextdf, nextdf_date = '', ''\n",
    "    \n",
    "    # Save dir\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        unique_dates = df['date'].tolist()\n",
    "        unique_dates = sorted(set(unique_dates), key=unique_dates.index) # date values\n",
    "        df_date = df.groupby('date') # df grouped by date   \n",
    "\n",
    "        for date in unique_dates:\n",
    "            df_oneday = df_date.get_group(date) # df of one day\n",
    "            dfh = df_oneday.groupby('hour').sum()\n",
    "            dfh = dfh.drop(['year', 'month', 'user_id', 'RT_flag', 'day'],axis=1)\n",
    "            \n",
    "            # Save csv\n",
    "            outname = filetype+str(date)+'.csv'\n",
    "            savename = os.path.join(outdir, outname)\n",
    "            \n",
    "            if os.path.exists(savename):\n",
    "                # File1\n",
    "                existedfile = pd.read_csv(savename)\n",
    "                # ã„ã£ãŸã‚“csvã«ã™ã‚‹\n",
    "                _saveonce = 'once.csv'\n",
    "                dfh.to_csv(_saveonce)\n",
    "                # File2 \n",
    "                once = pd.read_csv(_saveonce)\n",
    "                # Concat \n",
    "                df_oneday1 = pd.concat([existedfile, once], axis=0)\n",
    "                # Finally save\n",
    "                dfh = df_oneday1.groupby('hour').sum()\n",
    "                \n",
    "            # Save to csv\n",
    "            dfh.to_csv(savename)\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Orjp, Oren, Rtjp, Rten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:24<00:00,  8.47s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:05<00:00, 12.57s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:12<00:00, 13.22s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:43<00:00,  4.33s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:54<00:00,  5.50s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:31<00:00,  9.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# Orjp\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfors, filetype=filetype, savedir='../new_results_jst2/orjp/hour', key_lang='jp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.57s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.26s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:10<00:00,  1.06s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:10<00:00,  1.01s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:10<00:00,  1.02s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "# Oren\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfors, filetype=filetype, savedir='../new_results_jst2/oren/hour', key_lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:25<00:00,  8.57s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:31<00:00,  9.10s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:42<00:00, 10.22s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:38<00:00,  3.90s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:05<00:00,  6.53s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:56<00:00, 11.65s/it]\n"
     ]
    }
   ],
   "source": [
    "# Rtjp\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfrts, filetype=filetype, savedir='../new_results_jst2/rtjp/hour', key_lang='jp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.37s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:14<00:00,  1.46s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.55s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.20s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.23s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:17<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "# Rten\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfrts, filetype=filetype, savedir='../new_results_jst2/rten/hour', key_lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7827"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft = pd.read_csv('../results/rtjp/hour/T2020-02-28.csv')\n",
    "dft.T1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tT1\n",
    "21\t2958\n",
    "22\t2637\n",
    "23\t2546\n",
    "24\t4883\n",
    "25\t3352\n",
    "26\t2553\n",
    "27\t3206\n",
    "28\t5117\n",
    "29\t7827\n",
    "30\t2925"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã‚³ãƒ­ãƒŠå«æœ‰ç‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "963821 tweets conatin \"ã‚³ãƒ­ãƒŠ\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_corona(dfors, filetype=\"T\"):\n",
    "    # Prepration of keywords\n",
    "    ## ã‚³ãƒ­ãƒŠå°‚ç”¨\n",
    "    keywords = ['ã‚³ãƒ­ãƒŠ']\n",
    "    \n",
    "    c = [] # count volume\n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "        \n",
    "        c.append(len(dfor_c))\n",
    "    \n",
    "    print(sum(c), 'tweets conatin \"ã‚³ãƒ­ãƒŠ\"')\n",
    "\n",
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "findtwt_corona(dfrts, filetype=\"T\")\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corona by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwtCrn_day(dfors, savename, savedir):\n",
    "    \n",
    "    dfs = []\n",
    "\n",
    "    # Keyowrds\n",
    "    keywords = ['ã‚³ãƒ­ãƒŠ', 'Corona', 'corona']\n",
    "    filenames = ['ã‚³ãƒ­ãƒŠ', 'Corona', 'corona']\n",
    "\n",
    "    \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet include keywords\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "        \n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        \n",
    "        # Make rows\n",
    "        uni_dates = dfor_c['date'].tolist()\n",
    "        uni_dates = sorted(set(uni_dates), key=uni_dates.index) # date values\n",
    "        rows = []\n",
    "        for date in uni_dates:\n",
    "            d = {}\n",
    "            d['date'] = str(date)\n",
    "            for col in filenames:\n",
    "                _df = dfor_c.groupby('date').get_group(date)\n",
    "                d[col] = _df[col].sum()\n",
    "            rows.append(d)\n",
    "        \n",
    "        # Make cols\n",
    "        cols = filenames.copy()\n",
    "        cols.insert(0, 'date')\n",
    "        \n",
    "        # Make dfs with rows and cols\n",
    "        dft = pd.DataFrame(columns=cols)\n",
    "        for row in rows:\n",
    "            dft = dft.append(row, ignore_index=True)\n",
    "        \n",
    "        dfs.append(dft)\n",
    "\n",
    "    # Finally Connect dfs\n",
    "    for i, df in enumerate(dfs):\n",
    "        if i == 0:\n",
    "            dfnew = df\n",
    "        else:\n",
    "            dfnew = pd.concat([dfnew, df], axis=0)\n",
    "            \n",
    "    # Groupby and sort by date\n",
    "    dfnew = dfnew.groupby('date').sum()\n",
    "    \n",
    "    # Save\n",
    "    outname = savename+'.csv'\n",
    "    outdir = savedir\n",
    "\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "\n",
    "    savename = os.path.join(outdir, outname)    \n",
    "    dfnew.to_csv(savename)\n",
    "\n",
    "    return dfnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:18<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "dfnew = findtwtCrn_day(dfors, savename='corona', savedir='../new_results_jst/Corona/or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "dfnew = findtwtCrn_day(dfrts, savename='corona', savedir='../new_results_jst2/Corona/rt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corona by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findtwtCrn_hour(dfors, savename, savedir):\n",
    "    # Prepartion \n",
    "    results = []\n",
    "    dfs = []\n",
    "    \n",
    "    # Save dir\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "    \n",
    "    # Keyowrds\n",
    "    keywords = ['ã‚³ãƒ­ãƒŠ', 'Corona', 'corona']\n",
    "    filenames = ['ã‚³ãƒ­ãƒŠ', 'Corona', 'corona']\n",
    "    \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "\n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        \n",
    "        dfs.append(dfor_c)\n",
    "        \n",
    "    # Create csv file by day\n",
    "    all_df = []\n",
    "    nextdf, nextdf_date = '', ''\n",
    "    \n",
    "    # Save dir\n",
    "    outdir = savedir\n",
    "    filetype = savename\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        unique_dates = df['date'].tolist()\n",
    "        unique_dates = sorted(set(unique_dates), key=unique_dates.index) # date values\n",
    "        df_date = df.groupby('date') # df grouped by date   \n",
    "        \n",
    "        for date in unique_dates:\n",
    "            df_oneday = df_date.get_group(date) # df of one day\n",
    "            dfh = df_oneday.groupby('hour').sum()\n",
    "            dfh = dfh.drop(['year', 'month', 'user_id', 'RT_flag', 'day'],axis=1)\n",
    "            \n",
    "            outname = filetype+str(date)+'.csv'\n",
    "            savename = os.path.join(outdir, outname)\n",
    "            \n",
    "            if os.path.exists(savename):\n",
    "                # File1\n",
    "                existedfile = pd.read_csv(savename)\n",
    "                \n",
    "                # ã„ã£ãŸã‚“csvã«ã™ã‚‹\n",
    "                _saveonce = 'once.csv'\n",
    "                dfh.to_csv(_saveonce)\n",
    "                # File2 \n",
    "                once = pd.read_csv(_saveonce)\n",
    "                \n",
    "                # Concat \n",
    "                df_oneday1 = pd.concat([existedfile, once], axis=0)\n",
    "\n",
    "                # Finally save\n",
    "                dfh = df_oneday1.groupby('hour').sum()\n",
    "                \n",
    "            # Save to csv\n",
    "            dfh.to_csv(savename)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:09<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "findtwtCrn_hour(dfors, savename='corona', savedir='../new_results_jst2/Corona/or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "findtwtCrn_hour(dfrts, savename='corona', savedir='../new_results_jst/Corona/rt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_csv('../new_results_jst/rten/hour/V2020-02-01.csv')\n",
    "t2 = pd.read_csv('../new_results/rten/hour/V2020-01-31.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "print(t1.loc[0:8,'V4'].sum(), t2.V4.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>RT_flag</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1111855071380168705</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @k7LssPYI5D85fxI: https://t.co/5KgwBN00NSå¿—æ‘...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127471475</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @84MadokaMary: ã“ã®å›½ã«ã¯çŸ¥ã‚‰ã‚“ã ã‘ã§ã‚ã¡ã‚ƒã‚ã¡ã‚ƒãªé‡ã®ç¤¾ä¼šä¿éšœãŒã‚ã‚‹ã®...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138821520</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @zibumitunari: æ˜æ—¥ã¯ã‚¨ã‚¤ãƒ—ãƒªãƒ«ãƒ•ãƒ¼ãƒ«ã ãŒã€ã‚³ãƒ­ãƒŠé–¢é€£ã®å˜˜ã¯çµ¶å¯¾ã«ã‚„ã‚ã‚ˆ...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>962192359059472384</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @akagiichirou: ãƒã‚¹ã‚¯ã®ï¼—å‰²ãŒä¸­å›½ã‹ã‚‰ã®è¼¸å…¥ã«é ¼ã£ã¦ããŸãŒæµé€šç¶²ãŒæ··ä¹±ã™...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128360620</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @bbcnewsjapan: BBCãƒ‹ãƒ¥ãƒ¼ã‚¹ - ãƒãƒ³ã‚¬ãƒªãƒ¼æ”¿åºœã€æ–°å‹ã‚¦ã‚¤ãƒ«ã‚¹å¯¾ç­–ã§å¼·...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user_id        date  \\\n",
       "0  1111855071380168705  2020-03-31   \n",
       "1            127471475  2020-03-31   \n",
       "2            138821520  2020-03-31   \n",
       "3   962192359059472384  2020-03-31   \n",
       "4            128360620  2020-03-31   \n",
       "\n",
       "                                               tweet  RT_flag  year  month  \\\n",
       "0  RT @k7LssPYI5D85fxI: https://t.co/5KgwBN00NSå¿—æ‘...     True  2020      3   \n",
       "1  RT @84MadokaMary: ã“ã®å›½ã«ã¯çŸ¥ã‚‰ã‚“ã ã‘ã§ã‚ã¡ã‚ƒã‚ã¡ã‚ƒãªé‡ã®ç¤¾ä¼šä¿éšœãŒã‚ã‚‹ã®...     True  2020      3   \n",
       "2  RT @zibumitunari: æ˜æ—¥ã¯ã‚¨ã‚¤ãƒ—ãƒªãƒ«ãƒ•ãƒ¼ãƒ«ã ãŒã€ã‚³ãƒ­ãƒŠé–¢é€£ã®å˜˜ã¯çµ¶å¯¾ã«ã‚„ã‚ã‚ˆ...     True  2020      3   \n",
       "3  RT @akagiichirou: ãƒã‚¹ã‚¯ã®ï¼—å‰²ãŒä¸­å›½ã‹ã‚‰ã®è¼¸å…¥ã«é ¼ã£ã¦ããŸãŒæµé€šç¶²ãŒæ··ä¹±ã™...     True  2020      3   \n",
       "4  RT @bbcnewsjapan: BBCãƒ‹ãƒ¥ãƒ¼ã‚¹ - ãƒãƒ³ã‚¬ãƒªãƒ¼æ”¿åºœã€æ–°å‹ã‚¦ã‚¤ãƒ«ã‚¹å¯¾ç­–ã§å¼·...     True  2020      3   \n",
       "\n",
       "   day  hour      time  \n",
       "0   31    23  23:59:58  \n",
       "1   31    23  23:59:58  \n",
       "2   31    23  23:59:58  \n",
       "3   31    23  23:59:58  \n",
       "4   31    23  23:59:57  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfrts[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
