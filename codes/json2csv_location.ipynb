{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "from datetime import datetime\n",
    "import os\n",
    "from os import path\n",
    "from glob import glob\n",
    "\n",
    "import ijson\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_dir  = \"../data/zip2\"\n",
    "\n",
    "files = glob(os.path.join(zip_dir, '*'))\n",
    "for filename in files:\n",
    "    if '.zip' in filename:\n",
    "        pass\n",
    "    elif '.json' in filename:\n",
    "        pass\n",
    "    else:\n",
    "        newfile = filename+'.json'\n",
    "        os.rename(filename, newfile)\n",
    "\n",
    "jsons = glob(os.path.join(zip_dir, '*.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/zip2/part-v003-o001-r-00000.json',\n",
       " '../data/zip2/part-v003-o001-r-00001.json',\n",
       " '../data/zip2/part-v003-o001-r-00002.json',\n",
       " '../data/zip2/part-v003-o001-r-00003.json',\n",
       " '../data/zip2/part-v003-o001-r-00004.json',\n",
       " '../data/zip2/part-v003-o001-r-00005.json',\n",
       " '../data/zip2/part-v003-o001-r-00006.json',\n",
       " '../data/zip2/part-v003-o001-r-00007.json',\n",
       " '../data/zip2/part-v003-o001-r-00008.json',\n",
       " '../data/zip2/part-v003-o001-r-00009.json']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jsons = jsons[:2]\n",
    "jsons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading json elapsed_time:718.873034954071[sec]\n",
      "Loading json elapsed_time:648.138466835022[sec]\n",
      "Loading json elapsed_time:526.3302919864655[sec]\n",
      "Loading json elapsed_time:507.3329060077667[sec]\n",
      "Loading json elapsed_time:496.4747657775879[sec]\n",
      "Loading json elapsed_time:536.7752206325531[sec]\n",
      "Loading json elapsed_time:555.0317189693451[sec]\n",
      "Loading json elapsed_time:566.3812553882599[sec]\n",
      "Loading json elapsed_time:529.7259113788605[sec]\n",
      "Loading json elapsed_time:740.5443599224091[sec]\n"
     ]
    }
   ],
   "source": [
    "# jsons = ['../data/part_v003_o001_r_00000.json', ..., '../data/part_v003_o001_r_00001.json']\n",
    "\n",
    "dfrts = []\n",
    "dfors = []\n",
    "for i, json in enumerate(jsons):\n",
    "    # Instance Preparation\n",
    "    dates = []\n",
    "    tweets = []\n",
    "    user_ids = []\n",
    "#     rts = []\n",
    "\n",
    "#     # tweet's location\n",
    "#     place_tweet = []\n",
    "    # tweet's location\n",
    "    place_fullname_tweet = []\n",
    "    # tweet's country\n",
    "    nationality = []\n",
    "    \n",
    "    start = time.time()\n",
    "    # Load json file: date and tweet\n",
    "    with open(json, 'r', encoding='utf8') as file:\n",
    "        pet_parse = ijson.parse(file, multiple_values=True)\n",
    "        for prefix, event, value  in pet_parse:\n",
    "            # Date\n",
    "            if prefix == 'created_at':\n",
    "                dates.append(datetime.strptime(value, '%a %b %d %H:%M:%S +0000 %Y'))\n",
    "            # Tweet \n",
    "            if prefix == 'text':\n",
    "                tweets.append(value.replace('\\n', '').replace('\\t', '').replace('\\r', '').replace('\\r\\n', '').replace('ã€€', '')) # Delte space and indet and \\r\n",
    "            # User id \n",
    "            if prefix == 'user.id':\n",
    "                user_ids.append(value)\n",
    "                \n",
    "#             # RT Flag\n",
    "#             if len(dates)-1 == len(rts) and prefix == 'retweeted_status':\n",
    "#                 rts.append(True)\n",
    "#             if len(dates)-2 == len(rts):\n",
    "#                 rts.append(False)\n",
    "            \n",
    "#             # tweet's location\n",
    "#             if prefix == 'place.name':\n",
    "#                 place_tweet.append(value)\n",
    "#             if len(dates)-2 == len(place_tweet):\n",
    "#                 place_tweet.append('nan')\n",
    "\n",
    "            # tweet's location - fullname\n",
    "            if prefix == 'place.full_name':\n",
    "                place_fullname_tweet.append(value)\n",
    "            if len(dates)-2 == len(place_fullname_tweet):\n",
    "                place_fullname_tweet.append('nan')\n",
    "                \n",
    "            # tweet's nationality\n",
    "            if prefix == 'place.country':\n",
    "                nationality.append(value)\n",
    "            if len(dates)-2 == len(nationality):\n",
    "                nationality.append('nan')\n",
    "\n",
    "        # Add missing data\n",
    "#         if len(dates) != len(rts):\n",
    "#                 rts.append(False)\n",
    "        if len(dates) != len(place_fullname_tweet):\n",
    "                place_fullname_tweet.append('nan')\n",
    "        if len(dates) != len(nationality):\n",
    "                nationality.append('nan')\n",
    "\n",
    "    print(\"Loading json elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n",
    "    \n",
    "#     print(len(dates), len(tweets), len(user_ids), len(rts), len(nationality), len(place_tweet))\n",
    "    \n",
    "    # Create dataframe\n",
    "    data = np.vstack([user_ids, dates, tweets, nationality, place_fullname_tweet]).T\n",
    "    df = pd.DataFrame(data, columns=['user_id', 'date', 'tweet', 'nationality', 'place_tweet'])\n",
    "    \n",
    "    import datetime as dt\n",
    "    \n",
    "    # Df only with location data\n",
    "    df = df[df.place_tweet != 'nan']\n",
    "    \n",
    "    # Change time zone\n",
    "    df['date'] = df['date'] + dt.timedelta(hours = 9)\n",
    "    \n",
    "    # Date\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['time'] = df['date'].dt.time\n",
    "    df['date'] = df['date'].dt.date\n",
    "    \n",
    "#     # divide df by RT_flag \n",
    "#     dfrt = df[df['RT_flag'] == True]\n",
    "#     dfor = df[df['RT_flag'] == False]\n",
    "    \n",
    "    # Save to csv file\n",
    "    df.to_csv('../data/dfs_location2/df_lct'+str(i)+'.csv')\n",
    "#     dfrt.to_csv('../data/dfs_location/dfrt_lct'+str(i)+'.csv')\n",
    "    \n",
    "    # Append to dfs\n",
    "#     dfrts.append(dfrt)\n",
    "#     dfors.append(dfor)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### ***************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|            Variable Name|    Memory|\n",
      " ------------------------------------ \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"{}{: >25}{}{: >10}{}\".format('|','Variable Name','|','Memory','|'))\n",
    "print(\" ------------------------------------ \")\n",
    "for var_name in dir():\n",
    "    if not var_name.startswith(\"_\") and sys.getsizeof(eval(var_name)) > 10000: #ã“ã“ã ã‘ã‚¢ãƒ¬ãƒ³ã‚¸\n",
    "        print(\"{}{: >25}{}{: >10}{}\".format('|',var_name,'|',sys.getsizeof(eval(var_name)),'|'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv of dictionary\n",
    "def crate_area_dictionary(file):\n",
    "    area = pd.read_csv(file)\n",
    "    area_jp = area.drop(['pref_en'], axis=1).set_index(['pref_jp']).Area.to_dict()\n",
    "    area_en = area.drop(['pref_jp'], axis=1).set_index(['pref_en']).Area.to_dict()\n",
    "    return area_jp, area_en\n",
    "\n",
    "area_jp, area_en = crate_area_dictionary('../data/area.csv')\n",
    "area_shutoken_jp, area_shutoken_en = crate_area_dictionary('../data/area_shutoken.csv')\n",
    "area_tokyo_jp, area_tokyo_en = crate_area_dictionary('../data/area_tokyo.csv')\n",
    "\n",
    "\n",
    "def df_by_area(df, d_area_jp, d_area_en):\n",
    "    l_area = []\n",
    "    for natio, place in zip(df.nationality, df.place_tweet):\n",
    "        if natio == 'æ—¥æœ¬' and place.split(' ')[0] in d_area_jp:\n",
    "            l_area.append(d_area_jp[place.split(' ')[0]])\n",
    "        elif natio == 'æ—¥æœ¬' and place.split(' ')[-1] in d_area_jp:\n",
    "            l_area.append(d_area_jp[place.split(' ')[-1]])\n",
    "        elif l_area == 'Japan' and place.split(', ')[-1] in d_area_en:\n",
    "            l_area.append(d_area_en[place.split(', ')[-1]])\n",
    "        elif natio == 'Japan' and place.split(', ')[0] in d_area_en:\n",
    "            l_area.append(d_area_en[place.split(', ')[0]])\n",
    "        else:\n",
    "            l_area.append('nan')\n",
    "    df['area'] = l_area\n",
    "    df_lct = df[df.area != 'nan']\n",
    "    return df_lct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ORs\n",
    "zip_dir  = \"../data/dfs_location2\" # new_dfs2_jst\n",
    "csvfiles = glob(os.path.join(zip_dir, 'df*'))\n",
    "\n",
    "dfs = []\n",
    "for csvfile in csvfiles:\n",
    "    df = pd.read_csv(csvfile).drop('Unnamed: 0', axis=1)\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lcts = [df_by_area(df, area_jp, area_en)  for df in dfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze data monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat dfs\n",
    "for i, df in enumerate(df_lcts):\n",
    "    if i == 0:\n",
    "        dfm = df\n",
    "    else:\n",
    "        dfm = pd.concat([dfm, df], axis=0)\n",
    "_df = dfm.copy()\n",
    "df_shutoken = df_by_area(_df, area_shutoken_jp, area_shutoken_en)\n",
    "dfm = pd.concat([dfm, df_shutoken], axis=0)\n",
    "df_tokyo = df_by_area(_df, area_tokyo_jp, area_tokyo_en)\n",
    "dfm = pd.concat([dfm, df_tokyo], axis=0)\n",
    "# Drop axis\n",
    "dfm = dfm.drop(['nationality', 'place_tweet'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## group by area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_dict = {}\n",
    "areas = dfm.area.unique().tolist()\n",
    "initial_area = ['A','B','C','D','E','F','G','H','I','J']\n",
    "for area, initial in zip(areas, initial_area):\n",
    "    area_dict[area] = initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into csv of each area\n",
    "for area in areas:\n",
    "    dfm.groupby('area').get_group(area).to_csv('../data/dfs_by_area2/mar_'+area_dict[area]+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>time</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43809995</td>\n",
       "      <td>2020-02-29</td>\n",
       "      <td>@rihito_karube ãŠå¿™ã—ã„ã®ã«ãŠè¿”äº‹ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ãã‚ãã‚ç¤¾å†…ã§æ”¿æ²»éƒ¨ã¨ã‚¬...</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:52</td>\n",
       "      <td>é–¢æ±åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>949838786287906817</td>\n",
       "      <td>2020-02-29</td>\n",
       "      <td>ãƒã‚«ã¯è¨€ã„éãã§ã™ãŒã€ãªã‚“ã®æ ¹æ‹ ã‚‚ãªã„ãƒ‡ãƒã«è¸Šã‚‰ã•ã›ã¦â€¦ã“ã†ã„ã†ã®ã‚’ä»˜å’Œé›·åŒã£ã¦è¨€ã„ã¾ã™ã€‚ã¡...</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "      <td>23:58:47</td>\n",
       "      <td>ä¸­å›½åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>925958009381036032</td>\n",
       "      <td>2020-02-29</td>\n",
       "      <td>@piyo_0126 GPSæ’®å½±ä¼šã®äº•ã®é ­å…¬åœ’ã®æ¡œæ’®å½±ğŸŒ¸ã«ç”³ã—è¾¼ã¿ã¾ã—ãŸã€‚ã¾ã ä¸€åº¦ã‚‚æŠ½é¸ã«å½“...</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "      <td>23:58:41</td>\n",
       "      <td>é–¢æ±åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>143131975</td>\n",
       "      <td>2020-02-29</td>\n",
       "      <td>@mugithecat ã„ã„ã‚ˆã€‚ã„ã„ã‚ˆã€‚ä¿ºã‚‚ä¹…ã—ã¶ã‚Šã®é–¢è¥¿é å¾æ¥½ã—ã¿ã«ã—ã¦ãŸã®ã«å®¶ã§ãã ãã ...</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "      <td>23:58:04</td>\n",
       "      <td>é–¢æ±åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3312098114</td>\n",
       "      <td>2020-02-29</td>\n",
       "      <td>#ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹ã°ã‹ã‚Šã§ã‚¦ãƒ³ã‚¶ãƒªãªã®ã§å±±é›»ã®ç”»åƒã‚’è²¼ã‚‹6002ï¼‹6003ã®ç›´é€šç‰¹æ€¥ãŒæ‡ã‹ã—ã„...</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "      <td>23:57:20</td>\n",
       "      <td>è¿‘ç•¿åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3702</th>\n",
       "      <td>3050839291</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>@kent711hatogaya ãƒ©ãƒ”ã‚¢ãƒªãƒãƒ¬ãƒ«ã‚ã¡ã‚ƒã‚ã¡ã‚ƒæ€ ã„ã§ã™ã‚ˆğŸ˜…</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:15:37</td>\n",
       "      <td>æ±äº¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3703</th>\n",
       "      <td>2924259060</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>@ro_ktn å¤‰ãªäººã‚‚ã„ã‚‹ã‹ã‚‰æ°—ã‚’ä»˜ã‘ã¦ã‚ˆï½ğŸ¤”ç´ã£ã¡ã‚ƒã‚“å¯æ„›ã„ã‚“ã ã‹ã‚‰å¤œé“å¿ƒé…ã ã‚ˆâ€¦â€¦ğŸ˜©</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:14:30</td>\n",
       "      <td>æ±äº¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3705</th>\n",
       "      <td>197788314</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>æ¥­è€…ã•ã‚“ãŒæ³£ããã†ãªãã‚‰ã„ã‚¬ãƒƒã‚«ãƒªã—ã¦å¸°ã£ã¦ã„ã£ãŸã€‚ã ã£ã¦ã€æ•™ãˆã¦ãã‚Œãªã‹ã£ãŸã‹ã‚‰ç›´ãã«äºˆå®š...</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:12:39</td>\n",
       "      <td>æ±äº¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3712</th>\n",
       "      <td>93646446</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>çŠ ç‰²è€…ã«ã¯æ‚ªã„ã®ã ã‘ã©ã€èª­ã¾ã›ã‚‹æ‰‹è¨˜ã ãªã‚ã€‚ãã—ã¦ã€è­¦å¯Ÿã¯ã“ã‚Œèª­ã‚“ã§æƒ¨ã‚ã«ãªã‚‰ãªã„ã®ã‹ã­ï¼Ÿè‡ª...</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:09:40</td>\n",
       "      <td>æ±äº¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3714</th>\n",
       "      <td>2842016210</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>@rYz8i6qQfJK6ZGt å®Ÿã¯æ•´æ•°1ã¤ã«ã¤ã10ç‚¹ã®ç‚¹å–ã‚Šå•é¡Œèª¬(éŒ¯ä¹±)</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:07:47</td>\n",
       "      <td>æ±äº¬</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49919 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 user_id        date  \\\n",
       "0               43809995  2020-02-29   \n",
       "1     949838786287906817  2020-02-29   \n",
       "2     925958009381036032  2020-02-29   \n",
       "4              143131975  2020-02-29   \n",
       "5             3312098114  2020-02-29   \n",
       "...                  ...         ...   \n",
       "3702          3050839291  2020-02-01   \n",
       "3703          2924259060  2020-02-01   \n",
       "3705           197788314  2020-02-01   \n",
       "3712            93646446  2020-02-01   \n",
       "3714          2842016210  2020-02-01   \n",
       "\n",
       "                                                  tweet  year  month  day  \\\n",
       "0     @rihito_karube ãŠå¿™ã—ã„ã®ã«ãŠè¿”äº‹ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ãã‚ãã‚ç¤¾å†…ã§æ”¿æ²»éƒ¨ã¨ã‚¬...  2020      2   29   \n",
       "1     ãƒã‚«ã¯è¨€ã„éãã§ã™ãŒã€ãªã‚“ã®æ ¹æ‹ ã‚‚ãªã„ãƒ‡ãƒã«è¸Šã‚‰ã•ã›ã¦â€¦ã“ã†ã„ã†ã®ã‚’ä»˜å’Œé›·åŒã£ã¦è¨€ã„ã¾ã™ã€‚ã¡...  2020      2   29   \n",
       "2     @piyo_0126 GPSæ’®å½±ä¼šã®äº•ã®é ­å…¬åœ’ã®æ¡œæ’®å½±ğŸŒ¸ã«ç”³ã—è¾¼ã¿ã¾ã—ãŸã€‚ã¾ã ä¸€åº¦ã‚‚æŠ½é¸ã«å½“...  2020      2   29   \n",
       "4     @mugithecat ã„ã„ã‚ˆã€‚ã„ã„ã‚ˆã€‚ä¿ºã‚‚ä¹…ã—ã¶ã‚Šã®é–¢è¥¿é å¾æ¥½ã—ã¿ã«ã—ã¦ãŸã®ã«å®¶ã§ãã ãã ...  2020      2   29   \n",
       "5     #ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹ã°ã‹ã‚Šã§ã‚¦ãƒ³ã‚¶ãƒªãªã®ã§å±±é›»ã®ç”»åƒã‚’è²¼ã‚‹6002ï¼‹6003ã®ç›´é€šç‰¹æ€¥ãŒæ‡ã‹ã—ã„...  2020      2   29   \n",
       "...                                                 ...   ...    ...  ...   \n",
       "3702               @kent711hatogaya ãƒ©ãƒ”ã‚¢ãƒªãƒãƒ¬ãƒ«ã‚ã¡ã‚ƒã‚ã¡ã‚ƒæ€ ã„ã§ã™ã‚ˆğŸ˜…  2020      2    1   \n",
       "3703      @ro_ktn å¤‰ãªäººã‚‚ã„ã‚‹ã‹ã‚‰æ°—ã‚’ä»˜ã‘ã¦ã‚ˆï½ğŸ¤”ç´ã£ã¡ã‚ƒã‚“å¯æ„›ã„ã‚“ã ã‹ã‚‰å¤œé“å¿ƒé…ã ã‚ˆâ€¦â€¦ğŸ˜©  2020      2    1   \n",
       "3705  æ¥­è€…ã•ã‚“ãŒæ³£ããã†ãªãã‚‰ã„ã‚¬ãƒƒã‚«ãƒªã—ã¦å¸°ã£ã¦ã„ã£ãŸã€‚ã ã£ã¦ã€æ•™ãˆã¦ãã‚Œãªã‹ã£ãŸã‹ã‚‰ç›´ãã«äºˆå®š...  2020      2    1   \n",
       "3712  çŠ ç‰²è€…ã«ã¯æ‚ªã„ã®ã ã‘ã©ã€èª­ã¾ã›ã‚‹æ‰‹è¨˜ã ãªã‚ã€‚ãã—ã¦ã€è­¦å¯Ÿã¯ã“ã‚Œèª­ã‚“ã§æƒ¨ã‚ã«ãªã‚‰ãªã„ã®ã‹ã­ï¼Ÿè‡ª...  2020      2    1   \n",
       "3714           @rYz8i6qQfJK6ZGt å®Ÿã¯æ•´æ•°1ã¤ã«ã¤ã10ç‚¹ã®ç‚¹å–ã‚Šå•é¡Œèª¬(éŒ¯ä¹±)  2020      2    1   \n",
       "\n",
       "      hour      time  area  \n",
       "0       23  23:59:52  é–¢æ±åœ°æ–¹  \n",
       "1       23  23:58:47  ä¸­å›½åœ°æ–¹  \n",
       "2       23  23:58:41  é–¢æ±åœ°æ–¹  \n",
       "4       23  23:58:04  é–¢æ±åœ°æ–¹  \n",
       "5       23  23:57:20  è¿‘ç•¿åœ°æ–¹  \n",
       "...    ...       ...   ...  \n",
       "3702     0  00:15:37    æ±äº¬  \n",
       "3703     0  00:14:30    æ±äº¬  \n",
       "3705     0  00:12:39    æ±äº¬  \n",
       "3712     0  00:09:40    æ±äº¬  \n",
       "3714     0  00:07:47    æ±äº¬  \n",
       "\n",
       "[49919 rows x 9 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ORs\n",
    "zip_dir  = \"../data/dfs_by_area2\" # new_dfs2_jst\n",
    "csvfiles = glob(os.path.join(zip_dir, 'mar_*'))\n",
    "\n",
    "feb_dfs = []\n",
    "for csvfile in csvfiles:\n",
    "    df = pd.read_csv(csvfile).drop(['Unnamed: 0', 'area'], axis=1)\n",
    "    feb_dfs.append(df)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>778167031598374912</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>ãªã‚“ã‹ã‚‚ã†ã ã‚‹ã„è‡ªåˆ†ãŒå«Œ</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2989602496</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>æ—©ã!!å¯¾å¿œã—ã¦ãã ã•ã„ï¼é‡å…šã‚„ã‚‰ä¸å…šã‚„ã‚‰è¨€ã£ã¦ã‚‹å ´åˆã§ã¯ãªã„ã§ã™ã‚ˆï¼äº¡ããªã£ãŸæ–¹ã‚’æŒã¡ã ã—...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2300772428</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>ã¨ãƒ¼ã‚„ã¾æ ¡é•·10å¹´é–“ãŠç–²ã‚Œæ§˜ã§ã—ãŸç§ã®é’æ˜¥æ™‚ä»£ã«ã¯å¿…ãšè²´æ–¹ãŒã„ã¾ã—ãŸéŸ³æ¥½ã¨å¤¢ã¸ã®æ´»åŠ›ã‚’ä¸ãˆã¦...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:57:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>235017720</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>å¨˜ã‚„å¨˜ã¨ä»²è‰¯ãã—ã¦ãã‚ŒãŸãŠå‹é”ã«ã¯ã“ã‚“ãªæ™‚ã§ä¸å®‰ã ã‚ã†ã‘ã©æ˜æ—¥ã‹ã‚‰ã®ç¤¾ä¼šäººç”Ÿæ´»é ‘å¼µã£ã¦ã»ã—ã„...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:44:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1188641708751212544</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>æ˜æ—¥ã‹ã‚‰å¹´åº¦å§‹ã‚ã å¿™ã—ããªã‚‹ã‚ˆğŸ˜¢ä½•ã‚ˆã‚Šã‚³ãƒ­ãƒŠãŒæ°—ãŒã‹ã‚Šã ğŸ˜µ</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:36:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>604331616</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>ã€Œæœ¬å½“ã«ç§ã§ã‚‚å‰¯åå…¥ã‚’ã‚²ãƒƒãƒˆã§ãã‚‹ã®ï¼Ÿã€èª°ã§ã‚‚æœ€åˆã¯ä¸å®‰ã§ã™ã‚ˆã­ã€‚è‡ªåˆ†ã«ã‚‚ãƒ€ã‚¦ãƒ³ã•ã‚“ã«ã‚‚ç„¡ç†...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>01:56:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>604331616</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>ã€Œæœ¬å½“ã«ç§ã§ã‚‚å‰¯åå…¥ã‚’ã‚²ãƒƒãƒˆã§ãã‚‹ã®ï¼Ÿã€èª°ã§ã‚‚æœ€åˆã¯ä¸å®‰ã§ã™ã‚ˆã­ã€‚è‡ªåˆ†ã«ã‚‚ãƒ€ã‚¦ãƒ³ã•ã‚“ã«ã‚‚ç„¡ç†...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>01:14:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>3592667114</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>ãƒ€ãƒ«ã„ã€ã—ã€ã‚‚ã†è‰¯ã„ã‚„ã€é–‹å§‹ã¸è‡ªå‹•ã¸</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>01:00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>97611577</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>@xHaxrgiqIgj4DRK @sukai4055 ä½¿ã„ã‹ã‘ã ã¨å£²ã‚‰ã‚Œã¦ã„ã‚‹ã‚‚ã®ã¾ã§ã‚ã‚Š...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:22:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2132</th>\n",
       "      <td>2177307212</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>æ˜æ—¥ã‚‚æœã‹ã‚‰æˆ¦äº‰ã ã¨æ€ã†ã¨ã™ã‚“ã‚“ã‚“ã‚“ã”ã„æ†‚é¬±ã€‚</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:21:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2133 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id        date  \\\n",
       "0      778167031598374912  2020-03-31   \n",
       "1              2989602496  2020-03-31   \n",
       "2              2300772428  2020-03-31   \n",
       "3               235017720  2020-03-31   \n",
       "4     1188641708751212544  2020-03-31   \n",
       "...                   ...         ...   \n",
       "2128            604331616  2020-03-01   \n",
       "2129            604331616  2020-03-01   \n",
       "2130           3592667114  2020-03-01   \n",
       "2131             97611577  2020-03-01   \n",
       "2132           2177307212  2020-03-01   \n",
       "\n",
       "                                                  tweet  year  month  day  \\\n",
       "0                                          ãªã‚“ã‹ã‚‚ã†ã ã‚‹ã„è‡ªåˆ†ãŒå«Œ  2020      3   31   \n",
       "1     æ—©ã!!å¯¾å¿œã—ã¦ãã ã•ã„ï¼é‡å…šã‚„ã‚‰ä¸å…šã‚„ã‚‰è¨€ã£ã¦ã‚‹å ´åˆã§ã¯ãªã„ã§ã™ã‚ˆï¼äº¡ããªã£ãŸæ–¹ã‚’æŒã¡ã ã—...  2020      3   31   \n",
       "2     ã¨ãƒ¼ã‚„ã¾æ ¡é•·10å¹´é–“ãŠç–²ã‚Œæ§˜ã§ã—ãŸç§ã®é’æ˜¥æ™‚ä»£ã«ã¯å¿…ãšè²´æ–¹ãŒã„ã¾ã—ãŸéŸ³æ¥½ã¨å¤¢ã¸ã®æ´»åŠ›ã‚’ä¸ãˆã¦...  2020      3   31   \n",
       "3     å¨˜ã‚„å¨˜ã¨ä»²è‰¯ãã—ã¦ãã‚ŒãŸãŠå‹é”ã«ã¯ã“ã‚“ãªæ™‚ã§ä¸å®‰ã ã‚ã†ã‘ã©æ˜æ—¥ã‹ã‚‰ã®ç¤¾ä¼šäººç”Ÿæ´»é ‘å¼µã£ã¦ã»ã—ã„...  2020      3   31   \n",
       "4                         æ˜æ—¥ã‹ã‚‰å¹´åº¦å§‹ã‚ã å¿™ã—ããªã‚‹ã‚ˆğŸ˜¢ä½•ã‚ˆã‚Šã‚³ãƒ­ãƒŠãŒæ°—ãŒã‹ã‚Šã ğŸ˜µ  2020      3   31   \n",
       "...                                                 ...   ...    ...  ...   \n",
       "2128  ã€Œæœ¬å½“ã«ç§ã§ã‚‚å‰¯åå…¥ã‚’ã‚²ãƒƒãƒˆã§ãã‚‹ã®ï¼Ÿã€èª°ã§ã‚‚æœ€åˆã¯ä¸å®‰ã§ã™ã‚ˆã­ã€‚è‡ªåˆ†ã«ã‚‚ãƒ€ã‚¦ãƒ³ã•ã‚“ã«ã‚‚ç„¡ç†...  2020      3    1   \n",
       "2129  ã€Œæœ¬å½“ã«ç§ã§ã‚‚å‰¯åå…¥ã‚’ã‚²ãƒƒãƒˆã§ãã‚‹ã®ï¼Ÿã€èª°ã§ã‚‚æœ€åˆã¯ä¸å®‰ã§ã™ã‚ˆã­ã€‚è‡ªåˆ†ã«ã‚‚ãƒ€ã‚¦ãƒ³ã•ã‚“ã«ã‚‚ç„¡ç†...  2020      3    1   \n",
       "2130                                 ãƒ€ãƒ«ã„ã€ã—ã€ã‚‚ã†è‰¯ã„ã‚„ã€é–‹å§‹ã¸è‡ªå‹•ã¸  2020      3    1   \n",
       "2131  @xHaxrgiqIgj4DRK @sukai4055 ä½¿ã„ã‹ã‘ã ã¨å£²ã‚‰ã‚Œã¦ã„ã‚‹ã‚‚ã®ã¾ã§ã‚ã‚Š...  2020      3    1   \n",
       "2132                            æ˜æ—¥ã‚‚æœã‹ã‚‰æˆ¦äº‰ã ã¨æ€ã†ã¨ã™ã‚“ã‚“ã‚“ã‚“ã”ã„æ†‚é¬±ã€‚  2020      3    1   \n",
       "\n",
       "      hour      time  \n",
       "0       23  23:59:58  \n",
       "1       23  23:59:55  \n",
       "2       23  23:57:53  \n",
       "3       23  23:44:35  \n",
       "4       23  23:36:05  \n",
       "...    ...       ...  \n",
       "2128     1  01:56:32  \n",
       "2129     1  01:14:45  \n",
       "2130     1  01:00:03  \n",
       "2131     0  00:22:28  \n",
       "2132     0  00:21:21  \n",
       "\n",
       "[2133 rows x 8 columns]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feb_dfs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_hour(dfors, filetype, savedir, key_lang, area):\n",
    "    # Prepartion of dfw \n",
    "    results = []\n",
    "    dfs = []\n",
    "    dfw = pd.read_csv('../data/WordTimeSeries.csv', encoding='utf-8').rename(columns={'Unnamed: 0': 'types',\n",
    "                                                                                    'file name': 'file_name', \n",
    "                                                                                    'orignal form': 'orignal_form',\n",
    "                                                                                    'English translation': 'English_translation'})\n",
    "    booleanDictionary = {True: 'TRUE', False: 'FALSE'} # Drop nan and False in order to use query \n",
    "    dfw = dfw.replace(booleanDictionary)\n",
    "    dfw = dfw.dropna(how='all')\n",
    "\n",
    "    # Prepration of keywords\n",
    "    filenames = [filename for filename in dfw.file_name.tolist() if filetype in filename]\n",
    "    xd = {}\n",
    "    xd['T'] = 'file_name.str.contains(\"T\")'\n",
    "    xd['D'] = 'file_name.str.contains(\"D\")'\n",
    "    xd['A'] = 'file_name.str.contains(\"A\")'\n",
    "    xd['V'] = 'file_name.str.contains(\"V\")'\n",
    "    xd['F'] = 'file_name.str.contains(\"F\")'\n",
    "    xd['C'] = 'file_name.str.contains(\"C\")'\n",
    "\n",
    "    query = xd[filetype]\n",
    "    dfw_c = dfw.query(query, engine='python')\n",
    "    \n",
    "    # Keyword \n",
    "    if key_lang == 'jp':\n",
    "        keywords = dfw_c.orignal_form.tolist() # English.ver: keywords = dfw_c.English_translation.tolist() \n",
    "    elif key_lang == 'en':\n",
    "        keywords = dfw_c.English_translation.tolist() \n",
    "    else:\n",
    "        print('Put accurate keyword language')\n",
    "    \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "\n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        dfs.append(dfor_c)\n",
    "        \n",
    "    # Create csv file by day\n",
    "    all_df = []\n",
    "    nextdf, nextdf_date = '', ''\n",
    "    \n",
    "    # Save dir\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        unique_dates = df['date'].tolist()\n",
    "        unique_dates = sorted(set(unique_dates), key=unique_dates.index) # date values\n",
    "        df_date = df.groupby('date') # df grouped by date   \n",
    "\n",
    "        for date in unique_dates:\n",
    "            df_oneday = df_date.get_group(date) # df of one day\n",
    "            dfh = df_oneday.groupby('hour').sum()\n",
    "            dfh = dfh.drop(['year', 'month', 'user_id', 'day'],axis=1)\n",
    "        \n",
    "            # Save csv\n",
    "            outname = 'area'+area+'_'+filetype+str(date)+'.csv'\n",
    "            savename = os.path.join(outdir, outname)\n",
    "            \n",
    "            if os.path.exists(savename):\n",
    "                # File1\n",
    "#                 existedfile = pd.read_csv(savename)\n",
    "                # ã„ã£ãŸã‚“csvã«ã™ã‚‹\n",
    "                _saveonce = 'once.csv'\n",
    "                dfh.to_csv(_saveonce)\n",
    "                # File2 \n",
    "                once = pd.read_csv(_saveonce)\n",
    "                # Concat \n",
    "                df_oneday1 = pd.concat([existedfile, once], axis=0)\n",
    "                # Finally save\n",
    "                dfh = df_oneday1.groupby('hour').sum()\n",
    "                \n",
    "            # Save to csv\n",
    "            dfh.to_csv(savename)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12.40it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 11.33it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12.07it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 16.33it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14.57it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.72it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12.44it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 10.93it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 11.21it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 17.33it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 11.19it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.89it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13.16it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 11.08it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12.17it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.34it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.09it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.65it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13.79it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.02it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12.00it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 18.53it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15.59it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.74it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14.32it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 11.81it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 11.99it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 19.51it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15.61it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.46it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14.93it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 11.18it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12.09it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 17.57it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15.45it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.07it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13.63it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 10.80it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12.22it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 17.91it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15.78it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.19it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15.04it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12.56it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12.71it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 19.25it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15.46it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.39it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15.21it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12.46it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12.01it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 19.00it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15.20it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.59it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14.02it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.54it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.57it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12.31it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12.80it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['A', 'A', 'V', 'F', 'C']\n",
    "for area in initial_area:\n",
    "    for filetype in filetypes:\n",
    "        findtwt_hour(feb_dfs, filetype=filetype, savedir='../results_location2/hour', key_lang='jp', area=area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### ***************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ORs\n",
    "zip_dir  = \"../data/dfs\" # new_dfs2_jst\n",
    "csvfiles = glob(os.path.join(zip_dir, 'dfor*'))\n",
    "\n",
    "dfors = []\n",
    "for csvfile in csvfiles:\n",
    "    df = pd.read_csv(csvfile).drop('Unnamed: 0', axis=1)\n",
    "    dfors.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Rts\n",
    "zip_dir  = \"../data/new_dfs2_jst\"\n",
    "csvfiles = glob(os.path.join(zip_dir, 'dfor*'))\n",
    "\n",
    "dfors = []\n",
    "for csvfile in csvfiles:\n",
    "    df1 = pd.read_csv(csvfile).drop('Unnamed: 0', axis=1)\n",
    "    dfors.append(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n"
     ]
    }
   ],
   "source": [
    "for df in dfors:\n",
    "    print(df.RT_flag.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n"
     ]
    }
   ],
   "source": [
    "for df in dfrts:\n",
    "    print(df.RT_flag.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of tweets included RT: 9286476\n"
     ]
    }
   ],
   "source": [
    "c = [len(df) for df in dfors]\n",
    "print('The amount of tweets included RT:', sum(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of tweets included RT: 6692071\n"
     ]
    }
   ],
   "source": [
    "c = [len(df) for df in dfrts]\n",
    "print('The amount of tweets included RT:', sum(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create csv by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_day(dfors, filetype, savedir, key_lang):\n",
    "    # Prepartion of dfw \n",
    "    results = []\n",
    "    dfs = []\n",
    "    dfw = pd.read_csv('../data/WordTimeSeries.csv', encoding='utf-8').rename(columns={'Unnamed: 0': 'types',\n",
    "                                                                                    'file name': 'file_name', \n",
    "                                                                                    'orignal form': 'orignal_form',\n",
    "                                                                                    'English translation': 'English_translation'})\n",
    "    booleanDictionary = {True: 'TRUE', False: 'FALSE'} # Drop nan and False in order to use query \n",
    "    dfw = dfw.replace(booleanDictionary)\n",
    "    dfw = dfw.dropna(how='all')\n",
    "    \n",
    "    # Prepration of keywords\n",
    "    filenames = [filename for filename in dfw.file_name.tolist() if filetype in filename]\n",
    "    xd = {}\n",
    "    xd['T'] = 'file_name.str.contains(\"T\")'\n",
    "    xd['D'] = 'file_name.str.contains(\"D\")'\n",
    "    xd['A'] = 'file_name.str.contains(\"A\")'\n",
    "    xd['V'] = 'file_name.str.contains(\"V\")'\n",
    "    xd['F'] = 'file_name.str.contains(\"F\")'\n",
    "    xd['C'] = 'file_name.str.contains(\"C\")'\n",
    "    \n",
    "    query = xd[filetype]\n",
    "    dfw_c = dfw.query(query, engine='python')\n",
    "    if key_lang == 'jp':\n",
    "        keywords = dfw_c.orignal_form.tolist() # English.ver: keywords = dfw_c.English_translation.tolist() \n",
    "    elif key_lang == 'en':\n",
    "        keywords = dfw_c.English_translation.tolist() \n",
    "    else:\n",
    "        print('Put accurate keyword language')\n",
    "            \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "         # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "        \n",
    "        # Add columns of keywords whose cell have 1 if this tweet includes a keyword \n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        \n",
    "        # Make rows\n",
    "        uni_dates = dfor_c['date'].tolist()\n",
    "        uni_dates = sorted(set(uni_dates), key=uni_dates.index) # date values\n",
    "        rows = []\n",
    "        for date in uni_dates:\n",
    "            d = {}\n",
    "            d['date'] = str(date)\n",
    "            for col in filenames:\n",
    "                _df = dfor_c.groupby('date').get_group(date)\n",
    "                d[col] = _df[col].sum()\n",
    "            rows.append(d)\n",
    "        \n",
    "        # Make cols\n",
    "        cols = filenames.copy()\n",
    "        cols.insert(0, 'date')\n",
    "        \n",
    "        # Make dfs with rows and cols\n",
    "        dft = pd.DataFrame(columns=cols)\n",
    "        for row in rows:\n",
    "            dft = dft.append(row, ignore_index=True) \n",
    "        dfs.append(dft)\n",
    "        \n",
    "    # Finally Connect dfs\n",
    "    for i, df in enumerate(dfs):\n",
    "        if i == 0:\n",
    "            dfnew = df\n",
    "        else:\n",
    "            dfnew = pd.concat([dfnew, df], axis=0)\n",
    "            \n",
    "    # Groupby and sort by date\n",
    "    dfnew = dfnew.groupby('date').sum()\n",
    "    \n",
    "    # Save\n",
    "    outname = filetype+'_original.csv'\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "    savename = os.path.join(outdir, outname)\n",
    "    dfnew.to_csv(savename)\n",
    "\n",
    "    return dfnew\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:38<00:00,  9.81s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:01<00:00, 12.16s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:09<00:00, 12.97s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:37<00:00,  3.78s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:54<00:00,  5.48s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:29<00:00,  8.92s/it]\n"
     ]
    }
   ],
   "source": [
    "# OR\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfors, filetype=filetype, savedir='../new_results_jst/orjp/day', key_lang='jp') # new_results_jst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:21<00:00,  2.10s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.27s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:10<00:00,  1.03s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:09<00:00,  1.07it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:09<00:00,  1.05it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "# OR\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfors, filetype=filetype, savedir='../new_results_jst/oren/day', key_lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:25<00:00, 20.53s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:34<00:00, 21.48s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:20<00:00, 20.05s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:41<00:00, 10.17s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:28<00:00,  8.88s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:53<00:00, 23.33s/it]\n"
     ]
    }
   ],
   "source": [
    "# RT\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfrts, filetype=filetype, savedir='../new_results_jst/rtjp/day', key_lang='jp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.91s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.51s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.33s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:11<00:00,  1.18s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.52s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:17<00:00,  1.74s/it]\n"
     ]
    }
   ],
   "source": [
    "# RT\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfrts, filetype=filetype, savedir='../new_results_jst/rten/day', key_lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create csv by hour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_hour(dfors, filetype, savedir, key_lang):\n",
    "    # Prepartion of dfw \n",
    "    results = []\n",
    "    dfs = []\n",
    "    dfw = pd.read_csv('../data/WordTimeSeries.csv', encoding='utf-8').rename(columns={'Unnamed: 0': 'types',\n",
    "                                                                                    'file name': 'file_name', \n",
    "                                                                                    'orignal form': 'orignal_form',\n",
    "                                                                                    'English translation': 'English_translation'})\n",
    "    booleanDictionary = {True: 'TRUE', False: 'FALSE'} # Drop nan and False in order to use query \n",
    "    dfw = dfw.replace(booleanDictionary)\n",
    "    dfw = dfw.dropna(how='all')\n",
    "\n",
    "    # Prepration of keywords\n",
    "    filenames = [filename for filename in dfw.file_name.tolist() if filetype in filename]\n",
    "    xd = {}\n",
    "    xd['T'] = 'file_name.str.contains(\"T\")'\n",
    "    xd['D'] = 'file_name.str.contains(\"D\")'\n",
    "    xd['A'] = 'file_name.str.contains(\"A\")'\n",
    "    xd['V'] = 'file_name.str.contains(\"V\")'\n",
    "    xd['F'] = 'file_name.str.contains(\"F\")'\n",
    "    xd['C'] = 'file_name.str.contains(\"C\")'\n",
    "\n",
    "    query = xd[filetype]\n",
    "    dfw_c = dfw.query(query, engine='python')\n",
    "    \n",
    "    # Keyword \n",
    "    if key_lang == 'jp':\n",
    "        keywords = dfw_c.orignal_form.tolist() # English.ver: keywords = dfw_c.English_translation.tolist() \n",
    "    elif key_lang == 'en':\n",
    "        keywords = dfw_c.English_translation.tolist() \n",
    "    else:\n",
    "        print('Put accurate keyword language')\n",
    "    \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "\n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        dfs.append(dfor_c)\n",
    "        \n",
    "    # Create csv file by day\n",
    "    all_df = []\n",
    "    nextdf, nextdf_date = '', ''\n",
    "    \n",
    "    # Save dir\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        unique_dates = df['date'].tolist()\n",
    "        unique_dates = sorted(set(unique_dates), key=unique_dates.index) # date values\n",
    "        df_date = df.groupby('date') # df grouped by date   \n",
    "\n",
    "        for date in unique_dates:\n",
    "            df_oneday = df_date.get_group(date) # df of one day\n",
    "            dfh = df_oneday.groupby('hour').sum()\n",
    "            dfh = dfh.drop(['year', 'month', 'user_id', 'RT_flag', 'day'],axis=1)\n",
    "            \n",
    "            # Save csv\n",
    "            outname = filetype+str(date)+'.csv'\n",
    "            savename = os.path.join(outdir, outname)\n",
    "            \n",
    "            if os.path.exists(savename):\n",
    "                # File1\n",
    "                existedfile = pd.read_csv(savename)\n",
    "                # ã„ã£ãŸã‚“csvã«ã™ã‚‹\n",
    "                _saveonce = 'once.csv'\n",
    "                dfh.to_csv(_saveonce)\n",
    "                # File2 \n",
    "                once = pd.read_csv(_saveonce)\n",
    "                # Concat \n",
    "                df_oneday1 = pd.concat([existedfile, once], axis=0)\n",
    "                # Finally save\n",
    "                dfh = df_oneday1.groupby('hour').sum()\n",
    "                \n",
    "            # Save to csv\n",
    "            dfh.to_csv(savename)\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Orjp, Oren, Rtjp, Rten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:24<00:00,  8.47s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:05<00:00, 12.57s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:12<00:00, 13.22s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:43<00:00,  4.33s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:54<00:00,  5.50s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:31<00:00,  9.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# Orjp\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfors, filetype=filetype, savedir='../new_results_jst2/orjp/hour', key_lang='jp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.57s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.26s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:10<00:00,  1.06s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:10<00:00,  1.01s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:10<00:00,  1.02s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "# Oren\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfors, filetype=filetype, savedir='../new_results_jst2/oren/hour', key_lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:25<00:00,  8.57s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:31<00:00,  9.10s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:42<00:00, 10.22s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:38<00:00,  3.90s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:05<00:00,  6.53s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:56<00:00, 11.65s/it]\n"
     ]
    }
   ],
   "source": [
    "# Rtjp\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfrts, filetype=filetype, savedir='../new_results_jst2/rtjp/hour', key_lang='jp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.37s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:14<00:00,  1.46s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.55s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.20s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.23s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:17<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "# Rten\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfrts, filetype=filetype, savedir='../new_results_jst2/rten/hour', key_lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7827"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft = pd.read_csv('../results/rtjp/hour/T2020-02-28.csv')\n",
    "dft.T1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tT1\n",
    "21\t2958\n",
    "22\t2637\n",
    "23\t2546\n",
    "24\t4883\n",
    "25\t3352\n",
    "26\t2553\n",
    "27\t3206\n",
    "28\t5117\n",
    "29\t7827\n",
    "30\t2925"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã‚³ãƒ­ãƒŠå«æœ‰ç‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "963821 tweets conatin \"ã‚³ãƒ­ãƒŠ\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_corona(dfors, filetype=\"T\"):\n",
    "    # Prepration of keywords\n",
    "    ## ã‚³ãƒ­ãƒŠå°‚ç”¨\n",
    "    keywords = ['ã‚³ãƒ­ãƒŠ']\n",
    "    \n",
    "    c = [] # count volume\n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "        \n",
    "        c.append(len(dfor_c))\n",
    "    \n",
    "    print(sum(c), 'tweets conatin \"ã‚³ãƒ­ãƒŠ\"')\n",
    "\n",
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "findtwt_corona(dfrts, filetype=\"T\")\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corona by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwtCrn_day(dfors, savename, savedir):\n",
    "    \n",
    "    dfs = []\n",
    "\n",
    "    # Keyowrds\n",
    "    keywords = ['ã‚³ãƒ­ãƒŠ', 'Corona', 'corona']\n",
    "    filenames = ['ã‚³ãƒ­ãƒŠ', 'Corona', 'corona']\n",
    "\n",
    "    \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet include keywords\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "        \n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        \n",
    "        # Make rows\n",
    "        uni_dates = dfor_c['date'].tolist()\n",
    "        uni_dates = sorted(set(uni_dates), key=uni_dates.index) # date values\n",
    "        rows = []\n",
    "        for date in uni_dates:\n",
    "            d = {}\n",
    "            d['date'] = str(date)\n",
    "            for col in filenames:\n",
    "                _df = dfor_c.groupby('date').get_group(date)\n",
    "                d[col] = _df[col].sum()\n",
    "            rows.append(d)\n",
    "        \n",
    "        # Make cols\n",
    "        cols = filenames.copy()\n",
    "        cols.insert(0, 'date')\n",
    "        \n",
    "        # Make dfs with rows and cols\n",
    "        dft = pd.DataFrame(columns=cols)\n",
    "        for row in rows:\n",
    "            dft = dft.append(row, ignore_index=True)\n",
    "        \n",
    "        dfs.append(dft)\n",
    "\n",
    "    # Finally Connect dfs\n",
    "    for i, df in enumerate(dfs):\n",
    "        if i == 0:\n",
    "            dfnew = df\n",
    "        else:\n",
    "            dfnew = pd.concat([dfnew, df], axis=0)\n",
    "            \n",
    "    # Groupby and sort by date\n",
    "    dfnew = dfnew.groupby('date').sum()\n",
    "    \n",
    "    # Save\n",
    "    outname = savename+'.csv'\n",
    "    outdir = savedir\n",
    "\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "\n",
    "    savename = os.path.join(outdir, outname)    \n",
    "    dfnew.to_csv(savename)\n",
    "\n",
    "    return dfnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:18<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "dfnew = findtwtCrn_day(dfors, savename='corona', savedir='../new_results_jst/Corona/or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "dfnew = findtwtCrn_day(dfrts, savename='corona', savedir='../new_results_jst2/Corona/rt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corona by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findtwtCrn_hour(dfors, savename, savedir):\n",
    "    # Prepartion \n",
    "    results = []\n",
    "    dfs = []\n",
    "    \n",
    "    # Save dir\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "    \n",
    "    # Keyowrds\n",
    "    keywords = ['ã‚³ãƒ­ãƒŠ', 'Corona', 'corona']\n",
    "    filenames = ['ã‚³ãƒ­ãƒŠ', 'Corona', 'corona']\n",
    "    \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "\n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        \n",
    "        dfs.append(dfor_c)\n",
    "        \n",
    "    # Create csv file by day\n",
    "    all_df = []\n",
    "    nextdf, nextdf_date = '', ''\n",
    "    \n",
    "    # Save dir\n",
    "    outdir = savedir\n",
    "    filetype = savename\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        unique_dates = df['date'].tolist()\n",
    "        unique_dates = sorted(set(unique_dates), key=unique_dates.index) # date values\n",
    "        df_date = df.groupby('date') # df grouped by date   \n",
    "        \n",
    "        for date in unique_dates:\n",
    "            df_oneday = df_date.get_group(date) # df of one day\n",
    "            dfh = df_oneday.groupby('hour').sum()\n",
    "            dfh = dfh.drop(['year', 'month', 'user_id', 'RT_flag', 'day'],axis=1)\n",
    "            \n",
    "            outname = filetype+str(date)+'.csv'\n",
    "            savename = os.path.join(outdir, outname)\n",
    "            \n",
    "            if os.path.exists(savename):\n",
    "                # File1\n",
    "                existedfile = pd.read_csv(savename)\n",
    "                \n",
    "                # ã„ã£ãŸã‚“csvã«ã™ã‚‹\n",
    "                _saveonce = 'once.csv'\n",
    "                dfh.to_csv(_saveonce)\n",
    "                # File2 \n",
    "                once = pd.read_csv(_saveonce)\n",
    "                \n",
    "                # Concat \n",
    "                df_oneday1 = pd.concat([existedfile, once], axis=0)\n",
    "\n",
    "                # Finally save\n",
    "                dfh = df_oneday1.groupby('hour').sum()\n",
    "                \n",
    "            # Save to csv\n",
    "            dfh.to_csv(savename)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:09<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "findtwtCrn_hour(dfors, savename='corona', savedir='../new_results_jst2/Corona/or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "findtwtCrn_hour(dfrts, savename='corona', savedir='../new_results_jst/Corona/rt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_csv('../new_results_jst/rten/hour/V2020-02-01.csv')\n",
    "t2 = pd.read_csv('../new_results/rten/hour/V2020-01-31.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "print(t1.loc[0:8,'V4'].sum(), t2.V4.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>RT_flag</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1111855071380168705</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @k7LssPYI5D85fxI: https://t.co/5KgwBN00NSå¿—æ‘...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127471475</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @84MadokaMary: ã“ã®å›½ã«ã¯çŸ¥ã‚‰ã‚“ã ã‘ã§ã‚ã¡ã‚ƒã‚ã¡ã‚ƒãªé‡ã®ç¤¾ä¼šä¿éšœãŒã‚ã‚‹ã®...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138821520</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @zibumitunari: æ˜æ—¥ã¯ã‚¨ã‚¤ãƒ—ãƒªãƒ«ãƒ•ãƒ¼ãƒ«ã ãŒã€ã‚³ãƒ­ãƒŠé–¢é€£ã®å˜˜ã¯çµ¶å¯¾ã«ã‚„ã‚ã‚ˆ...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>962192359059472384</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @akagiichirou: ãƒã‚¹ã‚¯ã®ï¼—å‰²ãŒä¸­å›½ã‹ã‚‰ã®è¼¸å…¥ã«é ¼ã£ã¦ããŸãŒæµé€šç¶²ãŒæ··ä¹±ã™...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128360620</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @bbcnewsjapan: BBCãƒ‹ãƒ¥ãƒ¼ã‚¹ - ãƒãƒ³ã‚¬ãƒªãƒ¼æ”¿åºœã€æ–°å‹ã‚¦ã‚¤ãƒ«ã‚¹å¯¾ç­–ã§å¼·...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user_id        date  \\\n",
       "0  1111855071380168705  2020-03-31   \n",
       "1            127471475  2020-03-31   \n",
       "2            138821520  2020-03-31   \n",
       "3   962192359059472384  2020-03-31   \n",
       "4            128360620  2020-03-31   \n",
       "\n",
       "                                               tweet  RT_flag  year  month  \\\n",
       "0  RT @k7LssPYI5D85fxI: https://t.co/5KgwBN00NSå¿—æ‘...     True  2020      3   \n",
       "1  RT @84MadokaMary: ã“ã®å›½ã«ã¯çŸ¥ã‚‰ã‚“ã ã‘ã§ã‚ã¡ã‚ƒã‚ã¡ã‚ƒãªé‡ã®ç¤¾ä¼šä¿éšœãŒã‚ã‚‹ã®...     True  2020      3   \n",
       "2  RT @zibumitunari: æ˜æ—¥ã¯ã‚¨ã‚¤ãƒ—ãƒªãƒ«ãƒ•ãƒ¼ãƒ«ã ãŒã€ã‚³ãƒ­ãƒŠé–¢é€£ã®å˜˜ã¯çµ¶å¯¾ã«ã‚„ã‚ã‚ˆ...     True  2020      3   \n",
       "3  RT @akagiichirou: ãƒã‚¹ã‚¯ã®ï¼—å‰²ãŒä¸­å›½ã‹ã‚‰ã®è¼¸å…¥ã«é ¼ã£ã¦ããŸãŒæµé€šç¶²ãŒæ··ä¹±ã™...     True  2020      3   \n",
       "4  RT @bbcnewsjapan: BBCãƒ‹ãƒ¥ãƒ¼ã‚¹ - ãƒãƒ³ã‚¬ãƒªãƒ¼æ”¿åºœã€æ–°å‹ã‚¦ã‚¤ãƒ«ã‚¹å¯¾ç­–ã§å¼·...     True  2020      3   \n",
       "\n",
       "   day  hour      time  \n",
       "0   31    23  23:59:58  \n",
       "1   31    23  23:59:58  \n",
       "2   31    23  23:59:58  \n",
       "3   31    23  23:59:58  \n",
       "4   31    23  23:59:57  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfrts[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
