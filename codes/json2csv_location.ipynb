{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "from datetime import datetime\n",
    "import os\n",
    "from os import path\n",
    "from glob import glob\n",
    "\n",
    "import ijson\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_dir  = \"../data/zip2\"\n",
    "\n",
    "files = glob(os.path.join(zip_dir, '*'))\n",
    "for filename in files:\n",
    "    if '.zip' in filename:\n",
    "        pass\n",
    "    elif '.json' in filename:\n",
    "        pass\n",
    "    else:\n",
    "        newfile = filename+'.json'\n",
    "        os.rename(filename, newfile)\n",
    "\n",
    "jsons = glob(os.path.join(zip_dir, '*.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading json elapsed_time:718.873034954071[sec]\n",
      "Loading json elapsed_time:648.138466835022[sec]\n",
      "Loading json elapsed_time:526.3302919864655[sec]\n",
      "Loading json elapsed_time:507.3329060077667[sec]\n",
      "Loading json elapsed_time:496.4747657775879[sec]\n",
      "Loading json elapsed_time:536.7752206325531[sec]\n",
      "Loading json elapsed_time:555.0317189693451[sec]\n",
      "Loading json elapsed_time:566.3812553882599[sec]\n",
      "Loading json elapsed_time:529.7259113788605[sec]\n",
      "Loading json elapsed_time:740.5443599224091[sec]\n"
     ]
    }
   ],
   "source": [
    "dfrts = []\n",
    "dfors = []\n",
    "for i, json in enumerate(jsons):\n",
    "    # Instance Preparation\n",
    "    dates = []\n",
    "    tweets = []\n",
    "    user_ids = []\n",
    "    \n",
    "    # tweet's location\n",
    "    place_fullname_tweet = []\n",
    "    # tweet's country\n",
    "    nationality = []\n",
    "    \n",
    "    start = time.time()\n",
    "    # Load json file: date and tweet\n",
    "    with open(json, 'r', encoding='utf8') as file:\n",
    "        pet_parse = ijson.parse(file, multiple_values=True)\n",
    "        for prefix, event, value  in pet_parse:\n",
    "            # Date\n",
    "            if prefix == 'created_at':\n",
    "                dates.append(datetime.strptime(value, '%a %b %d %H:%M:%S +0000 %Y'))\n",
    "            # Tweet \n",
    "            if prefix == 'text':\n",
    "                tweets.append(value.replace('\\n', '').replace('\\t', '').replace('\\r', '').replace('\\r\\n', '').replace('　', '')) # Delte space and indet and \\r\n",
    "            # User id \n",
    "            if prefix == 'user.id':\n",
    "                user_ids.append(value)\n",
    "\n",
    "            # tweet's location - fullname\n",
    "            if prefix == 'place.full_name':\n",
    "                place_fullname_tweet.append(value)\n",
    "            if len(dates)-2 == len(place_fullname_tweet):\n",
    "                place_fullname_tweet.append('nan')\n",
    "                \n",
    "            # tweet's nationality\n",
    "            if prefix == 'place.country':\n",
    "                nationality.append(value)\n",
    "            if len(dates)-2 == len(nationality):\n",
    "                nationality.append('nan')\n",
    "\n",
    "        # Add missing last one data\n",
    "        if len(dates) != len(place_fullname_tweet):\n",
    "                place_fullname_tweet.append('nan')\n",
    "        if len(dates) != len(nationality):\n",
    "                nationality.append('nan')\n",
    "\n",
    "    print(\"Loading json elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n",
    "    \n",
    "    # Create dataframe\n",
    "    data = np.vstack([user_ids, dates, tweets, nationality, place_fullname_tweet]).T\n",
    "    df = pd.DataFrame(data, columns=['user_id', 'date', 'tweet', 'nationality', 'place_tweet'])\n",
    "    \n",
    "    import datetime as dt\n",
    "    \n",
    "    # Df only with location data\n",
    "    df = df[df.place_tweet != 'nan']\n",
    "    \n",
    "    # Change time zone\n",
    "    df['date'] = df['date'] + dt.timedelta(hours = 9)\n",
    "    \n",
    "    # Date\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['time'] = df['date'].dt.time\n",
    "    df['date'] = df['date'].dt.date\n",
    "    \n",
    "#     # divide df by RT_flag \n",
    "#     dfrt = df[df['RT_flag'] == True]\n",
    "#     dfor = df[df['RT_flag'] == False]\n",
    "    \n",
    "    # Save to csv file\n",
    "    df.to_csv('../data/dfs_location2/df_lct'+str(i)+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv of dictionary\n",
    "def crate_area_dictionary(file):\n",
    "    area = pd.read_csv(file)\n",
    "    area_jp = area.drop(['pref_en'], axis=1).set_index(['pref_jp']).Area.to_dict()\n",
    "    area_en = area.drop(['pref_jp'], axis=1).set_index(['pref_en']).Area.to_dict()\n",
    "    return area_jp, area_en\n",
    "\n",
    "area_jp, area_en = crate_area_dictionary('../data/area.csv')\n",
    "area_shutoken_jp, area_shutoken_en = crate_area_dictionary('../data/area_shutoken.csv')\n",
    "area_tokyo_jp, area_tokyo_en = crate_area_dictionary('../data/area_tokyo.csv')\n",
    "area_kitakan_jp, area_kitakan_en = crate_area_dictionary('../data/area_kitakanto.csv')\n",
    "\n",
    "# Divide tweet by area\n",
    "def df_by_area(df, d_area_jp, d_area_en):\n",
    "    l_area = []\n",
    "    for natio, place in zip(df.nationality, df.place_tweet):\n",
    "        if natio == '日本' and place.split(' ')[0] in d_area_jp:\n",
    "            l_area.append(d_area_jp[place.split(' ')[0]])\n",
    "        elif natio == '日本' and place.split(' ')[-1] in d_area_jp:\n",
    "            l_area.append(d_area_jp[place.split(' ')[-1]])\n",
    "        elif l_area == 'Japan' and place.split(', ')[-1] in d_area_en:\n",
    "            l_area.append(d_area_en[place.split(', ')[-1]])\n",
    "        elif natio == 'Japan' and place.split(', ')[0] in d_area_en:\n",
    "            l_area.append(d_area_en[place.split(', ')[0]])\n",
    "        else:\n",
    "            l_area.append('nan')\n",
    "    df['area'] = l_area\n",
    "    df_lct = df[df.area != 'nan']\n",
    "    return df_lct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "zip_dir  = \"../data/dfs_location2\" # new_dfs2_jst\n",
    "csvfiles = glob(os.path.join(zip_dir, 'df*'))\n",
    "\n",
    "dfs = []\n",
    "for csvfile in csvfiles:\n",
    "    df = pd.read_csv(csvfile).drop('Unnamed: 0', axis=1)\n",
    "    dfs.append(df)\n",
    "    \n",
    "# Connet all df to one data\n",
    "df_lcts = [df_by_area(df, area_jp, area_en)  for df in dfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze data monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat dfs\n",
    "for i, df in enumerate(df_lcts):\n",
    "    if i == 0:\n",
    "        dfm = df\n",
    "    else:\n",
    "        dfm = pd.concat([dfm, df], axis=0)\n",
    "_df = dfm.copy()\n",
    "# 首都圏\n",
    "df_shutoken = df_by_area(_df, area_shutoken_jp, area_shutoken_en)\n",
    "dfm = pd.concat([dfm, df_shutoken], axis=0)\n",
    "# 東京\n",
    "df_tokyo = df_by_area(_df, area_tokyo_jp, area_tokyo_en)\n",
    "dfm = pd.concat([dfm, df_tokyo], axis=0)\n",
    "# Drop axis\n",
    "dfm = dfm.drop(['nationality', 'place_tweet'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>time</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>778167031598374912</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>なんかもうだるい自分が嫌</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "      <td>東北地方</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2989602496</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>早く!!対応してください！野党やら与党やら言ってる場合ではないですよ！亡くなった方を持ちだし...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:55</td>\n",
       "      <td>東北地方</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>986162527007490048</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>宮藤官九郎さんが感染してしまったみたいですが22日に中山優馬くんの舞台に宮藤官九郎さんと日村...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:34</td>\n",
       "      <td>関東地方</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3281248128</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>はぁぁ、すでに緊張するなぁ。。。</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:58:33</td>\n",
       "      <td>中部地方</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50943527</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>#志村けん面白い。見たら泣けてきた。悲しい。</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:58:11</td>\n",
       "      <td>関東地方</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4973</th>\n",
       "      <td>334036907</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>安倍ちゃんにはまだ辞められちゃ困る。ドン底まで見届けて貰わないと</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:13:24</td>\n",
       "      <td>東京</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4975</th>\n",
       "      <td>611507926</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>@nyantaremama ティッシュ一箱買うのに‼️何軒まわったか？結局地元のコンビニ🏪に...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:13:11</td>\n",
       "      <td>東京</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>352308836</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>株安で年金がまた、減らされる‼️株式投資しろはと言ってないのに何だか文句ばかりの、ツイート自...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:04:04</td>\n",
       "      <td>東京</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>1635284976</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>熊本は3年+2年（間あいてる）住んでたけど、パルコなくなったの悲しい。もっと長く住んでる熊本...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:03:22</td>\n",
       "      <td>東京</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>141486067</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>怒りをどこかにぶつけたくなるのはわかるけど、店員さんにぶつける奴もいるんだな。こういう事態の...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:01:33</td>\n",
       "      <td>東京</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57880 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 user_id        date  \\\n",
       "0     778167031598374912  2020-03-31   \n",
       "1             2989602496  2020-03-31   \n",
       "2     986162527007490048  2020-03-31   \n",
       "3             3281248128  2020-03-31   \n",
       "4               50943527  2020-03-31   \n",
       "...                  ...         ...   \n",
       "4973           334036907  2020-03-01   \n",
       "4975           611507926  2020-03-01   \n",
       "4992           352308836  2020-03-01   \n",
       "4993          1635284976  2020-03-01   \n",
       "4994           141486067  2020-03-01   \n",
       "\n",
       "                                                  tweet  year  month  day  \\\n",
       "0                                          なんかもうだるい自分が嫌  2020      3   31   \n",
       "1     早く!!対応してください！野党やら与党やら言ってる場合ではないですよ！亡くなった方を持ちだし...  2020      3   31   \n",
       "2     宮藤官九郎さんが感染してしまったみたいですが22日に中山優馬くんの舞台に宮藤官九郎さんと日村...  2020      3   31   \n",
       "3                                      はぁぁ、すでに緊張するなぁ。。。  2020      3   31   \n",
       "4                                #志村けん面白い。見たら泣けてきた。悲しい。  2020      3   31   \n",
       "...                                                 ...   ...    ...  ...   \n",
       "4973                   安倍ちゃんにはまだ辞められちゃ困る。ドン底まで見届けて貰わないと  2020      3    1   \n",
       "4975  @nyantaremama ティッシュ一箱買うのに‼️何軒まわったか？結局地元のコンビニ🏪に...  2020      3    1   \n",
       "4992  株安で年金がまた、減らされる‼️株式投資しろはと言ってないのに何だか文句ばかりの、ツイート自...  2020      3    1   \n",
       "4993  熊本は3年+2年（間あいてる）住んでたけど、パルコなくなったの悲しい。もっと長く住んでる熊本...  2020      3    1   \n",
       "4994  怒りをどこかにぶつけたくなるのはわかるけど、店員さんにぶつける奴もいるんだな。こういう事態の...  2020      3    1   \n",
       "\n",
       "      hour      time  area  \n",
       "0       23  23:59:58  東北地方  \n",
       "1       23  23:59:55  東北地方  \n",
       "2       23  23:59:34  関東地方  \n",
       "3       23  23:58:33  中部地方  \n",
       "4       23  23:58:11  関東地方  \n",
       "...    ...       ...   ...  \n",
       "4973     0  00:13:24    東京  \n",
       "4975     0  00:13:11    東京  \n",
       "4992     0  00:04:04    東京  \n",
       "4993     0  00:03:22    東京  \n",
       "4994     0  00:01:33    東京  \n",
       "\n",
       "[57880 rows x 9 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## group by area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_dict = {}\n",
    "# areas = dfm.area.unique().tolist()\n",
    "areas = ['北海道地方',\n",
    "         '東北地方',\n",
    "         '関東地方',\n",
    "         '中部地方',\n",
    "         '近畿地方',\n",
    "         '中国地方',\n",
    "         '四国地方',\n",
    "         '九州・沖縄地方',\n",
    "         '首都圏',\n",
    "         '東京']\n",
    "initial_area = ['A','B','C','D','E','F','G','H','I','J']\n",
    "for area, initial in zip(areas, initial_area):\n",
    "    area_dict[area] = initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into csv of each area\n",
    "for area in areas:\n",
    "    dfm.groupby('area').get_group(area).to_csv('../data/dfs_by_area2/mar_'+area_dict[area]+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read \n",
    "zip_dir  = \"../data/dfs_by_area2\" # new_dfs2_jst\n",
    "csvfiles = glob(os.path.join(zip_dir, 'mar_*'))\n",
    "\n",
    "feb_dfs = []\n",
    "for csvfile in csvfiles:\n",
    "    df = pd.read_csv(csvfile).drop(['Unnamed: 0'], axis=1)\n",
    "    feb_dfs.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count tweet by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_hour(dfor, filetype, savedir, key_lang, area):\n",
    "    # Prepartion of dfw \n",
    "    results = []\n",
    "    dfw = pd.read_csv('../data/WordTimeSeries.csv', encoding='utf-8').rename(columns={'Unnamed: 0': 'types',\n",
    "                                                                                    'file name': 'file_name', \n",
    "                                                                                    'orignal form': 'orignal_form',\n",
    "                                                                                    'English translation': 'English_translation'})\n",
    "    booleanDictionary = {True: 'TRUE', False: 'FALSE'} # Drop nan and False in order to use query \n",
    "    dfw = dfw.replace(booleanDictionary)\n",
    "    dfw = dfw.dropna(how='all')\n",
    "\n",
    "    # Prepration of keywords\n",
    "    filenames = [filename for filename in dfw.file_name.tolist() if filetype in filename]\n",
    "    xd = {}\n",
    "    xd['T'] = 'file_name.str.contains(\"T\")'\n",
    "    xd['D'] = 'file_name.str.contains(\"D\")'\n",
    "    xd['A'] = 'file_name.str.contains(\"A\")'\n",
    "    xd['V'] = 'file_name.str.contains(\"V\")'\n",
    "    xd['F'] = 'file_name.str.contains(\"F\")'\n",
    "    xd['C'] = 'file_name.str.contains(\"C\")'\n",
    "\n",
    "    query = xd[filetype]\n",
    "    dfw_c = dfw.query(query, engine='python')\n",
    "    \n",
    "    # Keyword \n",
    "    if key_lang == 'jp':\n",
    "        keywords = dfw_c.orignal_form.tolist() # English.ver: keywords = dfw_c.English_translation.tolist() \n",
    "    elif key_lang == 'en':\n",
    "        keywords = dfw_c.English_translation.tolist() \n",
    "    else:\n",
    "        print('Put accurate keyword language')\n",
    "    \n",
    "    # Find tweets\n",
    "    # Extract tweet\n",
    "    dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "\n",
    "    for col, key in zip(filenames, keywords):\n",
    "        l = []\n",
    "        for row in dfor_c.itertuples():\n",
    "            if key in row.tweet:\n",
    "                l.append(1)\n",
    "            else:\n",
    "                l.append(0)\n",
    "        dfor_c[col] = l\n",
    "\n",
    "    # Save dir\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "\n",
    "#     for i, df in enumerate(dfs):\n",
    "    df = dfor_c\n",
    "    unique_dates = df['date'].tolist()\n",
    "    unique_dates = sorted(set(unique_dates), key=unique_dates.index) # date values\n",
    "    df_date = df.groupby('date') # df grouped by date   \n",
    "\n",
    "    for date in unique_dates:\n",
    "        df_oneday = df_date.get_group(date) # df of one day\n",
    "        dfh = df_oneday.groupby('hour').sum()\n",
    "        dfh = dfh.drop(['year', 'month', 'user_id', 'day'],axis=1)\n",
    "        \n",
    "        # Save csv\n",
    "        outname = 'area'+area+'_'+filetype+str(date)+'.csv'\n",
    "        savename = os.path.join(outdir, outname)\n",
    "\n",
    "        # Save to csv\n",
    "        dfh.to_csv(savename)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "G\n",
      "H\n",
      "I\n",
      "J\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "initial_area = ['A','B','C','D','E','F','G','H','I','J']\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for area, df in zip(initial_area, feb_dfs):\n",
    "    print(area)\n",
    "    for filetype in filetypes:\n",
    "        findtwt_hour(df, filetype=filetype, savedir='../results_location2/hour', key_lang='jp', area=area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['北海道地方']\n",
      "['東北地方']\n",
      "['関東地方']\n",
      "['中部地方']\n",
      "['近畿地方']\n",
      "['中国地方']\n",
      "['四国地方']\n",
      "['九州・沖縄地方']\n",
      "['首都圏']\n",
      "['東京']\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "for df in feb_dfs:\n",
    "    print(df.area.unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
