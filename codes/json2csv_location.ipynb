{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "from datetime import datetime\n",
    "import os\n",
    "from os import path\n",
    "from glob import glob\n",
    "\n",
    "import ijson\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_dir  = \"../data/zip2\"\n",
    "\n",
    "files = glob(os.path.join(zip_dir, '*'))\n",
    "for filename in files:\n",
    "    if '.zip' in filename:\n",
    "        pass\n",
    "    elif '.json' in filename:\n",
    "        pass\n",
    "    else:\n",
    "        newfile = filename+'.json'\n",
    "        os.rename(filename, newfile)\n",
    "\n",
    "jsons = glob(os.path.join(zip_dir, '*.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/zip2/part-v003-o001-r-00000.json',\n",
       " '../data/zip2/part-v003-o001-r-00001.json',\n",
       " '../data/zip2/part-v003-o001-r-00002.json',\n",
       " '../data/zip2/part-v003-o001-r-00003.json',\n",
       " '../data/zip2/part-v003-o001-r-00004.json',\n",
       " '../data/zip2/part-v003-o001-r-00005.json',\n",
       " '../data/zip2/part-v003-o001-r-00006.json',\n",
       " '../data/zip2/part-v003-o001-r-00007.json',\n",
       " '../data/zip2/part-v003-o001-r-00008.json',\n",
       " '../data/zip2/part-v003-o001-r-00009.json']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jsons = jsons[:2]\n",
    "jsons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading json elapsed_time:718.873034954071[sec]\n",
      "Loading json elapsed_time:648.138466835022[sec]\n",
      "Loading json elapsed_time:526.3302919864655[sec]\n",
      "Loading json elapsed_time:507.3329060077667[sec]\n",
      "Loading json elapsed_time:496.4747657775879[sec]\n",
      "Loading json elapsed_time:536.7752206325531[sec]\n",
      "Loading json elapsed_time:555.0317189693451[sec]\n",
      "Loading json elapsed_time:566.3812553882599[sec]\n",
      "Loading json elapsed_time:529.7259113788605[sec]\n",
      "Loading json elapsed_time:740.5443599224091[sec]\n"
     ]
    }
   ],
   "source": [
    "# jsons = ['../data/part_v003_o001_r_00000.json', ..., '../data/part_v003_o001_r_00001.json']\n",
    "\n",
    "dfrts = []\n",
    "dfors = []\n",
    "for i, json in enumerate(jsons):\n",
    "    # Instance Preparation\n",
    "    dates = []\n",
    "    tweets = []\n",
    "    user_ids = []\n",
    "#     rts = []\n",
    "\n",
    "#     # tweet's location\n",
    "#     place_tweet = []\n",
    "    # tweet's location\n",
    "    place_fullname_tweet = []\n",
    "    # tweet's country\n",
    "    nationality = []\n",
    "    \n",
    "    start = time.time()\n",
    "    # Load json file: date and tweet\n",
    "    with open(json, 'r', encoding='utf8') as file:\n",
    "        pet_parse = ijson.parse(file, multiple_values=True)\n",
    "        for prefix, event, value  in pet_parse:\n",
    "            # Date\n",
    "            if prefix == 'created_at':\n",
    "                dates.append(datetime.strptime(value, '%a %b %d %H:%M:%S +0000 %Y'))\n",
    "            # Tweet \n",
    "            if prefix == 'text':\n",
    "                tweets.append(value.replace('\\n', '').replace('\\t', '').replace('\\r', '').replace('\\r\\n', '').replace('　', '')) # Delte space and indet and \\r\n",
    "            # User id \n",
    "            if prefix == 'user.id':\n",
    "                user_ids.append(value)\n",
    "                \n",
    "#             # RT Flag\n",
    "#             if len(dates)-1 == len(rts) and prefix == 'retweeted_status':\n",
    "#                 rts.append(True)\n",
    "#             if len(dates)-2 == len(rts):\n",
    "#                 rts.append(False)\n",
    "            \n",
    "#             # tweet's location\n",
    "#             if prefix == 'place.name':\n",
    "#                 place_tweet.append(value)\n",
    "#             if len(dates)-2 == len(place_tweet):\n",
    "#                 place_tweet.append('nan')\n",
    "\n",
    "            # tweet's location - fullname\n",
    "            if prefix == 'place.full_name':\n",
    "                place_fullname_tweet.append(value)\n",
    "            if len(dates)-2 == len(place_fullname_tweet):\n",
    "                place_fullname_tweet.append('nan')\n",
    "                \n",
    "            # tweet's nationality\n",
    "            if prefix == 'place.country':\n",
    "                nationality.append(value)\n",
    "            if len(dates)-2 == len(nationality):\n",
    "                nationality.append('nan')\n",
    "\n",
    "        # Add missing data\n",
    "#         if len(dates) != len(rts):\n",
    "#                 rts.append(False)\n",
    "        if len(dates) != len(place_fullname_tweet):\n",
    "                place_fullname_tweet.append('nan')\n",
    "        if len(dates) != len(nationality):\n",
    "                nationality.append('nan')\n",
    "\n",
    "    print(\"Loading json elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n",
    "    \n",
    "#     print(len(dates), len(tweets), len(user_ids), len(rts), len(nationality), len(place_tweet))\n",
    "    \n",
    "    # Create dataframe\n",
    "    data = np.vstack([user_ids, dates, tweets, nationality, place_fullname_tweet]).T\n",
    "    df = pd.DataFrame(data, columns=['user_id', 'date', 'tweet', 'nationality', 'place_tweet'])\n",
    "    \n",
    "    import datetime as dt\n",
    "    \n",
    "    # Df only with location data\n",
    "    df = df[df.place_tweet != 'nan']\n",
    "    \n",
    "    # Change time zone\n",
    "    df['date'] = df['date'] + dt.timedelta(hours = 9)\n",
    "    \n",
    "    # Date\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['time'] = df['date'].dt.time\n",
    "    df['date'] = df['date'].dt.date\n",
    "    \n",
    "#     # divide df by RT_flag \n",
    "#     dfrt = df[df['RT_flag'] == True]\n",
    "#     dfor = df[df['RT_flag'] == False]\n",
    "    \n",
    "    # Save to csv file\n",
    "    df.to_csv('../data/dfs_location2/df_lct'+str(i)+'.csv')\n",
    "#     dfrt.to_csv('../data/dfs_location/dfrt_lct'+str(i)+'.csv')\n",
    "    \n",
    "    # Append to dfs\n",
    "#     dfrts.append(dfrt)\n",
    "#     dfors.append(dfor)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### ***************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|            Variable Name|    Memory|\n",
      " ------------------------------------ \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"{}{: >25}{}{: >10}{}\".format('|','Variable Name','|','Memory','|'))\n",
    "print(\" ------------------------------------ \")\n",
    "for var_name in dir():\n",
    "    if not var_name.startswith(\"_\") and sys.getsizeof(eval(var_name)) > 10000: #ここだけアレンジ\n",
    "        print(\"{}{: >25}{}{: >10}{}\".format('|',var_name,'|',sys.getsizeof(eval(var_name)),'|'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = pd.read_csv('../data/area_japan.csv')\n",
    "t = pd.read_csv('../data/dfs_location/df_lct1.csv').drop('Unnamed: 0', axis=1)\n",
    "area_jp = area.drop(['pref_en'], axis=1).set_index(['pref_jp']).Area.to_dict()\n",
    "area_en = area.drop(['pref_jp'], axis=1).set_index(['pref_en']).Area.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = []\n",
    "for natio, place in zip(t.nationality, t.place_tweet):\n",
    "    if natio == '日本' and place.split(' ')[0] in area_jp:\n",
    "        area.append(area_jp[place.split(' ')[0]])\n",
    "    elif natio == '日本' and place.split(' ')[-1] in area_jp:\n",
    "        area.append(area_jp[place.split(' ')[-1]])\n",
    "    elif natio == 'Japan' and place.split(', ')[-1] in area_en:\n",
    "        area.append(area_en[place.split(', ')[-1]])\n",
    "    elif natio == 'Japan' and place.split(', ')[0] in area_en:\n",
    "        area.append(area_en[place.split(', ')[0]])\n",
    "    else:\n",
    "        area.append('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "t['area'] = area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>nationality</th>\n",
       "      <th>place_tweet</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>time</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>823461826339872768</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>トルちゃん最終日、EXジム夏の思い出、自色で金ジム、終わりギリ大親友1人と人が来なくて困った...</td>\n",
       "      <td>日本</td>\n",
       "      <td>宮城 青葉区</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>02:14:36</td>\n",
       "      <td>東北地方</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200059773</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>まだ前半だけどシナリオ的には天司たちの物語と匹敵するほどではないって感じw pvは壮大すぎで...</td>\n",
       "      <td>中华人民共和国</td>\n",
       "      <td>广东, 中华人民共和国</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>02:08:45</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1221438530867515392</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>追加B'zhide←バンドじゃなくてもでも絵を描く時は絶対#米津玄師固定されました。最近描い...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Minamiboso-shi, Chiba</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>02:08:25</td>\n",
       "      <td>関東地方</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>140297465</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>どえらいエロニキビが治らない。困った。</td>\n",
       "      <td>日本</td>\n",
       "      <td>鹿児 鹿児島市</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>02:05:23</td>\n",
       "      <td>九州・沖縄地方</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>931291856477986816</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>おはようございます。って言うかこんばんわ。m(_ _)m昨日で熊本での仕事も終わり、さ〜呑み...</td>\n",
       "      <td>日本</td>\n",
       "      <td>熊本 宇城市</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>02:02:48</td>\n",
       "      <td>九州・沖縄地方</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4659</th>\n",
       "      <td>1102038905811955713</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>@kento47011659 原因分からないのが不安ですよね？病院で検査してくださいね。</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Numazu-shi, Shizuoka</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>05:02:55</td>\n",
       "      <td>中部地方</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4660</th>\n",
       "      <td>4799944992</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>何曲か歌った所で「名古屋の名前を出して大阪で煽ったんだ今日盛り上がらなかったら7曲で帰る❗・...</td>\n",
       "      <td>日本</td>\n",
       "      <td>岐阜 輪之内町</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>04:56:59</td>\n",
       "      <td>中部地方</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4661</th>\n",
       "      <td>392696345</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>ウォルトディズニーもスティーブ・ジョブズも好きなんやけど、2人は亡くなってる。今では偉人だの...</td>\n",
       "      <td>France</td>\n",
       "      <td>Paris, France</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>04:50:57</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4662</th>\n",
       "      <td>2220016902</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>宇野から高松に四国フェリーと同じくらいの値段と時刻で行く方法みつけた直島経由して高松に行くの...</td>\n",
       "      <td>日本</td>\n",
       "      <td>道の駅 みやま公園</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>04:49:30</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4663</th>\n",
       "      <td>1009053225503285248</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>@erinappan ピース指②本なのに.勝手に指あっち行きたいってなってるやんf(^_^)...</td>\n",
       "      <td>日本</td>\n",
       "      <td>富山 高岡市</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>04:48:26</td>\n",
       "      <td>中部地方</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4664 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id        date  \\\n",
       "0      823461826339872768  2020-02-27   \n",
       "1               200059773  2020-02-27   \n",
       "2     1221438530867515392  2020-02-27   \n",
       "3               140297465  2020-02-27   \n",
       "4      931291856477986816  2020-02-27   \n",
       "...                   ...         ...   \n",
       "4659  1102038905811955713  2020-02-24   \n",
       "4660           4799944992  2020-02-24   \n",
       "4661            392696345  2020-02-24   \n",
       "4662           2220016902  2020-02-24   \n",
       "4663  1009053225503285248  2020-02-24   \n",
       "\n",
       "                                                  tweet nationality  \\\n",
       "0     トルちゃん最終日、EXジム夏の思い出、自色で金ジム、終わりギリ大親友1人と人が来なくて困った...          日本   \n",
       "1     まだ前半だけどシナリオ的には天司たちの物語と匹敵するほどではないって感じw pvは壮大すぎで...     中华人民共和国   \n",
       "2     追加B'zhide←バンドじゃなくてもでも絵を描く時は絶対#米津玄師固定されました。最近描い...       Japan   \n",
       "3                                   どえらいエロニキビが治らない。困った。          日本   \n",
       "4     おはようございます。って言うかこんばんわ。m(_ _)m昨日で熊本での仕事も終わり、さ〜呑み...          日本   \n",
       "...                                                 ...         ...   \n",
       "4659       @kento47011659 原因分からないのが不安ですよね？病院で検査してくださいね。       Japan   \n",
       "4660  何曲か歌った所で「名古屋の名前を出して大阪で煽ったんだ今日盛り上がらなかったら7曲で帰る❗・...          日本   \n",
       "4661  ウォルトディズニーもスティーブ・ジョブズも好きなんやけど、2人は亡くなってる。今では偉人だの...      France   \n",
       "4662  宇野から高松に四国フェリーと同じくらいの値段と時刻で行く方法みつけた直島経由して高松に行くの...          日本   \n",
       "4663  @erinappan ピース指②本なのに.勝手に指あっち行きたいってなってるやんf(^_^)...          日本   \n",
       "\n",
       "                place_tweet  year  month  day  hour      time     area  \n",
       "0                    宮城 青葉区  2020      2   27     2  02:14:36     東北地方  \n",
       "1               广东, 中华人民共和国  2020      2   27     2  02:08:45      nan  \n",
       "2     Minamiboso-shi, Chiba  2020      2   27     2  02:08:25     関東地方  \n",
       "3                   鹿児 鹿児島市  2020      2   27     2  02:05:23  九州・沖縄地方  \n",
       "4                    熊本 宇城市  2020      2   27     2  02:02:48  九州・沖縄地方  \n",
       "...                     ...   ...    ...  ...   ...       ...      ...  \n",
       "4659   Numazu-shi, Shizuoka  2020      2   24     5  05:02:55     中部地方  \n",
       "4660                岐阜 輪之内町  2020      2   24     4  04:56:59     中部地方  \n",
       "4661          Paris, France  2020      2   24     4  04:50:57      nan  \n",
       "4662              道の駅 みやま公園  2020      2   24     4  04:49:30      nan  \n",
       "4663                 富山 高岡市  2020      2   24     4  04:48:26     中部地方  \n",
       "\n",
       "[4664 rows x 11 columns]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = t[t.area == 'nan']\n",
    "n2 = n[n.nationality == '日本']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>nationality</th>\n",
       "      <th>place_tweet</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>time</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>214091568</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>今日はお誕生日のこちらの方の落語会へー！！おめでとうございますー㊗️今日も笑わせてもらいまし...</td>\n",
       "      <td>日本</td>\n",
       "      <td>あべのハルカス (Abeno Harukas)</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>01:52:29</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>283672965</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>先週は、シゲ演出の『サゼン幕』を観てきました✨後味の良い舞台は大好きです‼️号泣を懸念してタ...</td>\n",
       "      <td>日本</td>\n",
       "      <td>あうるすぽっと (舞台芸術交流センター)</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>22:47:25</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>1020450848395046912</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>【新宿SAMURAIありがとう】💫2/26meiyoセットリスト💫1.シトラス2.溶ける青春...</td>\n",
       "      <td>日本</td>\n",
       "      <td>Shinjuku SAMURAI</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>22:44:14</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>1481572735</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>マジかぁやっぱそうなるよなぁめっちゃ楽しみにしてたのに...(◞‸◟ㆀ)でもメンバーの方がガ...</td>\n",
       "      <td>日本</td>\n",
       "      <td>バスタ新宿 (Shinjuku Expressway Bus Terminal)</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>22:22:50</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>184733957</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>お腹いっぱいになって落ち着いたのでデザートと紅茶を頂きながら東京ドームを眺めて想いにふける。...</td>\n",
       "      <td>日本</td>\n",
       "      <td>叙々苑</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>22:19:19</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>786091920145735680</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>旅行記録1日目。京都水族館を満喫。梅小路公園で意味もなく子どもたちと鬼ごっこして疲れる。ホテ...</td>\n",
       "      <td>日本</td>\n",
       "      <td>京都水族館 (KYOTO AQUARIUM)</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>07:20:27</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4606</th>\n",
       "      <td>78149438</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>かわいいげんき😍#いわてS1スイーツフェア https://t.co/bOOXYHfVDG</td>\n",
       "      <td>日本</td>\n",
       "      <td>ビッグルーフ滝沢</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>07:07:52</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4627</th>\n",
       "      <td>1085457911332036609</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>私は喧嘩した相手をブロックまでしなかった。たぶんこれもその人は見るだろう。今はFFではないそ...</td>\n",
       "      <td>日本</td>\n",
       "      <td>日本</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>06:34:42</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4628</th>\n",
       "      <td>1085457911332036609</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>朝はや！薬入れたからもうちょっと寝るけど、これ見るかわからんけど、Twitterっていろんな...</td>\n",
       "      <td>日本</td>\n",
       "      <td>日本</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>06:34:41</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4662</th>\n",
       "      <td>2220016902</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>宇野から高松に四国フェリーと同じくらいの値段と時刻で行く方法みつけた直島経由して高松に行くの...</td>\n",
       "      <td>日本</td>\n",
       "      <td>道の駅 みやま公園</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>04:49:30</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id        date  \\\n",
       "10              214091568  2020-02-27   \n",
       "217             283672965  2020-02-26   \n",
       "224   1020450848395046912  2020-02-26   \n",
       "264            1481572735  2020-02-26   \n",
       "268             184733957  2020-02-26   \n",
       "...                   ...         ...   \n",
       "4596   786091920145735680  2020-02-24   \n",
       "4606             78149438  2020-02-24   \n",
       "4627  1085457911332036609  2020-02-24   \n",
       "4628  1085457911332036609  2020-02-24   \n",
       "4662           2220016902  2020-02-24   \n",
       "\n",
       "                                                  tweet nationality  \\\n",
       "10    今日はお誕生日のこちらの方の落語会へー！！おめでとうございますー㊗️今日も笑わせてもらいまし...          日本   \n",
       "217   先週は、シゲ演出の『サゼン幕』を観てきました✨後味の良い舞台は大好きです‼️号泣を懸念してタ...          日本   \n",
       "224   【新宿SAMURAIありがとう】💫2/26meiyoセットリスト💫1.シトラス2.溶ける青春...          日本   \n",
       "264   マジかぁやっぱそうなるよなぁめっちゃ楽しみにしてたのに...(◞‸◟ㆀ)でもメンバーの方がガ...          日本   \n",
       "268   お腹いっぱいになって落ち着いたのでデザートと紅茶を頂きながら東京ドームを眺めて想いにふける。...          日本   \n",
       "...                                                 ...         ...   \n",
       "4596  旅行記録1日目。京都水族館を満喫。梅小路公園で意味もなく子どもたちと鬼ごっこして疲れる。ホテ...          日本   \n",
       "4606      かわいいげんき😍#いわてS1スイーツフェア https://t.co/bOOXYHfVDG          日本   \n",
       "4627  私は喧嘩した相手をブロックまでしなかった。たぶんこれもその人は見るだろう。今はFFではないそ...          日本   \n",
       "4628  朝はや！薬入れたからもうちょっと寝るけど、これ見るかわからんけど、Twitterっていろんな...          日本   \n",
       "4662  宇野から高松に四国フェリーと同じくらいの値段と時刻で行く方法みつけた直島経由して高松に行くの...          日本   \n",
       "\n",
       "                                   place_tweet  year  month  day  hour  \\\n",
       "10                     あべのハルカス (Abeno Harukas)  2020      2   27     1   \n",
       "217                       あうるすぽっと (舞台芸術交流センター)  2020      2   26    22   \n",
       "224                           Shinjuku SAMURAI  2020      2   26    22   \n",
       "264   バスタ新宿 (Shinjuku Expressway Bus Terminal)  2020      2   26    22   \n",
       "268                                        叙々苑  2020      2   26    22   \n",
       "...                                        ...   ...    ...  ...   ...   \n",
       "4596                    京都水族館 (KYOTO AQUARIUM)  2020      2   24     7   \n",
       "4606                                  ビッグルーフ滝沢  2020      2   24     7   \n",
       "4627                                        日本  2020      2   24     6   \n",
       "4628                                        日本  2020      2   24     6   \n",
       "4662                                 道の駅 みやま公園  2020      2   24     4   \n",
       "\n",
       "          time area  \n",
       "10    01:52:29  nan  \n",
       "217   22:47:25  nan  \n",
       "224   22:44:14  nan  \n",
       "264   22:22:50  nan  \n",
       "268   22:19:19  nan  \n",
       "...        ...  ...  \n",
       "4596  07:20:27  nan  \n",
       "4606  07:07:52  nan  \n",
       "4627  06:34:42  nan  \n",
       "4628  06:34:41  nan  \n",
       "4662  04:49:30  nan  \n",
       "\n",
       "[248 rows x 11 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= t.place_tweet[0].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>nationality</th>\n",
       "      <th>place_tweet</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>time</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1221438530867515392</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>追加B'zhide←バンドじゃなくてもでも絵を描く時は絶対#米津玄師固定されました。最近描い...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Minamiboso-shi, Chiba</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>02:08:25</td>\n",
       "      <td>関東地方</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1217417832020922368</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>そんな中で、私が学んだ空手は他の空手流派には無い「サバキ」という技術がありました。これは打撃...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Itabashi-ku, Tokyo</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>01:41:40</td>\n",
       "      <td>関東地方</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1221438530867515392</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>@95nEcyc70XvOl3i ビデオ、写真、山程撮ってます☆そして既に懐かしい😁💦嫌々期...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Minamiboso-shi, Chiba</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>01:34:32</td>\n",
       "      <td>関東地方</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1217417832020922368</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>皆さんこれ、どう思います？ケンカって、顔面パンチするでしょ？掴むでしょ？ケンカに反則も何もな...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Itabashi-ku, Tokyo</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>01:34:19</td>\n",
       "      <td>関東地方</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1217417832020922368</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>唐突ですが、ケンカの話をします。空手を学ぶと、ケンカに弱くなります。黒帯まで飛び抜けてしまえ...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Itabashi-ku, Tokyo</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>01:19:58</td>\n",
       "      <td>関東地方</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4626</th>\n",
       "      <td>999942880486608897</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>今日のパピコ高熱を出したので病院に連れていく。待合室でしんどそうだったので肩にもたれさせてや...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Sapporo-shi Kita-ku, Hokkaido</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>06:39:40</td>\n",
       "      <td>北海道地方</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4631</th>\n",
       "      <td>984416076220919808</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>@7eGKVE51tuxuPyg 糸井の守備は不安だらけやから高山使うべきだと思います</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Ikaruga-cho, Nara</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>06:29:38</td>\n",
       "      <td>近畿地方</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4649</th>\n",
       "      <td>1221774819248594945</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>@Sugi103258 @gorillaman017 お金がないので困りました</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Takamatsu-shi, Kagawa</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>05:34:11</td>\n",
       "      <td>四国地方</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4653</th>\n",
       "      <td>1227625153397309442</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>独居生活の独居老人の孤独さは悲惨過ぎるものだとつくづく思った。家族が一人も居ない独居老人はこ...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Higashisonogi-cho, Nagasaki</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>05:13:47</td>\n",
       "      <td>九州・沖縄地方</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4659</th>\n",
       "      <td>1102038905811955713</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>@kento47011659 原因分からないのが不安ですよね？病院で検査してくださいね。</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Numazu-shi, Shizuoka</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>05:02:55</td>\n",
       "      <td>中部地方</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>558 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id        date  \\\n",
       "2     1221438530867515392  2020-02-27   \n",
       "16    1217417832020922368  2020-02-27   \n",
       "19    1221438530867515392  2020-02-27   \n",
       "20    1217417832020922368  2020-02-27   \n",
       "27    1217417832020922368  2020-02-27   \n",
       "...                   ...         ...   \n",
       "4626   999942880486608897  2020-02-24   \n",
       "4631   984416076220919808  2020-02-24   \n",
       "4649  1221774819248594945  2020-02-24   \n",
       "4653  1227625153397309442  2020-02-24   \n",
       "4659  1102038905811955713  2020-02-24   \n",
       "\n",
       "                                                  tweet nationality  \\\n",
       "2     追加B'zhide←バンドじゃなくてもでも絵を描く時は絶対#米津玄師固定されました。最近描い...       Japan   \n",
       "16    そんな中で、私が学んだ空手は他の空手流派には無い「サバキ」という技術がありました。これは打撃...       Japan   \n",
       "19    @95nEcyc70XvOl3i ビデオ、写真、山程撮ってます☆そして既に懐かしい😁💦嫌々期...       Japan   \n",
       "20    皆さんこれ、どう思います？ケンカって、顔面パンチするでしょ？掴むでしょ？ケンカに反則も何もな...       Japan   \n",
       "27    唐突ですが、ケンカの話をします。空手を学ぶと、ケンカに弱くなります。黒帯まで飛び抜けてしまえ...       Japan   \n",
       "...                                                 ...         ...   \n",
       "4626  今日のパピコ高熱を出したので病院に連れていく。待合室でしんどそうだったので肩にもたれさせてや...       Japan   \n",
       "4631        @7eGKVE51tuxuPyg 糸井の守備は不安だらけやから高山使うべきだと思います       Japan   \n",
       "4649            @Sugi103258 @gorillaman017 お金がないので困りました       Japan   \n",
       "4653  独居生活の独居老人の孤独さは悲惨過ぎるものだとつくづく思った。家族が一人も居ない独居老人はこ...       Japan   \n",
       "4659       @kento47011659 原因分からないのが不安ですよね？病院で検査してくださいね。       Japan   \n",
       "\n",
       "                        place_tweet  year  month  day  hour      time     area  \n",
       "2             Minamiboso-shi, Chiba  2020      2   27     2  02:08:25     関東地方  \n",
       "16               Itabashi-ku, Tokyo  2020      2   27     1  01:41:40     関東地方  \n",
       "19            Minamiboso-shi, Chiba  2020      2   27     1  01:34:32     関東地方  \n",
       "20               Itabashi-ku, Tokyo  2020      2   27     1  01:34:19     関東地方  \n",
       "27               Itabashi-ku, Tokyo  2020      2   27     1  01:19:58     関東地方  \n",
       "...                             ...   ...    ...  ...   ...       ...      ...  \n",
       "4626  Sapporo-shi Kita-ku, Hokkaido  2020      2   24     6  06:39:40    北海道地方  \n",
       "4631              Ikaruga-cho, Nara  2020      2   24     6  06:29:38     近畿地方  \n",
       "4649          Takamatsu-shi, Kagawa  2020      2   24     5  05:34:11     四国地方  \n",
       "4653    Higashisonogi-cho, Nagasaki  2020      2   24     5  05:13:47  九州・沖縄地方  \n",
       "4659           Numazu-shi, Shizuoka  2020      2   24     5  05:02:55     中部地方  \n",
       "\n",
       "[558 rows x 11 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[t.nationality == 'Japan']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### ***************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ORs\n",
    "zip_dir  = \"../data/dfs\" # new_dfs2_jst\n",
    "csvfiles = glob(os.path.join(zip_dir, 'dfor*'))\n",
    "\n",
    "dfors = []\n",
    "for csvfile in csvfiles:\n",
    "    df = pd.read_csv(csvfile).drop('Unnamed: 0', axis=1)\n",
    "    dfors.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Rts\n",
    "zip_dir  = \"../data/new_dfs2_jst\"\n",
    "csvfiles = glob(os.path.join(zip_dir, 'dfor*'))\n",
    "\n",
    "dfors = []\n",
    "for csvfile in csvfiles:\n",
    "    df1 = pd.read_csv(csvfile).drop('Unnamed: 0', axis=1)\n",
    "    dfors.append(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n"
     ]
    }
   ],
   "source": [
    "for df in dfors:\n",
    "    print(df.RT_flag.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n"
     ]
    }
   ],
   "source": [
    "for df in dfrts:\n",
    "    print(df.RT_flag.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of tweets included RT: 9286476\n"
     ]
    }
   ],
   "source": [
    "c = [len(df) for df in dfors]\n",
    "print('The amount of tweets included RT:', sum(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of tweets included RT: 6692071\n"
     ]
    }
   ],
   "source": [
    "c = [len(df) for df in dfrts]\n",
    "print('The amount of tweets included RT:', sum(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create csv by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_day(dfors, filetype, savedir, key_lang):\n",
    "    # Prepartion of dfw \n",
    "    results = []\n",
    "    dfs = []\n",
    "    dfw = pd.read_csv('../data/WordTimeSeries.csv', encoding='utf-8').rename(columns={'Unnamed: 0': 'types',\n",
    "                                                                                    'file name': 'file_name', \n",
    "                                                                                    'orignal form': 'orignal_form',\n",
    "                                                                                    'English translation': 'English_translation'})\n",
    "    booleanDictionary = {True: 'TRUE', False: 'FALSE'} # Drop nan and False in order to use query \n",
    "    dfw = dfw.replace(booleanDictionary)\n",
    "    dfw = dfw.dropna(how='all')\n",
    "    \n",
    "    # Prepration of keywords\n",
    "    filenames = [filename for filename in dfw.file_name.tolist() if filetype in filename]\n",
    "    xd = {}\n",
    "    xd['T'] = 'file_name.str.contains(\"T\")'\n",
    "    xd['D'] = 'file_name.str.contains(\"D\")'\n",
    "    xd['A'] = 'file_name.str.contains(\"A\")'\n",
    "    xd['V'] = 'file_name.str.contains(\"V\")'\n",
    "    xd['F'] = 'file_name.str.contains(\"F\")'\n",
    "    xd['C'] = 'file_name.str.contains(\"C\")'\n",
    "    \n",
    "    query = xd[filetype]\n",
    "    dfw_c = dfw.query(query, engine='python')\n",
    "    if key_lang == 'jp':\n",
    "        keywords = dfw_c.orignal_form.tolist() # English.ver: keywords = dfw_c.English_translation.tolist() \n",
    "    elif key_lang == 'en':\n",
    "        keywords = dfw_c.English_translation.tolist() \n",
    "    else:\n",
    "        print('Put accurate keyword language')\n",
    "            \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "         # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "        \n",
    "        # Add columns of keywords whose cell have 1 if this tweet includes a keyword \n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        \n",
    "        # Make rows\n",
    "        uni_dates = dfor_c['date'].tolist()\n",
    "        uni_dates = sorted(set(uni_dates), key=uni_dates.index) # date values\n",
    "        rows = []\n",
    "        for date in uni_dates:\n",
    "            d = {}\n",
    "            d['date'] = str(date)\n",
    "            for col in filenames:\n",
    "                _df = dfor_c.groupby('date').get_group(date)\n",
    "                d[col] = _df[col].sum()\n",
    "            rows.append(d)\n",
    "        \n",
    "        # Make cols\n",
    "        cols = filenames.copy()\n",
    "        cols.insert(0, 'date')\n",
    "        \n",
    "        # Make dfs with rows and cols\n",
    "        dft = pd.DataFrame(columns=cols)\n",
    "        for row in rows:\n",
    "            dft = dft.append(row, ignore_index=True) \n",
    "        dfs.append(dft)\n",
    "        \n",
    "    # Finally Connect dfs\n",
    "    for i, df in enumerate(dfs):\n",
    "        if i == 0:\n",
    "            dfnew = df\n",
    "        else:\n",
    "            dfnew = pd.concat([dfnew, df], axis=0)\n",
    "            \n",
    "    # Groupby and sort by date\n",
    "    dfnew = dfnew.groupby('date').sum()\n",
    "    \n",
    "    # Save\n",
    "    outname = filetype+'_original.csv'\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "    savename = os.path.join(outdir, outname)\n",
    "    dfnew.to_csv(savename)\n",
    "\n",
    "    return dfnew\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [01:38<00:00,  9.81s/it]\n",
      "100%|██████████| 10/10 [02:01<00:00, 12.16s/it]\n",
      "100%|██████████| 10/10 [02:09<00:00, 12.97s/it]\n",
      "100%|██████████| 10/10 [00:37<00:00,  3.78s/it]\n",
      "100%|██████████| 10/10 [00:54<00:00,  5.48s/it]\n",
      "100%|██████████| 10/10 [01:29<00:00,  8.92s/it]\n"
     ]
    }
   ],
   "source": [
    "# OR\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfors, filetype=filetype, savedir='../new_results_jst/orjp/day', key_lang='jp') # new_results_jst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:21<00:00,  2.10s/it]\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.27s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.03s/it]\n",
      "100%|██████████| 10/10 [00:09<00:00,  1.07it/s]\n",
      "100%|██████████| 10/10 [00:09<00:00,  1.05it/s]\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "# OR\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfors, filetype=filetype, savedir='../new_results_jst/oren/day', key_lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [03:25<00:00, 20.53s/it]\n",
      "100%|██████████| 10/10 [03:34<00:00, 21.48s/it]\n",
      "100%|██████████| 10/10 [03:20<00:00, 20.05s/it]\n",
      "100%|██████████| 10/10 [01:41<00:00, 10.17s/it]\n",
      "100%|██████████| 10/10 [01:28<00:00,  8.88s/it]\n",
      "100%|██████████| 10/10 [03:53<00:00, 23.33s/it]\n"
     ]
    }
   ],
   "source": [
    "# RT\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfrts, filetype=filetype, savedir='../new_results_jst/rtjp/day', key_lang='jp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:19<00:00,  1.91s/it]\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.51s/it]\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.33s/it]\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.18s/it]\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.52s/it]\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.74s/it]\n"
     ]
    }
   ],
   "source": [
    "# RT\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfrts, filetype=filetype, savedir='../new_results_jst/rten/day', key_lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create csv by hour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_hour(dfors, filetype, savedir, key_lang):\n",
    "    # Prepartion of dfw \n",
    "    results = []\n",
    "    dfs = []\n",
    "    dfw = pd.read_csv('../data/WordTimeSeries.csv', encoding='utf-8').rename(columns={'Unnamed: 0': 'types',\n",
    "                                                                                    'file name': 'file_name', \n",
    "                                                                                    'orignal form': 'orignal_form',\n",
    "                                                                                    'English translation': 'English_translation'})\n",
    "    booleanDictionary = {True: 'TRUE', False: 'FALSE'} # Drop nan and False in order to use query \n",
    "    dfw = dfw.replace(booleanDictionary)\n",
    "    dfw = dfw.dropna(how='all')\n",
    "\n",
    "    # Prepration of keywords\n",
    "    filenames = [filename for filename in dfw.file_name.tolist() if filetype in filename]\n",
    "    xd = {}\n",
    "    xd['T'] = 'file_name.str.contains(\"T\")'\n",
    "    xd['D'] = 'file_name.str.contains(\"D\")'\n",
    "    xd['A'] = 'file_name.str.contains(\"A\")'\n",
    "    xd['V'] = 'file_name.str.contains(\"V\")'\n",
    "    xd['F'] = 'file_name.str.contains(\"F\")'\n",
    "    xd['C'] = 'file_name.str.contains(\"C\")'\n",
    "\n",
    "    query = xd[filetype]\n",
    "    dfw_c = dfw.query(query, engine='python')\n",
    "    \n",
    "    # Keyword \n",
    "    if key_lang == 'jp':\n",
    "        keywords = dfw_c.orignal_form.tolist() # English.ver: keywords = dfw_c.English_translation.tolist() \n",
    "    elif key_lang == 'en':\n",
    "        keywords = dfw_c.English_translation.tolist() \n",
    "    else:\n",
    "        print('Put accurate keyword language')\n",
    "    \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "\n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        dfs.append(dfor_c)\n",
    "        \n",
    "    # Create csv file by day\n",
    "    all_df = []\n",
    "    nextdf, nextdf_date = '', ''\n",
    "    \n",
    "    # Save dir\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        unique_dates = df['date'].tolist()\n",
    "        unique_dates = sorted(set(unique_dates), key=unique_dates.index) # date values\n",
    "        df_date = df.groupby('date') # df grouped by date   \n",
    "\n",
    "        for date in unique_dates:\n",
    "            df_oneday = df_date.get_group(date) # df of one day\n",
    "            dfh = df_oneday.groupby('hour').sum()\n",
    "            dfh = dfh.drop(['year', 'month', 'user_id', 'RT_flag', 'day'],axis=1)\n",
    "            \n",
    "            # Save csv\n",
    "            outname = filetype+str(date)+'.csv'\n",
    "            savename = os.path.join(outdir, outname)\n",
    "            \n",
    "            if os.path.exists(savename):\n",
    "                # File1\n",
    "                existedfile = pd.read_csv(savename)\n",
    "                # いったんcsvにする\n",
    "                _saveonce = 'once.csv'\n",
    "                dfh.to_csv(_saveonce)\n",
    "                # File2 \n",
    "                once = pd.read_csv(_saveonce)\n",
    "                # Concat \n",
    "                df_oneday1 = pd.concat([existedfile, once], axis=0)\n",
    "                # Finally save\n",
    "                dfh = df_oneday1.groupby('hour').sum()\n",
    "                \n",
    "            # Save to csv\n",
    "            dfh.to_csv(savename)\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Orjp, Oren, Rtjp, Rten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [01:24<00:00,  8.47s/it]\n",
      "100%|██████████| 10/10 [02:05<00:00, 12.57s/it]\n",
      "100%|██████████| 10/10 [02:12<00:00, 13.22s/it]\n",
      "100%|██████████| 10/10 [00:43<00:00,  4.33s/it]\n",
      "100%|██████████| 10/10 [00:54<00:00,  5.50s/it]\n",
      "100%|██████████| 10/10 [01:31<00:00,  9.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# Orjp\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfors, filetype=filetype, savedir='../new_results_jst2/orjp/hour', key_lang='jp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.57s/it]\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.26s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.06s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.01s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.02s/it]\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "# Oren\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfors, filetype=filetype, savedir='../new_results_jst2/oren/hour', key_lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [01:25<00:00,  8.57s/it]\n",
      "100%|██████████| 10/10 [01:31<00:00,  9.10s/it]\n",
      "100%|██████████| 10/10 [01:42<00:00, 10.22s/it]\n",
      "100%|██████████| 10/10 [00:38<00:00,  3.90s/it]\n",
      "100%|██████████| 10/10 [01:05<00:00,  6.53s/it]\n",
      "100%|██████████| 10/10 [01:56<00:00, 11.65s/it]\n"
     ]
    }
   ],
   "source": [
    "# Rtjp\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfrts, filetype=filetype, savedir='../new_results_jst2/rtjp/hour', key_lang='jp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.37s/it]\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.46s/it]\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.55s/it]\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.20s/it]\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.23s/it]\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "# Rten\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfrts, filetype=filetype, savedir='../new_results_jst2/rten/hour', key_lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7827"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft = pd.read_csv('../results/rtjp/hour/T2020-02-28.csv')\n",
    "dft.T1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tT1\n",
    "21\t2958\n",
    "22\t2637\n",
    "23\t2546\n",
    "24\t4883\n",
    "25\t3352\n",
    "26\t2553\n",
    "27\t3206\n",
    "28\t5117\n",
    "29\t7827\n",
    "30\t2925"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コロナ含有率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "963821 tweets conatin \"コロナ\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_corona(dfors, filetype=\"T\"):\n",
    "    # Prepration of keywords\n",
    "    ## コロナ専用\n",
    "    keywords = ['コロナ']\n",
    "    \n",
    "    c = [] # count volume\n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "        \n",
    "        c.append(len(dfor_c))\n",
    "    \n",
    "    print(sum(c), 'tweets conatin \"コロナ\"')\n",
    "\n",
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "findtwt_corona(dfrts, filetype=\"T\")\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corona by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwtCrn_day(dfors, savename, savedir):\n",
    "    \n",
    "    dfs = []\n",
    "\n",
    "    # Keyowrds\n",
    "    keywords = ['コロナ', 'Corona', 'corona']\n",
    "    filenames = ['コロナ', 'Corona', 'corona']\n",
    "\n",
    "    \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet include keywords\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "        \n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        \n",
    "        # Make rows\n",
    "        uni_dates = dfor_c['date'].tolist()\n",
    "        uni_dates = sorted(set(uni_dates), key=uni_dates.index) # date values\n",
    "        rows = []\n",
    "        for date in uni_dates:\n",
    "            d = {}\n",
    "            d['date'] = str(date)\n",
    "            for col in filenames:\n",
    "                _df = dfor_c.groupby('date').get_group(date)\n",
    "                d[col] = _df[col].sum()\n",
    "            rows.append(d)\n",
    "        \n",
    "        # Make cols\n",
    "        cols = filenames.copy()\n",
    "        cols.insert(0, 'date')\n",
    "        \n",
    "        # Make dfs with rows and cols\n",
    "        dft = pd.DataFrame(columns=cols)\n",
    "        for row in rows:\n",
    "            dft = dft.append(row, ignore_index=True)\n",
    "        \n",
    "        dfs.append(dft)\n",
    "\n",
    "    # Finally Connect dfs\n",
    "    for i, df in enumerate(dfs):\n",
    "        if i == 0:\n",
    "            dfnew = df\n",
    "        else:\n",
    "            dfnew = pd.concat([dfnew, df], axis=0)\n",
    "            \n",
    "    # Groupby and sort by date\n",
    "    dfnew = dfnew.groupby('date').sum()\n",
    "    \n",
    "    # Save\n",
    "    outname = savename+'.csv'\n",
    "    outdir = savedir\n",
    "\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "\n",
    "    savename = os.path.join(outdir, outname)    \n",
    "    dfnew.to_csv(savename)\n",
    "\n",
    "    return dfnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "dfnew = findtwtCrn_day(dfors, savename='corona', savedir='../new_results_jst/Corona/or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "dfnew = findtwtCrn_day(dfrts, savename='corona', savedir='../new_results_jst2/Corona/rt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corona by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findtwtCrn_hour(dfors, savename, savedir):\n",
    "    # Prepartion \n",
    "    results = []\n",
    "    dfs = []\n",
    "    \n",
    "    # Save dir\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "    \n",
    "    # Keyowrds\n",
    "    keywords = ['コロナ', 'Corona', 'corona']\n",
    "    filenames = ['コロナ', 'Corona', 'corona']\n",
    "    \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "\n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        \n",
    "        dfs.append(dfor_c)\n",
    "        \n",
    "    # Create csv file by day\n",
    "    all_df = []\n",
    "    nextdf, nextdf_date = '', ''\n",
    "    \n",
    "    # Save dir\n",
    "    outdir = savedir\n",
    "    filetype = savename\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        unique_dates = df['date'].tolist()\n",
    "        unique_dates = sorted(set(unique_dates), key=unique_dates.index) # date values\n",
    "        df_date = df.groupby('date') # df grouped by date   \n",
    "        \n",
    "        for date in unique_dates:\n",
    "            df_oneday = df_date.get_group(date) # df of one day\n",
    "            dfh = df_oneday.groupby('hour').sum()\n",
    "            dfh = dfh.drop(['year', 'month', 'user_id', 'RT_flag', 'day'],axis=1)\n",
    "            \n",
    "            outname = filetype+str(date)+'.csv'\n",
    "            savename = os.path.join(outdir, outname)\n",
    "            \n",
    "            if os.path.exists(savename):\n",
    "                # File1\n",
    "                existedfile = pd.read_csv(savename)\n",
    "                \n",
    "                # いったんcsvにする\n",
    "                _saveonce = 'once.csv'\n",
    "                dfh.to_csv(_saveonce)\n",
    "                # File2 \n",
    "                once = pd.read_csv(_saveonce)\n",
    "                \n",
    "                # Concat \n",
    "                df_oneday1 = pd.concat([existedfile, once], axis=0)\n",
    "\n",
    "                # Finally save\n",
    "                dfh = df_oneday1.groupby('hour').sum()\n",
    "                \n",
    "            # Save to csv\n",
    "            dfh.to_csv(savename)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:09<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "findtwtCrn_hour(dfors, savename='corona', savedir='../new_results_jst2/Corona/or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "findtwtCrn_hour(dfrts, savename='corona', savedir='../new_results_jst/Corona/rt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_csv('../new_results_jst/rten/hour/V2020-02-01.csv')\n",
    "t2 = pd.read_csv('../new_results/rten/hour/V2020-01-31.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "print(t1.loc[0:8,'V4'].sum(), t2.V4.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>RT_flag</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1111855071380168705</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @k7LssPYI5D85fxI: https://t.co/5KgwBN00NS志村...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127471475</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @84MadokaMary: この国には知らんだけでめちゃめちゃな量の社会保障があるの...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138821520</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @zibumitunari: 明日はエイプリルフールだが、コロナ関連の嘘は絶対にやめよ...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>962192359059472384</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @akagiichirou: マスクの７割が中国からの輸入に頼ってきたが流通網が混乱す...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128360620</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @bbcnewsjapan: BBCニュース - ハンガリー政府、新型ウイルス対策で強...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user_id        date  \\\n",
       "0  1111855071380168705  2020-03-31   \n",
       "1            127471475  2020-03-31   \n",
       "2            138821520  2020-03-31   \n",
       "3   962192359059472384  2020-03-31   \n",
       "4            128360620  2020-03-31   \n",
       "\n",
       "                                               tweet  RT_flag  year  month  \\\n",
       "0  RT @k7LssPYI5D85fxI: https://t.co/5KgwBN00NS志村...     True  2020      3   \n",
       "1  RT @84MadokaMary: この国には知らんだけでめちゃめちゃな量の社会保障があるの...     True  2020      3   \n",
       "2  RT @zibumitunari: 明日はエイプリルフールだが、コロナ関連の嘘は絶対にやめよ...     True  2020      3   \n",
       "3  RT @akagiichirou: マスクの７割が中国からの輸入に頼ってきたが流通網が混乱す...     True  2020      3   \n",
       "4  RT @bbcnewsjapan: BBCニュース - ハンガリー政府、新型ウイルス対策で強...     True  2020      3   \n",
       "\n",
       "   day  hour      time  \n",
       "0   31    23  23:59:58  \n",
       "1   31    23  23:59:58  \n",
       "2   31    23  23:59:58  \n",
       "3   31    23  23:59:58  \n",
       "4   31    23  23:59:57  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfrts[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
