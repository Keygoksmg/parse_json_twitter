{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "from datetime import datetime\n",
    "import os\n",
    "from os import path\n",
    "from glob import glob\n",
    "\n",
    "import ijson\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "zip_dir  = \"../data/zip2\"\n",
    "\n",
    "files = glob(os.path.join(zip_dir, '*'))\n",
    "for filename in files:\n",
    "    if '.zip' in filename:\n",
    "        pass\n",
    "    elif '.json' in filename:\n",
    "        pass\n",
    "    else:\n",
    "        newfile = filename+'.json'\n",
    "        os.rename(filename, newfile)\n",
    "\n",
    "jsons = glob(os.path.join(zip_dir, '*.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/zip2/part-v003-o001-r-00000.json',\n",
       " '../data/zip2/part-v003-o001-r-00001.json',\n",
       " '../data/zip2/part-v003-o001-r-00002.json',\n",
       " '../data/zip2/part-v003-o001-r-00003.json',\n",
       " '../data/zip2/part-v003-o001-r-00004.json',\n",
       " '../data/zip2/part-v003-o001-r-00005.json',\n",
       " '../data/zip2/part-v003-o001-r-00006.json',\n",
       " '../data/zip2/part-v003-o001-r-00007.json',\n",
       " '../data/zip2/part-v003-o001-r-00008.json',\n",
       " '../data/zip2/part-v003-o001-r-00009.json']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jsons = jsons[:2]\n",
    "jsons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading json elapsed_time:718.873034954071[sec]\n",
      "Loading json elapsed_time:648.138466835022[sec]\n",
      "Loading json elapsed_time:526.3302919864655[sec]\n",
      "Loading json elapsed_time:507.3329060077667[sec]\n",
      "Loading json elapsed_time:496.4747657775879[sec]\n",
      "Loading json elapsed_time:536.7752206325531[sec]\n",
      "Loading json elapsed_time:555.0317189693451[sec]\n",
      "Loading json elapsed_time:566.3812553882599[sec]\n",
      "Loading json elapsed_time:529.7259113788605[sec]\n",
      "Loading json elapsed_time:740.5443599224091[sec]\n"
     ]
    }
   ],
   "source": [
    "# jsons = ['../data/part_v003_o001_r_00000.json', ..., '../data/part_v003_o001_r_00001.json']\n",
    "\n",
    "dfrts = []\n",
    "dfors = []\n",
    "for i, json in enumerate(jsons):\n",
    "    # Instance Preparation\n",
    "    dates = []\n",
    "    tweets = []\n",
    "    user_ids = []\n",
    "#     rts = []\n",
    "\n",
    "#     # tweet's location\n",
    "#     place_tweet = []\n",
    "    # tweet's location\n",
    "    place_fullname_tweet = []\n",
    "    # tweet's country\n",
    "    nationality = []\n",
    "    \n",
    "    start = time.time()\n",
    "    # Load json file: date and tweet\n",
    "    with open(json, 'r', encoding='utf8') as file:\n",
    "        pet_parse = ijson.parse(file, multiple_values=True)\n",
    "        for prefix, event, value  in pet_parse:\n",
    "            # Date\n",
    "            if prefix == 'created_at':\n",
    "                dates.append(datetime.strptime(value, '%a %b %d %H:%M:%S +0000 %Y'))\n",
    "            # Tweet \n",
    "            if prefix == 'text':\n",
    "                tweets.append(value.replace('\\n', '').replace('\\t', '').replace('\\r', '').replace('\\r\\n', '').replace('ã€€', '')) # Delte space and indet and \\r\n",
    "            # User id \n",
    "            if prefix == 'user.id':\n",
    "                user_ids.append(value)\n",
    "                \n",
    "#             # RT Flag\n",
    "#             if len(dates)-1 == len(rts) and prefix == 'retweeted_status':\n",
    "#                 rts.append(True)\n",
    "#             if len(dates)-2 == len(rts):\n",
    "#                 rts.append(False)\n",
    "            \n",
    "#             # tweet's location\n",
    "#             if prefix == 'place.name':\n",
    "#                 place_tweet.append(value)\n",
    "#             if len(dates)-2 == len(place_tweet):\n",
    "#                 place_tweet.append('nan')\n",
    "\n",
    "            # tweet's location - fullname\n",
    "            if prefix == 'place.full_name':\n",
    "                place_fullname_tweet.append(value)\n",
    "            if len(dates)-2 == len(place_fullname_tweet):\n",
    "                place_fullname_tweet.append('nan')\n",
    "                \n",
    "            # tweet's nationality\n",
    "            if prefix == 'place.country':\n",
    "                nationality.append(value)\n",
    "            if len(dates)-2 == len(nationality):\n",
    "                nationality.append('nan')\n",
    "\n",
    "        # Add missing data\n",
    "#         if len(dates) != len(rts):\n",
    "#                 rts.append(False)\n",
    "        if len(dates) != len(place_fullname_tweet):\n",
    "                place_fullname_tweet.append('nan')\n",
    "        if len(dates) != len(nationality):\n",
    "                nationality.append('nan')\n",
    "\n",
    "    print(\"Loading json elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n",
    "    \n",
    "#     print(len(dates), len(tweets), len(user_ids), len(rts), len(nationality), len(place_tweet))\n",
    "    \n",
    "    # Create dataframe\n",
    "    data = np.vstack([user_ids, dates, tweets, nationality, place_fullname_tweet]).T\n",
    "    df = pd.DataFrame(data, columns=['user_id', 'date', 'tweet', 'nationality', 'place_tweet'])\n",
    "    \n",
    "    import datetime as dt\n",
    "    \n",
    "    # Df only with location data\n",
    "    df = df[df.place_tweet != 'nan']\n",
    "    \n",
    "    # Change time zone\n",
    "    df['date'] = df['date'] + dt.timedelta(hours = 9)\n",
    "    \n",
    "    # Date\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['time'] = df['date'].dt.time\n",
    "    df['date'] = df['date'].dt.date\n",
    "    \n",
    "#     # divide df by RT_flag \n",
    "#     dfrt = df[df['RT_flag'] == True]\n",
    "#     dfor = df[df['RT_flag'] == False]\n",
    "    \n",
    "    # Save to csv file\n",
    "    df.to_csv('../data/dfs_location2/df_lct'+str(i)+'.csv')\n",
    "#     dfrt.to_csv('../data/dfs_location/dfrt_lct'+str(i)+'.csv')\n",
    "    \n",
    "    # Append to dfs\n",
    "#     dfrts.append(dfrt)\n",
    "#     dfors.append(dfor)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### ***************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|            Variable Name|    Memory|\n",
      " ------------------------------------ \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"{}{: >25}{}{: >10}{}\".format('|','Variable Name','|','Memory','|'))\n",
    "print(\" ------------------------------------ \")\n",
    "for var_name in dir():\n",
    "    if not var_name.startswith(\"_\") and sys.getsizeof(eval(var_name)) > 10000: #ã“ã“ã ã‘ã‚¢ãƒ¬ãƒ³ã‚¸\n",
    "        print(\"{}{: >25}{}{: >10}{}\".format('|',var_name,'|',sys.getsizeof(eval(var_name)),'|'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv of dictionary\n",
    "def crate_area_dictionary(file):\n",
    "    area = pd.read_csv(file)\n",
    "    area_jp = area.drop(['pref_en'], axis=1).set_index(['pref_jp']).Area.to_dict()\n",
    "    area_en = area.drop(['pref_jp'], axis=1).set_index(['pref_en']).Area.to_dict()\n",
    "    return area_jp, area_en\n",
    "\n",
    "area_jp, area_en = crate_area_dictionary('../data/area.csv')\n",
    "area_shutoken_jp, area_shutoken_en = crate_area_dictionary('../data/area_shutoken.csv')\n",
    "area_tokyo_jp, area_tokyo_en = crate_area_dictionary('../data/area_tokyo.csv')\n",
    "\n",
    "\n",
    "def df_by_area(df, d_area_jp, d_area_en):\n",
    "    l_area = []\n",
    "    for natio, place in zip(df.nationality, df.place_tweet):\n",
    "        if natio == 'æ—¥æœ¬' and place.split(' ')[0] in d_area_jp:\n",
    "            l_area.append(d_area_jp[place.split(' ')[0]])\n",
    "        elif natio == 'æ—¥æœ¬' and place.split(' ')[-1] in d_area_jp:\n",
    "            l_area.append(d_area_jp[place.split(' ')[-1]])\n",
    "        elif l_area == 'Japan' and place.split(', ')[-1] in d_area_en:\n",
    "            l_area.append(d_area_en[place.split(', ')[-1]])\n",
    "        elif natio == 'Japan' and place.split(', ')[0] in d_area_en:\n",
    "            l_area.append(d_area_en[place.split(', ')[0]])\n",
    "        else:\n",
    "            l_area.append('nan')\n",
    "    df['area'] = l_area\n",
    "    df_lct = df[df.area != 'nan']\n",
    "    return df_lct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ORs\n",
    "zip_dir  = \"../data/dfs_location2\" # new_dfs2_jst\n",
    "csvfiles = glob(os.path.join(zip_dir, 'df*'))\n",
    "\n",
    "dfs = []\n",
    "for csvfile in csvfiles:\n",
    "    df = pd.read_csv(csvfile).drop('Unnamed: 0', axis=1)\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lcts = [df_by_area(df, area_jp, area_en)  for df in dfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze data monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat dfs\n",
    "for i, df in enumerate(df_lcts):\n",
    "    if i == 0:\n",
    "        dfm = df\n",
    "    else:\n",
    "        dfm = pd.concat([dfm, df], axis=0)\n",
    "_df = dfm.copy()\n",
    "df_shutoken = df_by_area(_df, area_shutoken_jp, area_shutoken_en)\n",
    "dfm = pd.concat([dfm, df_shutoken], axis=0)\n",
    "df_tokyo = df_by_area(_df, area_tokyo_jp, area_tokyo_en)\n",
    "dfm = pd.concat([dfm, df_tokyo], axis=0)\n",
    "# Drop axis\n",
    "dfm = dfm.drop(['nationality', 'place_tweet'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>time</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>778167031598374912</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>ãªã‚“ã‹ã‚‚ã†ã ã‚‹ã„è‡ªåˆ†ãŒå«Œ</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "      <td>æ±åŒ—åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2989602496</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>æ—©ã!!å¯¾å¿œã—ã¦ãã ã•ã„ï¼é‡å…šã‚„ã‚‰ä¸å…šã‚„ã‚‰è¨€ã£ã¦ã‚‹å ´åˆã§ã¯ãªã„ã§ã™ã‚ˆï¼äº¡ããªã£ãŸæ–¹ã‚’æŒã¡ã ã—...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:55</td>\n",
       "      <td>æ±åŒ—åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>986162527007490048</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>å®®è—¤å®˜ä¹éƒã•ã‚“ãŒæ„ŸæŸ“ã—ã¦ã—ã¾ã£ãŸã¿ãŸã„ã§ã™ãŒ22æ—¥ã«ä¸­å±±å„ªé¦¬ãã‚“ã®èˆå°ã«å®®è—¤å®˜ä¹éƒã•ã‚“ã¨æ—¥æ‘...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:34</td>\n",
       "      <td>é–¢æ±åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3281248128</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>ã¯ããã€ã™ã§ã«ç·Šå¼µã™ã‚‹ãªãã€‚ã€‚ã€‚</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:58:33</td>\n",
       "      <td>ä¸­éƒ¨åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50943527</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>#å¿—æ‘ã‘ã‚“é¢ç™½ã„ã€‚è¦‹ãŸã‚‰æ³£ã‘ã¦ããŸã€‚æ‚²ã—ã„ã€‚</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:58:11</td>\n",
       "      <td>é–¢æ±åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4973</th>\n",
       "      <td>334036907</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>å®‰å€ã¡ã‚ƒã‚“ã«ã¯ã¾ã è¾ã‚ã‚‰ã‚Œã¡ã‚ƒå›°ã‚‹ã€‚ãƒ‰ãƒ³åº•ã¾ã§è¦‹å±Šã‘ã¦è²°ã‚ãªã„ã¨</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:13:24</td>\n",
       "      <td>æ±äº¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4975</th>\n",
       "      <td>611507926</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>@nyantaremama ãƒ†ã‚£ãƒƒã‚·ãƒ¥ä¸€ç®±è²·ã†ã®ã«â€¼ï¸ä½•è»’ã¾ã‚ã£ãŸã‹ï¼Ÿçµå±€åœ°å…ƒã®ã‚³ãƒ³ãƒ“ãƒ‹ğŸªã«...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:13:11</td>\n",
       "      <td>æ±äº¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>352308836</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>æ ªå®‰ã§å¹´é‡‘ãŒã¾ãŸã€æ¸›ã‚‰ã•ã‚Œã‚‹â€¼ï¸æ ªå¼æŠ•è³‡ã—ã‚ã¯ã¨è¨€ã£ã¦ãªã„ã®ã«ä½•ã ã‹æ–‡å¥ã°ã‹ã‚Šã®ã€ãƒ„ã‚¤ãƒ¼ãƒˆè‡ª...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:04:04</td>\n",
       "      <td>æ±äº¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>1635284976</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>ç†Šæœ¬ã¯3å¹´+2å¹´ï¼ˆé–“ã‚ã„ã¦ã‚‹ï¼‰ä½ã‚“ã§ãŸã‘ã©ã€ãƒ‘ãƒ«ã‚³ãªããªã£ãŸã®æ‚²ã—ã„ã€‚ã‚‚ã£ã¨é•·ãä½ã‚“ã§ã‚‹ç†Šæœ¬...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:03:22</td>\n",
       "      <td>æ±äº¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>141486067</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>æ€’ã‚Šã‚’ã©ã“ã‹ã«ã¶ã¤ã‘ãŸããªã‚‹ã®ã¯ã‚ã‹ã‚‹ã‘ã©ã€åº—å“¡ã•ã‚“ã«ã¶ã¤ã‘ã‚‹å¥´ã‚‚ã„ã‚‹ã‚“ã ãªã€‚ã“ã†ã„ã†äº‹æ…‹ã®...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:01:33</td>\n",
       "      <td>æ±äº¬</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57880 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 user_id        date  \\\n",
       "0     778167031598374912  2020-03-31   \n",
       "1             2989602496  2020-03-31   \n",
       "2     986162527007490048  2020-03-31   \n",
       "3             3281248128  2020-03-31   \n",
       "4               50943527  2020-03-31   \n",
       "...                  ...         ...   \n",
       "4973           334036907  2020-03-01   \n",
       "4975           611507926  2020-03-01   \n",
       "4992           352308836  2020-03-01   \n",
       "4993          1635284976  2020-03-01   \n",
       "4994           141486067  2020-03-01   \n",
       "\n",
       "                                                  tweet  year  month  day  \\\n",
       "0                                          ãªã‚“ã‹ã‚‚ã†ã ã‚‹ã„è‡ªåˆ†ãŒå«Œ  2020      3   31   \n",
       "1     æ—©ã!!å¯¾å¿œã—ã¦ãã ã•ã„ï¼é‡å…šã‚„ã‚‰ä¸å…šã‚„ã‚‰è¨€ã£ã¦ã‚‹å ´åˆã§ã¯ãªã„ã§ã™ã‚ˆï¼äº¡ããªã£ãŸæ–¹ã‚’æŒã¡ã ã—...  2020      3   31   \n",
       "2     å®®è—¤å®˜ä¹éƒã•ã‚“ãŒæ„ŸæŸ“ã—ã¦ã—ã¾ã£ãŸã¿ãŸã„ã§ã™ãŒ22æ—¥ã«ä¸­å±±å„ªé¦¬ãã‚“ã®èˆå°ã«å®®è—¤å®˜ä¹éƒã•ã‚“ã¨æ—¥æ‘...  2020      3   31   \n",
       "3                                      ã¯ããã€ã™ã§ã«ç·Šå¼µã™ã‚‹ãªãã€‚ã€‚ã€‚  2020      3   31   \n",
       "4                                #å¿—æ‘ã‘ã‚“é¢ç™½ã„ã€‚è¦‹ãŸã‚‰æ³£ã‘ã¦ããŸã€‚æ‚²ã—ã„ã€‚  2020      3   31   \n",
       "...                                                 ...   ...    ...  ...   \n",
       "4973                   å®‰å€ã¡ã‚ƒã‚“ã«ã¯ã¾ã è¾ã‚ã‚‰ã‚Œã¡ã‚ƒå›°ã‚‹ã€‚ãƒ‰ãƒ³åº•ã¾ã§è¦‹å±Šã‘ã¦è²°ã‚ãªã„ã¨  2020      3    1   \n",
       "4975  @nyantaremama ãƒ†ã‚£ãƒƒã‚·ãƒ¥ä¸€ç®±è²·ã†ã®ã«â€¼ï¸ä½•è»’ã¾ã‚ã£ãŸã‹ï¼Ÿçµå±€åœ°å…ƒã®ã‚³ãƒ³ãƒ“ãƒ‹ğŸªã«...  2020      3    1   \n",
       "4992  æ ªå®‰ã§å¹´é‡‘ãŒã¾ãŸã€æ¸›ã‚‰ã•ã‚Œã‚‹â€¼ï¸æ ªå¼æŠ•è³‡ã—ã‚ã¯ã¨è¨€ã£ã¦ãªã„ã®ã«ä½•ã ã‹æ–‡å¥ã°ã‹ã‚Šã®ã€ãƒ„ã‚¤ãƒ¼ãƒˆè‡ª...  2020      3    1   \n",
       "4993  ç†Šæœ¬ã¯3å¹´+2å¹´ï¼ˆé–“ã‚ã„ã¦ã‚‹ï¼‰ä½ã‚“ã§ãŸã‘ã©ã€ãƒ‘ãƒ«ã‚³ãªããªã£ãŸã®æ‚²ã—ã„ã€‚ã‚‚ã£ã¨é•·ãä½ã‚“ã§ã‚‹ç†Šæœ¬...  2020      3    1   \n",
       "4994  æ€’ã‚Šã‚’ã©ã“ã‹ã«ã¶ã¤ã‘ãŸããªã‚‹ã®ã¯ã‚ã‹ã‚‹ã‘ã©ã€åº—å“¡ã•ã‚“ã«ã¶ã¤ã‘ã‚‹å¥´ã‚‚ã„ã‚‹ã‚“ã ãªã€‚ã“ã†ã„ã†äº‹æ…‹ã®...  2020      3    1   \n",
       "\n",
       "      hour      time  area  \n",
       "0       23  23:59:58  æ±åŒ—åœ°æ–¹  \n",
       "1       23  23:59:55  æ±åŒ—åœ°æ–¹  \n",
       "2       23  23:59:34  é–¢æ±åœ°æ–¹  \n",
       "3       23  23:58:33  ä¸­éƒ¨åœ°æ–¹  \n",
       "4       23  23:58:11  é–¢æ±åœ°æ–¹  \n",
       "...    ...       ...   ...  \n",
       "4973     0  00:13:24    æ±äº¬  \n",
       "4975     0  00:13:11    æ±äº¬  \n",
       "4992     0  00:04:04    æ±äº¬  \n",
       "4993     0  00:03:22    æ±äº¬  \n",
       "4994     0  00:01:33    æ±äº¬  \n",
       "\n",
       "[57880 rows x 9 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## group by area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_dict = {}\n",
    "areas = dfm.area.unique().tolist()\n",
    "initial_area = ['A','B','C','D','E','F','G','H','I','J']\n",
    "for area, initial in zip(areas, initial_area):\n",
    "    area_dict[area] = initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into csv of each area\n",
    "for area in areas:\n",
    "    dfm.groupby('area').get_group(area).to_csv('../data/dfs_by_area2/mar_'+area_dict[area]+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>time</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>778167031598374912</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>ãªã‚“ã‹ã‚‚ã†ã ã‚‹ã„è‡ªåˆ†ãŒå«Œ</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "      <td>æ±åŒ—åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2989602496</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>æ—©ã!!å¯¾å¿œã—ã¦ãã ã•ã„ï¼é‡å…šã‚„ã‚‰ä¸å…šã‚„ã‚‰è¨€ã£ã¦ã‚‹å ´åˆã§ã¯ãªã„ã§ã™ã‚ˆï¼äº¡ããªã£ãŸæ–¹ã‚’æŒã¡ã ã—...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:55</td>\n",
       "      <td>æ±åŒ—åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>986162527007490048</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>å®®è—¤å®˜ä¹éƒã•ã‚“ãŒæ„ŸæŸ“ã—ã¦ã—ã¾ã£ãŸã¿ãŸã„ã§ã™ãŒ22æ—¥ã«ä¸­å±±å„ªé¦¬ãã‚“ã®èˆå°ã«å®®è—¤å®˜ä¹éƒã•ã‚“ã¨æ—¥æ‘...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:34</td>\n",
       "      <td>é–¢æ±åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3281248128</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>ã¯ããã€ã™ã§ã«ç·Šå¼µã™ã‚‹ãªãã€‚ã€‚ã€‚</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:58:33</td>\n",
       "      <td>ä¸­éƒ¨åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50943527</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>#å¿—æ‘ã‘ã‚“é¢ç™½ã„ã€‚è¦‹ãŸã‚‰æ³£ã‘ã¦ããŸã€‚æ‚²ã—ã„ã€‚</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:58:11</td>\n",
       "      <td>é–¢æ±åœ°æ–¹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4973</th>\n",
       "      <td>334036907</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>å®‰å€ã¡ã‚ƒã‚“ã«ã¯ã¾ã è¾ã‚ã‚‰ã‚Œã¡ã‚ƒå›°ã‚‹ã€‚ãƒ‰ãƒ³åº•ã¾ã§è¦‹å±Šã‘ã¦è²°ã‚ãªã„ã¨</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:13:24</td>\n",
       "      <td>æ±äº¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4975</th>\n",
       "      <td>611507926</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>@nyantaremama ãƒ†ã‚£ãƒƒã‚·ãƒ¥ä¸€ç®±è²·ã†ã®ã«â€¼ï¸ä½•è»’ã¾ã‚ã£ãŸã‹ï¼Ÿçµå±€åœ°å…ƒã®ã‚³ãƒ³ãƒ“ãƒ‹ğŸªã«...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:13:11</td>\n",
       "      <td>æ±äº¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>352308836</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>æ ªå®‰ã§å¹´é‡‘ãŒã¾ãŸã€æ¸›ã‚‰ã•ã‚Œã‚‹â€¼ï¸æ ªå¼æŠ•è³‡ã—ã‚ã¯ã¨è¨€ã£ã¦ãªã„ã®ã«ä½•ã ã‹æ–‡å¥ã°ã‹ã‚Šã®ã€ãƒ„ã‚¤ãƒ¼ãƒˆè‡ª...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:04:04</td>\n",
       "      <td>æ±äº¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>1635284976</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>ç†Šæœ¬ã¯3å¹´+2å¹´ï¼ˆé–“ã‚ã„ã¦ã‚‹ï¼‰ä½ã‚“ã§ãŸã‘ã©ã€ãƒ‘ãƒ«ã‚³ãªããªã£ãŸã®æ‚²ã—ã„ã€‚ã‚‚ã£ã¨é•·ãä½ã‚“ã§ã‚‹ç†Šæœ¬...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:03:22</td>\n",
       "      <td>æ±äº¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>141486067</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>æ€’ã‚Šã‚’ã©ã“ã‹ã«ã¶ã¤ã‘ãŸããªã‚‹ã®ã¯ã‚ã‹ã‚‹ã‘ã©ã€åº—å“¡ã•ã‚“ã«ã¶ã¤ã‘ã‚‹å¥´ã‚‚ã„ã‚‹ã‚“ã ãªã€‚ã“ã†ã„ã†äº‹æ…‹ã®...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:01:33</td>\n",
       "      <td>æ±äº¬</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57880 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 user_id        date  \\\n",
       "0     778167031598374912  2020-03-31   \n",
       "1             2989602496  2020-03-31   \n",
       "2     986162527007490048  2020-03-31   \n",
       "3             3281248128  2020-03-31   \n",
       "4               50943527  2020-03-31   \n",
       "...                  ...         ...   \n",
       "4973           334036907  2020-03-01   \n",
       "4975           611507926  2020-03-01   \n",
       "4992           352308836  2020-03-01   \n",
       "4993          1635284976  2020-03-01   \n",
       "4994           141486067  2020-03-01   \n",
       "\n",
       "                                                  tweet  year  month  day  \\\n",
       "0                                          ãªã‚“ã‹ã‚‚ã†ã ã‚‹ã„è‡ªåˆ†ãŒå«Œ  2020      3   31   \n",
       "1     æ—©ã!!å¯¾å¿œã—ã¦ãã ã•ã„ï¼é‡å…šã‚„ã‚‰ä¸å…šã‚„ã‚‰è¨€ã£ã¦ã‚‹å ´åˆã§ã¯ãªã„ã§ã™ã‚ˆï¼äº¡ããªã£ãŸæ–¹ã‚’æŒã¡ã ã—...  2020      3   31   \n",
       "2     å®®è—¤å®˜ä¹éƒã•ã‚“ãŒæ„ŸæŸ“ã—ã¦ã—ã¾ã£ãŸã¿ãŸã„ã§ã™ãŒ22æ—¥ã«ä¸­å±±å„ªé¦¬ãã‚“ã®èˆå°ã«å®®è—¤å®˜ä¹éƒã•ã‚“ã¨æ—¥æ‘...  2020      3   31   \n",
       "3                                      ã¯ããã€ã™ã§ã«ç·Šå¼µã™ã‚‹ãªãã€‚ã€‚ã€‚  2020      3   31   \n",
       "4                                #å¿—æ‘ã‘ã‚“é¢ç™½ã„ã€‚è¦‹ãŸã‚‰æ³£ã‘ã¦ããŸã€‚æ‚²ã—ã„ã€‚  2020      3   31   \n",
       "...                                                 ...   ...    ...  ...   \n",
       "4973                   å®‰å€ã¡ã‚ƒã‚“ã«ã¯ã¾ã è¾ã‚ã‚‰ã‚Œã¡ã‚ƒå›°ã‚‹ã€‚ãƒ‰ãƒ³åº•ã¾ã§è¦‹å±Šã‘ã¦è²°ã‚ãªã„ã¨  2020      3    1   \n",
       "4975  @nyantaremama ãƒ†ã‚£ãƒƒã‚·ãƒ¥ä¸€ç®±è²·ã†ã®ã«â€¼ï¸ä½•è»’ã¾ã‚ã£ãŸã‹ï¼Ÿçµå±€åœ°å…ƒã®ã‚³ãƒ³ãƒ“ãƒ‹ğŸªã«...  2020      3    1   \n",
       "4992  æ ªå®‰ã§å¹´é‡‘ãŒã¾ãŸã€æ¸›ã‚‰ã•ã‚Œã‚‹â€¼ï¸æ ªå¼æŠ•è³‡ã—ã‚ã¯ã¨è¨€ã£ã¦ãªã„ã®ã«ä½•ã ã‹æ–‡å¥ã°ã‹ã‚Šã®ã€ãƒ„ã‚¤ãƒ¼ãƒˆè‡ª...  2020      3    1   \n",
       "4993  ç†Šæœ¬ã¯3å¹´+2å¹´ï¼ˆé–“ã‚ã„ã¦ã‚‹ï¼‰ä½ã‚“ã§ãŸã‘ã©ã€ãƒ‘ãƒ«ã‚³ãªããªã£ãŸã®æ‚²ã—ã„ã€‚ã‚‚ã£ã¨é•·ãä½ã‚“ã§ã‚‹ç†Šæœ¬...  2020      3    1   \n",
       "4994  æ€’ã‚Šã‚’ã©ã“ã‹ã«ã¶ã¤ã‘ãŸããªã‚‹ã®ã¯ã‚ã‹ã‚‹ã‘ã©ã€åº—å“¡ã•ã‚“ã«ã¶ã¤ã‘ã‚‹å¥´ã‚‚ã„ã‚‹ã‚“ã ãªã€‚ã“ã†ã„ã†äº‹æ…‹ã®...  2020      3    1   \n",
       "\n",
       "      hour      time  area  \n",
       "0       23  23:59:58  æ±åŒ—åœ°æ–¹  \n",
       "1       23  23:59:55  æ±åŒ—åœ°æ–¹  \n",
       "2       23  23:59:34  é–¢æ±åœ°æ–¹  \n",
       "3       23  23:58:33  ä¸­éƒ¨åœ°æ–¹  \n",
       "4       23  23:58:11  é–¢æ±åœ°æ–¹  \n",
       "...    ...       ...   ...  \n",
       "4973     0  00:13:24    æ±äº¬  \n",
       "4975     0  00:13:11    æ±äº¬  \n",
       "4992     0  00:04:04    æ±äº¬  \n",
       "4993     0  00:03:22    æ±äº¬  \n",
       "4994     0  00:01:33    æ±äº¬  \n",
       "\n",
       "[57880 rows x 9 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'é–¢æ±åœ°æ–¹': 'A',\n",
       " 'ä¸­å›½åœ°æ–¹': 'B',\n",
       " 'è¿‘ç•¿åœ°æ–¹': 'C',\n",
       " 'ä¸­éƒ¨åœ°æ–¹': 'D',\n",
       " 'ä¹å·ãƒ»æ²–ç¸„åœ°æ–¹': 'E',\n",
       " 'åŒ—æµ·é“åœ°æ–¹': 'F',\n",
       " 'æ±åŒ—åœ°æ–¹': 'G',\n",
       " 'å››å›½åœ°æ–¹': 'H',\n",
       " 'é¦–éƒ½åœ': 'I',\n",
       " 'æ±äº¬': 'J'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ORs\n",
    "zip_dir  = \"../data/dfs_by_area2\" # new_dfs2_jst\n",
    "csvfiles = glob(os.path.join(zip_dir, 'mar_*'))\n",
    "\n",
    "feb_dfs = []\n",
    "for csvfile in csvfiles:\n",
    "    df = pd.read_csv(csvfile).drop(['Unnamed: 0', 'area'], axis=1)\n",
    "    feb_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/dfs_by_area/feb_A.csv',\n",
       " '../data/dfs_by_area/feb_B.csv',\n",
       " '../data/dfs_by_area/feb_C.csv',\n",
       " '../data/dfs_by_area/feb_D.csv',\n",
       " '../data/dfs_by_area/feb_E.csv',\n",
       " '../data/dfs_by_area/feb_F.csv',\n",
       " '../data/dfs_by_area/feb_G.csv',\n",
       " '../data/dfs_by_area/feb_H.csv',\n",
       " '../data/dfs_by_area/feb_I.csv',\n",
       " '../data/dfs_by_area/feb_J.csv']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csvfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>108768768</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>å…«æœ¨ã¡ã‚ƒã‚“ã€å¿ƒé…ã§ã™ï¼ƒãã«ã¾ã‚‹ï½„ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒ‹ãƒ¥ãƒ¼ã‚¹:æŸåŸç«œäºŒæ°ãŒã‚³ãƒ­ãƒŠçµŒéè¦³å¯Ÿè€…ã«æ„ŸæŸ“å¡šåŸæ°ã¨...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:52:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1916947039</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>æ˜æ—¥ã‹ã‚‰ç¤¾ä¼šäººã€‚æ­£ç›´ä¸å®‰ã ã—ã€æ€–ã„ã€‚ã‘ã©ã€æ¥å¹´ãƒ¢ãƒãƒ¬ãƒ¼ãƒ«é‹è»¢å£«ã«ãªã‚‹ç‚ºã«ã€‚1æ—¥1æ—¥ãŒå‹è² ã ã¨...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:49:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1065957356079476736</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>@nikuohturbor @douji_yama æœ¬å½“ã¯è¡ŒããŸã„(è¡Œã‹ãªãã‚ƒã‚¤ã‚±ãªã„)ã®ã«...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:49:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1141379106199576576</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>@Hitomi1104dance ã²ã‚ƒãƒ¼ï¼ã‚ã‚ŠãŒã¨ã†ï¼ã“ã®æ™‚æœŸã¯çš†ã‚“ãªãŒä¸å®‰ã ã‚ˆã­ï¼åŠ©ã‘åˆãŠã†ï¼</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:48:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>990812722634768389</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>å°æ± éƒ½çŸ¥äº‹ã€æ–°ãŸã«78äººã®æ„ŸæŸ“ã‚’å—ã‘ã€Œæ„ŸæŸ“è€…ãŒã•ã‚‰ã«å¢—ãˆã¤ã¤ã‚ã‚‹ã€‚å¤§å¤‰æ‡¸å¿µã•ã‚Œã‚‹çŠ¶æ³ã€ ht...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:46:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7990</th>\n",
       "      <td>334036907</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>å®‰å€ã¡ã‚ƒã‚“ã«ã¯ã¾ã è¾ã‚ã‚‰ã‚Œã¡ã‚ƒå›°ã‚‹ã€‚ãƒ‰ãƒ³åº•ã¾ã§è¦‹å±Šã‘ã¦è²°ã‚ãªã„ã¨</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:13:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7991</th>\n",
       "      <td>611507926</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>@nyantaremama ãƒ†ã‚£ãƒƒã‚·ãƒ¥ä¸€ç®±è²·ã†ã®ã«â€¼ï¸ä½•è»’ã¾ã‚ã£ãŸã‹ï¼Ÿçµå±€åœ°å…ƒã®ã‚³ãƒ³ãƒ“ãƒ‹ğŸªã«...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:13:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7992</th>\n",
       "      <td>352308836</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>æ ªå®‰ã§å¹´é‡‘ãŒã¾ãŸã€æ¸›ã‚‰ã•ã‚Œã‚‹â€¼ï¸æ ªå¼æŠ•è³‡ã—ã‚ã¯ã¨è¨€ã£ã¦ãªã„ã®ã«ä½•ã ã‹æ–‡å¥ã°ã‹ã‚Šã®ã€ãƒ„ã‚¤ãƒ¼ãƒˆè‡ª...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:04:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7993</th>\n",
       "      <td>1635284976</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>ç†Šæœ¬ã¯3å¹´+2å¹´ï¼ˆé–“ã‚ã„ã¦ã‚‹ï¼‰ä½ã‚“ã§ãŸã‘ã©ã€ãƒ‘ãƒ«ã‚³ãªããªã£ãŸã®æ‚²ã—ã„ã€‚ã‚‚ã£ã¨é•·ãä½ã‚“ã§ã‚‹ç†Šæœ¬...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:03:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7994</th>\n",
       "      <td>141486067</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>æ€’ã‚Šã‚’ã©ã“ã‹ã«ã¶ã¤ã‘ãŸããªã‚‹ã®ã¯ã‚ã‹ã‚‹ã‘ã©ã€åº—å“¡ã•ã‚“ã«ã¶ã¤ã‘ã‚‹å¥´ã‚‚ã„ã‚‹ã‚“ã ãªã€‚ã“ã†ã„ã†äº‹æ…‹ã®...</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00:01:33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7995 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id        date  \\\n",
       "0               108768768  2020-03-31   \n",
       "1              1916947039  2020-03-31   \n",
       "2     1065957356079476736  2020-03-31   \n",
       "3     1141379106199576576  2020-03-31   \n",
       "4      990812722634768389  2020-03-31   \n",
       "...                   ...         ...   \n",
       "7990            334036907  2020-03-01   \n",
       "7991            611507926  2020-03-01   \n",
       "7992            352308836  2020-03-01   \n",
       "7993           1635284976  2020-03-01   \n",
       "7994            141486067  2020-03-01   \n",
       "\n",
       "                                                  tweet  year  month  day  \\\n",
       "0     å…«æœ¨ã¡ã‚ƒã‚“ã€å¿ƒé…ã§ã™ï¼ƒãã«ã¾ã‚‹ï½„ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒ‹ãƒ¥ãƒ¼ã‚¹:æŸåŸç«œäºŒæ°ãŒã‚³ãƒ­ãƒŠçµŒéè¦³å¯Ÿè€…ã«æ„ŸæŸ“å¡šåŸæ°ã¨...  2020      3   31   \n",
       "1     æ˜æ—¥ã‹ã‚‰ç¤¾ä¼šäººã€‚æ­£ç›´ä¸å®‰ã ã—ã€æ€–ã„ã€‚ã‘ã©ã€æ¥å¹´ãƒ¢ãƒãƒ¬ãƒ¼ãƒ«é‹è»¢å£«ã«ãªã‚‹ç‚ºã«ã€‚1æ—¥1æ—¥ãŒå‹è² ã ã¨...  2020      3   31   \n",
       "2     @nikuohturbor @douji_yama æœ¬å½“ã¯è¡ŒããŸã„(è¡Œã‹ãªãã‚ƒã‚¤ã‚±ãªã„)ã®ã«...  2020      3   31   \n",
       "3      @Hitomi1104dance ã²ã‚ƒãƒ¼ï¼ã‚ã‚ŠãŒã¨ã†ï¼ã“ã®æ™‚æœŸã¯çš†ã‚“ãªãŒä¸å®‰ã ã‚ˆã­ï¼åŠ©ã‘åˆãŠã†ï¼  2020      3   31   \n",
       "4     å°æ± éƒ½çŸ¥äº‹ã€æ–°ãŸã«78äººã®æ„ŸæŸ“ã‚’å—ã‘ã€Œæ„ŸæŸ“è€…ãŒã•ã‚‰ã«å¢—ãˆã¤ã¤ã‚ã‚‹ã€‚å¤§å¤‰æ‡¸å¿µã•ã‚Œã‚‹çŠ¶æ³ã€ ht...  2020      3   31   \n",
       "...                                                 ...   ...    ...  ...   \n",
       "7990                   å®‰å€ã¡ã‚ƒã‚“ã«ã¯ã¾ã è¾ã‚ã‚‰ã‚Œã¡ã‚ƒå›°ã‚‹ã€‚ãƒ‰ãƒ³åº•ã¾ã§è¦‹å±Šã‘ã¦è²°ã‚ãªã„ã¨  2020      3    1   \n",
       "7991  @nyantaremama ãƒ†ã‚£ãƒƒã‚·ãƒ¥ä¸€ç®±è²·ã†ã®ã«â€¼ï¸ä½•è»’ã¾ã‚ã£ãŸã‹ï¼Ÿçµå±€åœ°å…ƒã®ã‚³ãƒ³ãƒ“ãƒ‹ğŸªã«...  2020      3    1   \n",
       "7992  æ ªå®‰ã§å¹´é‡‘ãŒã¾ãŸã€æ¸›ã‚‰ã•ã‚Œã‚‹â€¼ï¸æ ªå¼æŠ•è³‡ã—ã‚ã¯ã¨è¨€ã£ã¦ãªã„ã®ã«ä½•ã ã‹æ–‡å¥ã°ã‹ã‚Šã®ã€ãƒ„ã‚¤ãƒ¼ãƒˆè‡ª...  2020      3    1   \n",
       "7993  ç†Šæœ¬ã¯3å¹´+2å¹´ï¼ˆé–“ã‚ã„ã¦ã‚‹ï¼‰ä½ã‚“ã§ãŸã‘ã©ã€ãƒ‘ãƒ«ã‚³ãªããªã£ãŸã®æ‚²ã—ã„ã€‚ã‚‚ã£ã¨é•·ãä½ã‚“ã§ã‚‹ç†Šæœ¬...  2020      3    1   \n",
       "7994  æ€’ã‚Šã‚’ã©ã“ã‹ã«ã¶ã¤ã‘ãŸããªã‚‹ã®ã¯ã‚ã‹ã‚‹ã‘ã©ã€åº—å“¡ã•ã‚“ã«ã¶ã¤ã‘ã‚‹å¥´ã‚‚ã„ã‚‹ã‚“ã ãªã€‚ã“ã†ã„ã†äº‹æ…‹ã®...  2020      3    1   \n",
       "\n",
       "      hour      time  \n",
       "0       23  23:52:20  \n",
       "1       23  23:49:39  \n",
       "2       23  23:49:32  \n",
       "3       23  23:48:53  \n",
       "4       23  23:46:43  \n",
       "...    ...       ...  \n",
       "7990     0  00:13:24  \n",
       "7991     0  00:13:11  \n",
       "7992     0  00:04:04  \n",
       "7993     0  00:03:22  \n",
       "7994     0  00:01:33  \n",
       "\n",
       "[7995 rows x 8 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feb_dfs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_hour(dfor, filetype, savedir, key_lang, area):\n",
    "    # Prepartion of dfw \n",
    "    results = []\n",
    "    dfw = pd.read_csv('../data/WordTimeSeries.csv', encoding='utf-8').rename(columns={'Unnamed: 0': 'types',\n",
    "                                                                                    'file name': 'file_name', \n",
    "                                                                                    'orignal form': 'orignal_form',\n",
    "                                                                                    'English translation': 'English_translation'})\n",
    "    booleanDictionary = {True: 'TRUE', False: 'FALSE'} # Drop nan and False in order to use query \n",
    "    dfw = dfw.replace(booleanDictionary)\n",
    "    dfw = dfw.dropna(how='all')\n",
    "\n",
    "    # Prepration of keywords\n",
    "    filenames = [filename for filename in dfw.file_name.tolist() if filetype in filename]\n",
    "    xd = {}\n",
    "    xd['T'] = 'file_name.str.contains(\"T\")'\n",
    "    xd['D'] = 'file_name.str.contains(\"D\")'\n",
    "    xd['A'] = 'file_name.str.contains(\"A\")'\n",
    "    xd['V'] = 'file_name.str.contains(\"V\")'\n",
    "    xd['F'] = 'file_name.str.contains(\"F\")'\n",
    "    xd['C'] = 'file_name.str.contains(\"C\")'\n",
    "\n",
    "    query = xd[filetype]\n",
    "    dfw_c = dfw.query(query, engine='python')\n",
    "    \n",
    "    # Keyword \n",
    "    if key_lang == 'jp':\n",
    "        keywords = dfw_c.orignal_form.tolist() # English.ver: keywords = dfw_c.English_translation.tolist() \n",
    "    elif key_lang == 'en':\n",
    "        keywords = dfw_c.English_translation.tolist() \n",
    "    else:\n",
    "        print('Put accurate keyword language')\n",
    "    \n",
    "    # Find tweets\n",
    "    # Extract tweet\n",
    "    dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "\n",
    "    for col, key in zip(filenames, keywords):\n",
    "        l = []\n",
    "        for row in dfor_c.itertuples():\n",
    "            if key in row.tweet:\n",
    "                l.append(1)\n",
    "            else:\n",
    "                l.append(0)\n",
    "        dfor_c[col] = l\n",
    "\n",
    "    # Save dir\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "\n",
    "#     for i, df in enumerate(dfs):\n",
    "    df = dfor_c\n",
    "    unique_dates = df['date'].tolist()\n",
    "    unique_dates = sorted(set(unique_dates), key=unique_dates.index) # date values\n",
    "    df_date = df.groupby('date') # df grouped by date   \n",
    "\n",
    "    for date in unique_dates:\n",
    "        df_oneday = df_date.get_group(date) # df of one day\n",
    "        dfh = df_oneday.groupby('hour').sum()\n",
    "        dfh = dfh.drop(['year', 'month', 'user_id', 'day'],axis=1)\n",
    "        \n",
    "        # Save csv\n",
    "        outname = 'area'+area+'_'+filetype+str(date)+'.csv'\n",
    "        savename = os.path.join(outdir, outname)\n",
    "\n",
    "        # Save to csv\n",
    "        dfh.to_csv(savename)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "G\n",
      "H\n",
      "I\n",
      "J\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "initial_area = ['A','B','C','D','E','F','G','H','I','J']\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['A', 'A', 'V', 'F', 'C']\n",
    "for area, df in zip(initial_area, feb_dfs):\n",
    "    print(area)\n",
    "    for filetype in filetypes:\n",
    "        findtwt_hour(df, filetype=filetype, savedir='../results_location2/hour', key_lang='jp', area=area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### ***************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ORs\n",
    "zip_dir  = \"../data/dfs\" # new_dfs2_jst\n",
    "csvfiles = glob(os.path.join(zip_dir, 'dfor*'))\n",
    "\n",
    "dfors = []\n",
    "for csvfile in csvfiles:\n",
    "    df = pd.read_csv(csvfile).drop('Unnamed: 0', axis=1)\n",
    "    dfors.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Rts\n",
    "zip_dir  = \"../data/new_dfs2_jst\"\n",
    "csvfiles = glob(os.path.join(zip_dir, 'dfor*'))\n",
    "\n",
    "dfors = []\n",
    "for csvfile in csvfiles:\n",
    "    df1 = pd.read_csv(csvfile).drop('Unnamed: 0', axis=1)\n",
    "    dfors.append(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n",
      "[False]\n"
     ]
    }
   ],
   "source": [
    "for df in dfors:\n",
    "    print(df.RT_flag.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n",
      "[True]\n"
     ]
    }
   ],
   "source": [
    "for df in dfrts:\n",
    "    print(df.RT_flag.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of tweets included RT: 9286476\n"
     ]
    }
   ],
   "source": [
    "c = [len(df) for df in dfors]\n",
    "print('The amount of tweets included RT:', sum(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of tweets included RT: 6692071\n"
     ]
    }
   ],
   "source": [
    "c = [len(df) for df in dfrts]\n",
    "print('The amount of tweets included RT:', sum(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create csv by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_day(dfors, filetype, savedir, key_lang):\n",
    "    # Prepartion of dfw \n",
    "    results = []\n",
    "    dfs = []\n",
    "    dfw = pd.read_csv('../data/WordTimeSeries.csv', encoding='utf-8').rename(columns={'Unnamed: 0': 'types',\n",
    "                                                                                    'file name': 'file_name', \n",
    "                                                                                    'orignal form': 'orignal_form',\n",
    "                                                                                    'English translation': 'English_translation'})\n",
    "    booleanDictionary = {True: 'TRUE', False: 'FALSE'} # Drop nan and False in order to use query \n",
    "    dfw = dfw.replace(booleanDictionary)\n",
    "    dfw = dfw.dropna(how='all')\n",
    "    \n",
    "    # Prepration of keywords\n",
    "    filenames = [filename for filename in dfw.file_name.tolist() if filetype in filename]\n",
    "    xd = {}\n",
    "    xd['T'] = 'file_name.str.contains(\"T\")'\n",
    "    xd['D'] = 'file_name.str.contains(\"D\")'\n",
    "    xd['A'] = 'file_name.str.contains(\"A\")'\n",
    "    xd['V'] = 'file_name.str.contains(\"V\")'\n",
    "    xd['F'] = 'file_name.str.contains(\"F\")'\n",
    "    xd['C'] = 'file_name.str.contains(\"C\")'\n",
    "    \n",
    "    query = xd[filetype]\n",
    "    dfw_c = dfw.query(query, engine='python')\n",
    "    if key_lang == 'jp':\n",
    "        keywords = dfw_c.orignal_form.tolist() # English.ver: keywords = dfw_c.English_translation.tolist() \n",
    "    elif key_lang == 'en':\n",
    "        keywords = dfw_c.English_translation.tolist() \n",
    "    else:\n",
    "        print('Put accurate keyword language')\n",
    "            \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "         # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "        \n",
    "        # Add columns of keywords whose cell have 1 if this tweet includes a keyword \n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        \n",
    "        # Make rows\n",
    "        uni_dates = dfor_c['date'].tolist()\n",
    "        uni_dates = sorted(set(uni_dates), key=uni_dates.index) # date values\n",
    "        rows = []\n",
    "        for date in uni_dates:\n",
    "            d = {}\n",
    "            d['date'] = str(date)\n",
    "            for col in filenames:\n",
    "                _df = dfor_c.groupby('date').get_group(date)\n",
    "                d[col] = _df[col].sum()\n",
    "            rows.append(d)\n",
    "        \n",
    "        # Make cols\n",
    "        cols = filenames.copy()\n",
    "        cols.insert(0, 'date')\n",
    "        \n",
    "        # Make dfs with rows and cols\n",
    "        dft = pd.DataFrame(columns=cols)\n",
    "        for row in rows:\n",
    "            dft = dft.append(row, ignore_index=True) \n",
    "        dfs.append(dft)\n",
    "        \n",
    "    # Finally Connect dfs\n",
    "    for i, df in enumerate(dfs):\n",
    "        if i == 0:\n",
    "            dfnew = df\n",
    "        else:\n",
    "            dfnew = pd.concat([dfnew, df], axis=0)\n",
    "            \n",
    "    # Groupby and sort by date\n",
    "    dfnew = dfnew.groupby('date').sum()\n",
    "    \n",
    "    # Save\n",
    "    outname = filetype+'_original.csv'\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "    savename = os.path.join(outdir, outname)\n",
    "    dfnew.to_csv(savename)\n",
    "\n",
    "    return dfnew\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:38<00:00,  9.81s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:01<00:00, 12.16s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:09<00:00, 12.97s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:37<00:00,  3.78s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:54<00:00,  5.48s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:29<00:00,  8.92s/it]\n"
     ]
    }
   ],
   "source": [
    "# OR\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfors, filetype=filetype, savedir='../new_results_jst/orjp/day', key_lang='jp') # new_results_jst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:21<00:00,  2.10s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.27s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:10<00:00,  1.03s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:09<00:00,  1.07it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:09<00:00,  1.05it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "# OR\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfors, filetype=filetype, savedir='../new_results_jst/oren/day', key_lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:25<00:00, 20.53s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:34<00:00, 21.48s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:20<00:00, 20.05s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:41<00:00, 10.17s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:28<00:00,  8.88s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:53<00:00, 23.33s/it]\n"
     ]
    }
   ],
   "source": [
    "# RT\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfrts, filetype=filetype, savedir='../new_results_jst/rtjp/day', key_lang='jp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.91s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.51s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.33s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:11<00:00,  1.18s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.52s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:17<00:00,  1.74s/it]\n"
     ]
    }
   ],
   "source": [
    "# RT\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    dfnew = findtwt_day(dfrts, filetype=filetype, savedir='../new_results_jst/rten/day', key_lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create csv by hour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_hour(dfors, filetype, savedir, key_lang):\n",
    "    # Prepartion of dfw \n",
    "    results = []\n",
    "    dfs = []\n",
    "    dfw = pd.read_csv('../data/WordTimeSeries.csv', encoding='utf-8').rename(columns={'Unnamed: 0': 'types',\n",
    "                                                                                    'file name': 'file_name', \n",
    "                                                                                    'orignal form': 'orignal_form',\n",
    "                                                                                    'English translation': 'English_translation'})\n",
    "    booleanDictionary = {True: 'TRUE', False: 'FALSE'} # Drop nan and False in order to use query \n",
    "    dfw = dfw.replace(booleanDictionary)\n",
    "    dfw = dfw.dropna(how='all')\n",
    "\n",
    "    # Prepration of keywords\n",
    "    filenames = [filename for filename in dfw.file_name.tolist() if filetype in filename]\n",
    "    xd = {}\n",
    "    xd['T'] = 'file_name.str.contains(\"T\")'\n",
    "    xd['D'] = 'file_name.str.contains(\"D\")'\n",
    "    xd['A'] = 'file_name.str.contains(\"A\")'\n",
    "    xd['V'] = 'file_name.str.contains(\"V\")'\n",
    "    xd['F'] = 'file_name.str.contains(\"F\")'\n",
    "    xd['C'] = 'file_name.str.contains(\"C\")'\n",
    "\n",
    "    query = xd[filetype]\n",
    "    dfw_c = dfw.query(query, engine='python')\n",
    "    \n",
    "    # Keyword \n",
    "    if key_lang == 'jp':\n",
    "        keywords = dfw_c.orignal_form.tolist() # English.ver: keywords = dfw_c.English_translation.tolist() \n",
    "    elif key_lang == 'en':\n",
    "        keywords = dfw_c.English_translation.tolist() \n",
    "    else:\n",
    "        print('Put accurate keyword language')\n",
    "    \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "\n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        dfs.append(dfor_c)\n",
    "        \n",
    "    # Create csv file by day\n",
    "    all_df = []\n",
    "    nextdf, nextdf_date = '', ''\n",
    "    \n",
    "    # Save dir\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        unique_dates = df['date'].tolist()\n",
    "        unique_dates = sorted(set(unique_dates), key=unique_dates.index) # date values\n",
    "        df_date = df.groupby('date') # df grouped by date   \n",
    "\n",
    "        for date in unique_dates:\n",
    "            df_oneday = df_date.get_group(date) # df of one day\n",
    "            dfh = df_oneday.groupby('hour').sum()\n",
    "            dfh = dfh.drop(['year', 'month', 'user_id', 'RT_flag', 'day'],axis=1)\n",
    "            \n",
    "            # Save csv\n",
    "            outname = filetype+str(date)+'.csv'\n",
    "            savename = os.path.join(outdir, outname)\n",
    "            \n",
    "            if os.path.exists(savename):\n",
    "                # File1\n",
    "                existedfile = pd.read_csv(savename)\n",
    "                # ã„ã£ãŸã‚“csvã«ã™ã‚‹\n",
    "                _saveonce = 'once.csv'\n",
    "                dfh.to_csv(_saveonce)\n",
    "                # File2 \n",
    "                once = pd.read_csv(_saveonce)\n",
    "                # Concat \n",
    "                df_oneday1 = pd.concat([existedfile, once], axis=0)\n",
    "                # Finally save\n",
    "                dfh = df_oneday1.groupby('hour').sum()\n",
    "                \n",
    "            # Save to csv\n",
    "            dfh.to_csv(savename)\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Orjp, Oren, Rtjp, Rten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:24<00:00,  8.47s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:05<00:00, 12.57s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:12<00:00, 13.22s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:43<00:00,  4.33s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:54<00:00,  5.50s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:31<00:00,  9.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# Orjp\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfors, filetype=filetype, savedir='../new_results_jst2/orjp/hour', key_lang='jp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.57s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.26s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:10<00:00,  1.06s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:10<00:00,  1.01s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:10<00:00,  1.02s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "# Oren\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfors, filetype=filetype, savedir='../new_results_jst2/oren/hour', key_lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:25<00:00,  8.57s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:31<00:00,  9.10s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:42<00:00, 10.22s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:38<00:00,  3.90s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:05<00:00,  6.53s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:56<00:00, 11.65s/it]\n"
     ]
    }
   ],
   "source": [
    "# Rtjp\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfrts, filetype=filetype, savedir='../new_results_jst2/rtjp/hour', key_lang='jp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.37s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:14<00:00,  1.46s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.55s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.20s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.23s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:17<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "# Rten\n",
    "filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "# filetypes = ['D', 'A', 'V', 'F', 'C']\n",
    "for filetype in filetypes:\n",
    "    findtwt_hour(dfrts, filetype=filetype, savedir='../new_results_jst2/rten/hour', key_lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7827"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft = pd.read_csv('../results/rtjp/hour/T2020-02-28.csv')\n",
    "dft.T1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tT1\n",
    "21\t2958\n",
    "22\t2637\n",
    "23\t2546\n",
    "24\t4883\n",
    "25\t3352\n",
    "26\t2553\n",
    "27\t3206\n",
    "28\t5117\n",
    "29\t7827\n",
    "30\t2925"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã‚³ãƒ­ãƒŠå«æœ‰ç‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "963821 tweets conatin \"ã‚³ãƒ­ãƒŠ\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwt_corona(dfors, filetype=\"T\"):\n",
    "    # Prepration of keywords\n",
    "    ## ã‚³ãƒ­ãƒŠå°‚ç”¨\n",
    "    keywords = ['ã‚³ãƒ­ãƒŠ']\n",
    "    \n",
    "    c = [] # count volume\n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "        \n",
    "        c.append(len(dfor_c))\n",
    "    \n",
    "    print(sum(c), 'tweets conatin \"ã‚³ãƒ­ãƒŠ\"')\n",
    "\n",
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "findtwt_corona(dfrts, filetype=\"T\")\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corona by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetypes = ['T', 'D', 'A', 'V', 'F', 'C']\n",
    "def findtwtCrn_day(dfors, savename, savedir):\n",
    "    \n",
    "    dfs = []\n",
    "\n",
    "    # Keyowrds\n",
    "    keywords = ['ã‚³ãƒ­ãƒŠ', 'Corona', 'corona']\n",
    "    filenames = ['ã‚³ãƒ­ãƒŠ', 'Corona', 'corona']\n",
    "\n",
    "    \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet include keywords\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "        \n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        \n",
    "        # Make rows\n",
    "        uni_dates = dfor_c['date'].tolist()\n",
    "        uni_dates = sorted(set(uni_dates), key=uni_dates.index) # date values\n",
    "        rows = []\n",
    "        for date in uni_dates:\n",
    "            d = {}\n",
    "            d['date'] = str(date)\n",
    "            for col in filenames:\n",
    "                _df = dfor_c.groupby('date').get_group(date)\n",
    "                d[col] = _df[col].sum()\n",
    "            rows.append(d)\n",
    "        \n",
    "        # Make cols\n",
    "        cols = filenames.copy()\n",
    "        cols.insert(0, 'date')\n",
    "        \n",
    "        # Make dfs with rows and cols\n",
    "        dft = pd.DataFrame(columns=cols)\n",
    "        for row in rows:\n",
    "            dft = dft.append(row, ignore_index=True)\n",
    "        \n",
    "        dfs.append(dft)\n",
    "\n",
    "    # Finally Connect dfs\n",
    "    for i, df in enumerate(dfs):\n",
    "        if i == 0:\n",
    "            dfnew = df\n",
    "        else:\n",
    "            dfnew = pd.concat([dfnew, df], axis=0)\n",
    "            \n",
    "    # Groupby and sort by date\n",
    "    dfnew = dfnew.groupby('date').sum()\n",
    "    \n",
    "    # Save\n",
    "    outname = savename+'.csv'\n",
    "    outdir = savedir\n",
    "\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "\n",
    "    savename = os.path.join(outdir, outname)    \n",
    "    dfnew.to_csv(savename)\n",
    "\n",
    "    return dfnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:18<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "dfnew = findtwtCrn_day(dfors, savename='corona', savedir='../new_results_jst/Corona/or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "dfnew = findtwtCrn_day(dfrts, savename='corona', savedir='../new_results_jst2/Corona/rt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corona by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findtwtCrn_hour(dfors, savename, savedir):\n",
    "    # Prepartion \n",
    "    results = []\n",
    "    dfs = []\n",
    "    \n",
    "    # Save dir\n",
    "    outdir = savedir\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "    \n",
    "    # Keyowrds\n",
    "    keywords = ['ã‚³ãƒ­ãƒŠ', 'Corona', 'corona']\n",
    "    filenames = ['ã‚³ãƒ­ãƒŠ', 'Corona', 'corona']\n",
    "    \n",
    "    # Find tweets\n",
    "    for dfor in tqdm(dfors):\n",
    "        # Extract tweet\n",
    "        dfor_c = dfor[dfor['tweet'].str.contains('|'.join(keywords))]\n",
    "\n",
    "        for col, key in zip(filenames, keywords):\n",
    "            l = []\n",
    "            for row in dfor_c.itertuples():\n",
    "                if key in row.tweet:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            dfor_c[col] = l\n",
    "        \n",
    "        dfs.append(dfor_c)\n",
    "        \n",
    "    # Create csv file by day\n",
    "    all_df = []\n",
    "    nextdf, nextdf_date = '', ''\n",
    "    \n",
    "    # Save dir\n",
    "    outdir = savedir\n",
    "    filetype = savename\n",
    "    if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        unique_dates = df['date'].tolist()\n",
    "        unique_dates = sorted(set(unique_dates), key=unique_dates.index) # date values\n",
    "        df_date = df.groupby('date') # df grouped by date   \n",
    "        \n",
    "        for date in unique_dates:\n",
    "            df_oneday = df_date.get_group(date) # df of one day\n",
    "            dfh = df_oneday.groupby('hour').sum()\n",
    "            dfh = dfh.drop(['year', 'month', 'user_id', 'RT_flag', 'day'],axis=1)\n",
    "            \n",
    "            outname = filetype+str(date)+'.csv'\n",
    "            savename = os.path.join(outdir, outname)\n",
    "            \n",
    "            if os.path.exists(savename):\n",
    "                # File1\n",
    "                existedfile = pd.read_csv(savename)\n",
    "                \n",
    "                # ã„ã£ãŸã‚“csvã«ã™ã‚‹\n",
    "                _saveonce = 'once.csv'\n",
    "                dfh.to_csv(_saveonce)\n",
    "                # File2 \n",
    "                once = pd.read_csv(_saveonce)\n",
    "                \n",
    "                # Concat \n",
    "                df_oneday1 = pd.concat([existedfile, once], axis=0)\n",
    "\n",
    "                # Finally save\n",
    "                dfh = df_oneday1.groupby('hour').sum()\n",
    "                \n",
    "            # Save to csv\n",
    "            dfh.to_csv(savename)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:09<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "findtwtCrn_hour(dfors, savename='corona', savedir='../new_results_jst2/Corona/or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "findtwtCrn_hour(dfrts, savename='corona', savedir='../new_results_jst/Corona/rt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_csv('../new_results_jst/rten/hour/V2020-02-01.csv')\n",
    "t2 = pd.read_csv('../new_results/rten/hour/V2020-01-31.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "print(t1.loc[0:8,'V4'].sum(), t2.V4.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>RT_flag</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1111855071380168705</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @k7LssPYI5D85fxI: https://t.co/5KgwBN00NSå¿—æ‘...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127471475</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @84MadokaMary: ã“ã®å›½ã«ã¯çŸ¥ã‚‰ã‚“ã ã‘ã§ã‚ã¡ã‚ƒã‚ã¡ã‚ƒãªé‡ã®ç¤¾ä¼šä¿éšœãŒã‚ã‚‹ã®...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138821520</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @zibumitunari: æ˜æ—¥ã¯ã‚¨ã‚¤ãƒ—ãƒªãƒ«ãƒ•ãƒ¼ãƒ«ã ãŒã€ã‚³ãƒ­ãƒŠé–¢é€£ã®å˜˜ã¯çµ¶å¯¾ã«ã‚„ã‚ã‚ˆ...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>962192359059472384</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @akagiichirou: ãƒã‚¹ã‚¯ã®ï¼—å‰²ãŒä¸­å›½ã‹ã‚‰ã®è¼¸å…¥ã«é ¼ã£ã¦ããŸãŒæµé€šç¶²ãŒæ··ä¹±ã™...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128360620</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>RT @bbcnewsjapan: BBCãƒ‹ãƒ¥ãƒ¼ã‚¹ - ãƒãƒ³ã‚¬ãƒªãƒ¼æ”¿åºœã€æ–°å‹ã‚¦ã‚¤ãƒ«ã‚¹å¯¾ç­–ã§å¼·...</td>\n",
       "      <td>True</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>23:59:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user_id        date  \\\n",
       "0  1111855071380168705  2020-03-31   \n",
       "1            127471475  2020-03-31   \n",
       "2            138821520  2020-03-31   \n",
       "3   962192359059472384  2020-03-31   \n",
       "4            128360620  2020-03-31   \n",
       "\n",
       "                                               tweet  RT_flag  year  month  \\\n",
       "0  RT @k7LssPYI5D85fxI: https://t.co/5KgwBN00NSå¿—æ‘...     True  2020      3   \n",
       "1  RT @84MadokaMary: ã“ã®å›½ã«ã¯çŸ¥ã‚‰ã‚“ã ã‘ã§ã‚ã¡ã‚ƒã‚ã¡ã‚ƒãªé‡ã®ç¤¾ä¼šä¿éšœãŒã‚ã‚‹ã®...     True  2020      3   \n",
       "2  RT @zibumitunari: æ˜æ—¥ã¯ã‚¨ã‚¤ãƒ—ãƒªãƒ«ãƒ•ãƒ¼ãƒ«ã ãŒã€ã‚³ãƒ­ãƒŠé–¢é€£ã®å˜˜ã¯çµ¶å¯¾ã«ã‚„ã‚ã‚ˆ...     True  2020      3   \n",
       "3  RT @akagiichirou: ãƒã‚¹ã‚¯ã®ï¼—å‰²ãŒä¸­å›½ã‹ã‚‰ã®è¼¸å…¥ã«é ¼ã£ã¦ããŸãŒæµé€šç¶²ãŒæ··ä¹±ã™...     True  2020      3   \n",
       "4  RT @bbcnewsjapan: BBCãƒ‹ãƒ¥ãƒ¼ã‚¹ - ãƒãƒ³ã‚¬ãƒªãƒ¼æ”¿åºœã€æ–°å‹ã‚¦ã‚¤ãƒ«ã‚¹å¯¾ç­–ã§å¼·...     True  2020      3   \n",
       "\n",
       "   day  hour      time  \n",
       "0   31    23  23:59:58  \n",
       "1   31    23  23:59:58  \n",
       "2   31    23  23:59:58  \n",
       "3   31    23  23:59:58  \n",
       "4   31    23  23:59:57  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfrts[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
